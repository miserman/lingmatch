[{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Specifying comparisons and groups in lingmatch","text":"’ll generate word category output, sort experimental design allows available comparison types: Imagine two studies paired participants, series interactions reading one set prompts:","code":"# load lingmatch library(\"lingmatch\")  # first, we have simple representations (function word category use frequencies) # of our prompts (3 prompts per study): prompts <- data.frame(   study = rep(paste(\"study\", 1:2), each = 3),   prompt = rep(paste(\"prompt\", 1:3), 2),   matrix(rnorm(3 * 2 * 7, 10, 4), 3 * 2, dimnames = list(NULL, names(lma_dict(1:7)))) ) prompts[1:5, 1:8] #>     study   prompt     ppron     ipron  article    adverb      conj      prep #> 1 study 1 prompt 1 -2.386199  8.173103 15.31614 11.796346 10.122909  6.299969 #> 2 study 1 prompt 2 10.367588 14.193150  8.28622  7.946068  7.827269 10.663010 #> 3 study 1 prompt 3 11.819483 17.814056 11.74569 11.775508 10.581855  9.707103 #> 4 study 2 prompt 1 11.838951  9.305286 15.46512  5.351728  3.975302 10.949702 #> 5 study 2 prompt 2  7.496894  5.511986 13.16816  4.214018 12.249418  7.070481  # then, the same representation of the language the participants produced: data <- data.frame(   study = sort(sample(paste(\"study\", 1:2), 100, TRUE)),   pair = sort(sample(paste(\"pair\", formatC(1:20, width = 2, flag = 0)), 100, TRUE)),   prompt = sample(paste(\"prompt\", 1:3), 100, TRUE),   speaker = sample(c(\"a\", \"b\"), 100, TRUE),   matrix(rnorm(100 * 7, 10, 4), 100, dimnames = list(NULL, colnames(prompts)[-(1:2)])) ) data[1:5, 1:8] #>     study    pair   prompt speaker     ppron     ipron   article    adverb #> 1 study 1 pair 01 prompt 3       b 14.736184 13.124716 13.658920  6.037883 #> 2 study 1 pair 01 prompt 3       a  7.541947  9.168116 14.739442  6.794754 #> 3 study 1 pair 01 prompt 1       a  8.959356 12.446371  1.253881 12.211032 #> 4 study 1 pair 01 prompt 2       b  6.226999 13.016239  5.640277 18.072344 #> 5 study 1 pair 01 prompt 1       a 10.169863 11.148340  5.959376  5.549767"},{"path":[]},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"sample-means","dir":"Articles","previous_headings":"Matching with a standard","what":"Sample means","title":"Specifying comparisons and groups in lingmatch","text":"Compare row (representing turn conversation) sample’s mean: considered baseline sample.","code":"# the `lsm` (Language Style Matching) type specifies the columns to consider, # and the metric to use (Canberra similarity) lsm_mean <- lingmatch(data, mean, type = \"lsm\")  # look at comparison information lsm_mean[c(\"comp.type\", \"comp\")] #> $comp.type #> [1] \"mean\" #>  #> $comp #>     ppron     ipron   article    adverb      conj      prep   auxverb  #> 10.196102 10.014049  9.517523  9.453215  9.609573 10.373243  9.194888  # and maybe the average similarity score mean(lsm_mean$sim) #> [1] 0.8219567"},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"stored-means","dir":"Articles","previous_headings":"Matching with a standard","what":"Stored means","title":"Specifying comparisons and groups in lingmatch","text":"LSM categories standard means stored internally, found LIWC manual.","code":"# compare with means from a set of tweets lsm_twitter <- lingmatch(data, \"twitter\", type = \"lsm\") lsm_twitter[c(\"comp.type\", \"comp\")] #> $comp.type #> [1] \"twitter\" #>  #> $comp #>         ppron ipron article adverb conj  prep auxverb #> twitter  9.02   4.6    5.58   5.13 4.19 11.88    8.27 mean(lsm_twitter$sim) #> [1] 0.732681  # or the means of the set that is most similar to the current set lsm_auto <- lingmatch(data, \"auto\", type = \"lsm\") lsm_auto[c(\"comp.type\", \"comp\")] #> $comp.type #> [1] \"auto: expressive\" #>  #> $comp #>            ppron ipron article adverb conj  prep auxverb #> expressive 12.74  5.28     5.7   6.02 7.46 14.27    9.25 mean(lsm_auto$sim) #> [1] 0.7641796"},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"external-means","dir":"Articles","previous_headings":"Matching with a standard","what":"External means","title":"Specifying comparisons and groups in lingmatch","text":"another set data, can also use means comparison:","code":"lsm_prmed <- lingmatch(data, colMeans(prompts[, -(1:2)]), type = \"lsm\") lsm_prmed[c(\"comp.type\", \"comp\")] #> $comp.type #> [1] \"colMeans(prompts[, -(1:2)])\" #>  #> $comp #>     ppron     ipron   article    adverb      conj      prep   auxverb  #>  7.192937 11.330314 12.025558  8.115529  8.951655  8.283233 11.151689 mean(lsm_prmed$sim) #> [1] 0.8063175"},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"group-means","dir":"Articles","previous_headings":"Matching with a standard","what":"Group means","title":"Specifying comparisons and groups in lingmatch","text":"can also compare means within groups. , studies might considered groups: type group variable just splitting data, performing comparisons within splits.","code":"lsm_topics <- lingmatch(data, group = study, type = \"lsm\") lsm_topics[c(\"comp.type\", \"comp\")] #> $comp.type #> [1] \"study group mean\" #>  #> $comp #>             ppron     ipron   article   adverb     conj      prep  auxverb #> study 1  9.641885 10.370336 10.099201 9.751910 9.742510 10.989931 8.849333 #> study 2 10.796504  9.628071  8.887371 9.129629 9.465558  9.705164 9.569239 tapply(lsm_topics$sim[, 2], lsm_topics$sim[, 1], mean) #>   study 1   study 2  #> 0.8221899 0.8244851"},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"matching-with-other-texts","dir":"Articles","previous_headings":"","what":"Matching with other texts","title":"Specifying comparisons and groups in lingmatch","text":"previous comparisons standards, LSM score interpreted indicating less generic language style (defined comparison grouping).","code":""},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"condition-id","dir":"Articles","previous_headings":"Matching with other texts","what":"Condition ID","title":"Specifying comparisons and groups in lingmatch","text":", prompts constitute experimental conditions. 3 unique prompt IDs, 6 unique prompts, since study set, need study prompt ID appropriately match prompts: , group argument just pasting together included variables, using resulting string identify single comparison text (acting condition ID).","code":"lsm <- lingmatch(data, prompts, group = c(\"study\", \"prompt\"), type = \"lsm\") lsm$comp.type #> [1] \"prompts\" lsm$comp[, 1:6] #>                      ppron     ipron   article    adverb      conj      prep #> study 1 prompt 1 -2.386199  8.173103 15.316144 11.796346 10.122909  6.299969 #> study 1 prompt 2 10.367588 14.193150  8.286220  7.946068  7.827269 10.663010 #> study 1 prompt 3 11.819483 17.814056 11.745690 11.775508 10.581855  9.707103 #> study 2 prompt 1 11.838951  9.305286 15.465124  5.351728  3.975302 10.949702 #> study 2 prompt 2  7.496894  5.511986 13.168156  4.214018 12.249418  7.070481 #> study 2 prompt 3  4.020906 12.984303  8.172014  7.609507  8.953179  5.009136 lsm$sim[1:10, ] #>                  g1  canberra #> 1  study 1 prompt 3 0.8404379 #> 2  study 1 prompt 3 0.7898045 #> 3  study 1 prompt 1 0.5974829 #> 4  study 1 prompt 2 0.6862665 #> 5  study 1 prompt 1 0.6322903 #> 6  study 1 prompt 1 0.7179879 #> 7  study 1 prompt 1 0.5270462 #> 8  study 1 prompt 1 0.7111559 #> 9  study 1 prompt 3 0.8668509 #> 10 study 1 prompt 2 0.8532957"},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"participant-id","dir":"Articles","previous_headings":"Matching with other texts","what":"Participant ID","title":"Specifying comparisons and groups in lingmatch","text":"Similarly, participants uniquely identified pair ID speaker ID (though just well single column unique IDs).","code":"interlsm <- lingmatch(data, group = c(\"pair\", \"speaker\"), type = \"lsm\") interlsm$comp[1:10, ] #>               ppron     ipron   article    adverb      conj      prep   auxverb #> pair 01 b 10.481591 13.070478  9.649599 12.055114 10.162593  6.435407  6.978507 #> pair 01 a  8.890389 10.920942  7.317566  8.185184 10.188682 15.954748  9.181015 #> pair 02 b 11.534186 12.768677 12.911620 11.688266  7.613752  8.475074  7.597097 #> pair 02 a 11.316318  8.106147 13.917521 10.073232 11.757045  7.945125  2.470896 #> pair 03 b 18.095285 11.326730 15.627835  8.515610  3.283834  8.836241 18.384651 #> pair 03 a 10.146101  2.069774 12.310361 11.662448 11.601301 12.962585  6.430815 #> pair 04 a  9.277130 10.037299  9.655987  5.983921 13.753607 11.388323 11.860396 #> pair 04 b  8.100905  9.277416 11.635250 11.869438 10.344109 13.508418  7.419389 #> pair 05 b  7.671408  9.513538 10.161844  7.602741 11.683079 11.092102  8.067743 #> pair 05 a  9.231082 13.474992 10.012660 11.400360 11.792820 13.499493  9.286318 interlsm$sim[1:10, ] #>           g1  canberra #> 1  pair 01 b 0.8093885 #> 2  pair 01 a 0.8655842 #> 3  pair 01 a 0.8200400 #> 4  pair 01 b 0.7615697 #> 5  pair 01 a 0.9066216 #> 6  pair 02 b 0.8905855 #> 7  pair 02 a 0.6713996 #> 8  pair 02 a 0.8228285 #> 9  pair 02 b 0.8550667 #> 10 pair 02 b 0.8839770"},{"path":"https://miserman.github.io/lingmatch/articles/groups.html","id":"matching-in-sequence","dir":"Articles","previous_headings":"","what":"Matching in sequence","title":"Specifying comparisons and groups in lingmatch","text":"Since participants interactions sequence, might compare turn sequence. last entry group argument specifies speaker: rownames sim show row numbers compared, aggregated speaker takes multiple turns row. also just compare edges adding agg = FALSE: Brought Language Use Social Interaction lab Texas Tech University","code":"seqlsm <- lingmatch(data, \"seq\", group = c(\"pair\", \"speaker\"), type = \"lsm\") seqlsm$sim[1:10, ] #>                         group  canberra #> 1 <-> 2, 3            pair 01 0.7801483 #> 2, 3 <-> 4            pair 01 0.7107801 #> 4 <-> 5               pair 01 0.7145824 #> 6 <-> 7, 8            pair 02 0.7986849 #> 7, 8 <-> 9, 10        pair 02 0.8377738 #> 11 <-> 12             pair 03 0.6461507 #> 13 <-> 14             pair 04 0.7725923 #> 14 <-> 15             pair 04 0.8448317 #> 15 <-> 16, 17, 18     pair 04 0.8243898 #> 16, 17, 18 <-> 19, 20 pair 04 0.8591672 lingmatch(   data, \"seq\",   group = c(\"pair\", \"speaker\"), type = \"lsm\", agg = FALSE )$sim[1:10, ] #>             group  canberra #> 1 <-> 2   pair 01 0.8504191 #> 3 <-> 4   pair 01 0.6873209 #> 4 <-> 5   pair 01 0.7145824 #> 6 <-> 7   pair 02 0.6747632 #> 8 <-> 9   pair 02 0.8265836 #> 11 <-> 12 pair 03 0.6461507 #> 13 <-> 14 pair 04 0.7725923 #> 14 <-> 15 pair 04 0.8448317 #> 15 <-> 16 pair 04 0.8561268 #> 18 <-> 19 pair 04 0.6755498"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Introduction to Text Analysis","text":"First, need text – anything results vector text .","code":"text <- c(   \"Hey, I like kittens. I think all kinds of cats really are just the best pet ever.\",   \"Oh yeah? Well I really like cars. All the wheels and the turbos... I think that's    the best ever.\",   \"You know what? Poo on you. Cats, dogs, rabbits -- you know, living creatures... to    think you'd care about anything else!\",   \"You can stick to your opinion. You can not be right if you want. You know what    life's about? Supercharging, diesel guzzling, exhaust spewing, piston moving    ignitions.\" )"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"manual-processing","dir":"Articles","previous_headings":"Setup","what":"Manual Processing","title":"Introduction to Text Analysis","text":"’ll taking ‘bag words’ approach text, means standardizing aggregating words sake simplicity. first step can convert characters lower case: Another standardizing step can remove punctuation, involve regular expression. Regular expression refined way parse strings characters. Forms regular expression implemented programming languages. can tricky think , offers great deal control comes parsing text. can use gsub function replace characters specify nothing. strsplit function splits one string many based regular expression string. Now list tokens (words object) corresponds original text: Now words separated, can translate original texts numerical vectors, resulting document-term matrix (DTM). first step can set empty matrix, row text, column unique word: Now can fill matrix counts word text. can count words , loop texts:","code":"# we'll name the processed text \"words\" since it will eventually be single words words <- tolower(text) words[1] #> [1] \"hey, i like kittens. i think all kinds of cats really are just the best pet ever.\" # you can get a feel for the way gsub works by entering an example string; # the first argument is the expression to match, the second is what to replace it with, # and the third is the text, so this will replace the blank space with an asterisk gsub(\" \", \"*\", \"a b\") #> [1] \"a*b\"  # []s define a set of single characters, and the + will match any repeats, # so one or more of the characters inside the brackets will be replaced with nothing. # entering a vector into the third position will apply the replacement to each element words <- gsub(\"[,.?!\\n-]+\", \"\", words) words[1] #> [1] \"hey i like kittens i think all kinds of cats really are just the best pet ever\" # this applies to each element of the vector, and returns a list of vectors words <- strsplit(words, \" +\") # the first text text[1] #> [1] \"Hey, I like kittens. I think all kinds of cats really are just the best pet ever.\"  # the tokenized version of that text words[[1]] #>  [1] \"hey\"     \"i\"       \"like\"    \"kittens\" \"i\"       \"think\"   \"all\"     #>  [8] \"kinds\"   \"of\"      \"cats\"    \"really\"  \"are\"     \"just\"    \"the\"     #> [15] \"best\"    \"pet\"     \"ever\" # since our words are in a list, we need to unlist them, then get only the unique ones # you don't need to sort the words, but it can make it easier to look through unique_words <- sort(unique(unlist(words)))  # then we can make a matrix of 0s, and put the unique words in as column names dtm <- matrix(   0, length(text), length(unique_words),   dimnames = list(NULL, unique_words) ) dtm[, 1:10] #>      about all and anything are be best can care cars #> [1,]     0   0   0        0   0  0    0   0    0    0 #> [2,]     0   0   0        0   0  0    0   0    0    0 #> [3,]     0   0   0        0   0  0    0   0    0    0 #> [4,]     0   0   0        0   0  0    0   0    0    0 # this will perform what's inside the curly brackets as many times as there are texts # i will be set to each number in the sequence, so i will equal 1 on the first run # the words and dtm objects all line up, so words[[i]] corresponds to dtm[i,] for (i in seq_along(words)) {    # we can use the table function to count the words in a text   counts <- table(words[[i]])    # now the counts object has words associated with their count for the ith text   # to get this into the dtm, we can use i to select the row,   # and counts' names to select the columns   dtm[i, names(counts)] <- counts }  # now dtm is filled in with counts for each text dtm[, 1:10] #>      about all and anything are be best can care cars #> [1,]     0   1   0        0   1  0    1   0    0    0 #> [2,]     0   1   1        0   0  0    1   0    0    1 #> [3,]     1   0   0        1   0  0    0   0    1    0 #> [4,]     1   0   0        0   0  1    0   2    0    0"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"processing-with-lingmatch","dir":"Articles","previous_headings":"Setup","what":"Processing with lingmatch","title":"Introduction to Text Analysis","text":"lma_dtm function lingmatch package sort processing just walked . even simple level processing, can get fun information: getting fundamentals information extraction/retrieval, , example, term similarity can used enhance searches, document similarity can search criteria. Document similarity also secretly linguistic similarity.","code":"# load in the lingmatch package library(lingmatch)  dtm <- lma_dtm(text, sparse = FALSE) dtm[, 1:10] #>      about all and anything are be best can care cars #> [1,]     0   1   0        0   1  0    1   0    0    0 #> [2,]     0   1   1        0   0  0    1   0    0    1 #> [3,]     1   0   0        1   0  0    0   0    1    0 #> [4,]     1   0   0        0   0  1    0   2    0    0 # a count of words across documents (total/global term count; collection frequency) colSums(dtm)[1:5] #>    about      all      and anything      are  #>        2        2        1        1        1  # a count of words in each document (word counts; document length) rowSums(dtm) #> [1] 17 19 20 27  # correlation between words (word vector similarity) cor(dtm)[1:5, 1:5] #>               about        all        and   anything        are #> about     1.0000000 -1.0000000 -0.5773503  0.5773503 -0.5773503 #> all      -1.0000000  1.0000000  0.5773503 -0.5773503  0.5773503 #> and      -0.5773503  0.5773503  1.0000000 -0.3333333 -0.3333333 #> anything  0.5773503 -0.5773503 -0.3333333  1.0000000 -0.3333333 #> are      -0.5773503  0.5773503 -0.3333333 -0.3333333  1.0000000  # correlation between texts (document similarity) lma_simets(dtm, \"pearson\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                       #> [1,]  I          .         .        . #> [2,]  0.4322086  I         .        . #> [3,] -0.2319170 -0.2721224 I        . #> [4,] -0.4062182 -0.3727363 0.309349 I"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"documentterm-weighting","dir":"Articles","previous_headings":"","what":"Document/Term Weighting","title":"Introduction to Text Analysis","text":"Using just raw word counts, common words weight – potentially many times . Often, ’re interested less common words, tend meaningful (tend content words, opposed function words, can better distinguish relevant texts search). simple, straightforward way adjust weight words transform count values. example, log transformations reduce difference values, increase reduction values increase: log(2) - log(1) = 0.6931472 log(5) - log(4) = 0.2231436 Another source weighting can come matrix-wide information, document term counts. example, dividing term document frequency reduces weight frequent terms: c(1, 1, 0) / sum(c(1, 1, 0)) = 0.5, 0.5, 0 c(1, 1, 1) / sum(c(1, 1, 1)) = 0.3333333, 0.3333333, 0.3333333 Different weighting schemes work either amplify dampen difference values document term frequencies. See ?lma_weight available schemes. get better sense effects scheme component might , can plot weighted raw word counts across range document frequencies:  plot weighted range variants raw document frequencies. y-axis shows relative values (z-scores), positive higher weight’s mean. , can see types terms scheme gives weight:  document weight types sensitive term frequency, can interesting see two extremes:   can weight example dtm see affects correlation documents: now, ’ll just apply standard word count (frequency) weighting, sometimes called normalization:","code":"# load in the splot package library(splot)  # available term weight types term_weights <- c(\"binary\", \"log\", \"sqrt\", \"count\", \"amplify\")  # apply each type of weighting to a 1:20 sequence Weighted <- sapply(term_weights, function(w) lma_weight(1:20, w, FALSE))  # plot weighted ~ raw term frequencies ## note that dark = TRUE makes the text white, so you might need to remove it splot(Weighted ~ 1:20, labx = \"Raw Count\", lines = \"connected\", dark = TRUE) # available document weight types doc_weights <- c(   \"df\", \"dflog\", \"dfmax\", \"dfmlog\", \"normal\",   \"idf\", \"ridf\", \"dpois\", \"ppois\", \"entropy\" )  # function to demonstrates weights over a range document frequencies weight_range <- function(w, value = 1) {   m <- diag(20)   m[upper.tri(m, TRUE)] <- if (is.numeric(value)) {     value   } else {     unlist(lapply(       1:20, function(v) rep(if (value == \"inverted\") 21 - v else v, v)     ))   }   lma_weight(m, w, FALSE, doc.only = TRUE) }  # categories of weightings for coloring category <- rep(c(\"df\", \"normal\", \"idf\", \"poisson\", \"entropy\"), c(4, 1, 2, 2, 1))  # set up options for reuse op <- list(   laby = \"Relative (Scaled) Weight\", labx = \"Document Frequency\",   leg = \"outside\", colorby = list(quote(category), grade = TRUE),   lines = \"connected\", mv.scale = TRUE, note = FALSE, dark = TRUE )  # then plot a few different examples splot(   sapply(doc_weights, weight_range) ~ 1:20,   options = op, title = \"Same Term, Varying Document Frequencies\",   sud = \"All term frequencies are 1.\" ) # here, counts increase from 1 to 20, and appear in 1 to 20 documents, # so less common terms also have lower term frequencies splot(   sapply(doc_weights, weight_range, value = \"sequence\") ~ 1:20,   options = op, title = \"Term as Document Frequencies\",   sud = \"Non-zero terms are the number of non-zero terms.\" ) # here, counts decrease from 20 to 1, while appearing in 1 to 20 documents, # so less common terms have higher term frequencies splot(   sapply(doc_weights, weight_range, value = \"inverted\") ~ 1:20,   options = op, title = \"Term Opposite of Document Frequencies\",   sud = \"Non-zero terms are the number of zero terms + 1.\" ) # idf dampens common words lma_simets(lma_weight(dtm, \"idf\"), \"pearson\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                          #> [1,]  I           .          .         . #> [2,]  0.03946144  I          .         . #> [3,] -0.30972726 -0.3372407  I         . #> [4,] -0.42504381 -0.4232513 -0.1271555 I  # whereas Poisson amplifies common words lma_simets(lma_weight(dtm, \"ppois\"), \"pearson\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                          #> [1,]  I           .          .         . #> [2,]  0.85434575  I          .         . #> [3,] -0.03475687 -0.03813932 I         . #> [4,] -0.05282330 -0.04472950 0.9891512 I wdtm <- lma_weight(dtm)"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"dimension-reduction","dir":"Articles","previous_headings":"","what":"Dimension Reduction","title":"Introduction to Text Analysis","text":"thorough way change dtm space reducing dimensions. effectively LIWC (dictionary-based approach) – reducing number terms combining categories. default, internal dictionary (?lma_dict) contains function word categories:","code":"rwdtm <- lma_termcat(wdtm) rwdtm[, 1:6] #>          ppron      ipron    article     adverb       conj       prep #> [1,] 0.1176471 0.00000000 0.05882353 0.17647059 0.00000000 0.05882353 #> [2,] 0.1052632 0.05263158 0.15789474 0.15789474 0.05263158 0.00000000 #> [3,] 0.2000000 0.10000000 0.00000000 0.00000000 0.05000000 0.15000000 #> [4,] 0.1851852 0.03703704 0.00000000 0.07407407 0.03703704 0.07407407"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"language-style-matching","dir":"Articles","previous_headings":"","what":"Language Style Matching","title":"Introduction to Text Analysis","text":"way ’ve set things allows us look standard Language Style Matching calculation: makes language style matching “Language Style Matching” choice weighting, dimension reduction, similarity metric.","code":"lma_simets(rwdtm, \"canberra\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                      #> [1,] I         .         .         . #> [2,] 0.4862434 I         .         . #> [3,] 0.2560134 0.4837410 I         . #> [4,] 0.3273152 0.3350735 0.5571444 I  # you could get the same thing from the lsm default lingmatch(text, type = \"lsm\")$sim #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                      #> [1,] I         .         .         . #> [2,] 0.4862434 I         .         . #> [3,] 0.2560134 0.4837410 I         . #> [4,] 0.3273152 0.3350735 0.5571444 I"},{"path":"https://miserman.github.io/lingmatch/articles/introduction.html","id":"latent-semantic-similarity","dir":"Articles","previous_headings":"","what":"Latent Semantic Similarity","title":"Introduction to Text Analysis","text":"example, exclude function words, weight term frequency - inverse document frequency, use latent semantic space reduce dimensions, cosine similarity measure matching, “latent semantic similarity”: Brought Language Use Social Interaction lab Texas Tech University","code":"dtm <- lma_dtm(text, \"function\") wdtm <- lma_weight(dtm, \"tf-idf\") rwdtm <- lma_lspace(wdtm, space = \"100k\", dir = \"~/Latent Semantic Spaces\") lma_simets(rwdtm, \"cosine\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                      #> [1,] I         .         .         . #> [2,] 0.5368533 I         .         . #> [3,] 0.8737899 0.4574263 I         . #> [4,] 0.6336500 0.8192378 0.5609665 I"},{"path":"https://miserman.github.io/lingmatch/articles/quickstart.html","id":"loading-your-texts","dir":"Articles","previous_headings":"","what":"Loading your texts","title":"Lingmatch Quick Start","text":"need path file containing texts. … Select interactively file.choose(), returns path. Windows: 'c:/users/name/documents/texts.txt' Linux: '/home/Name/Documents/texts.txt' Mac: '/users/name/documents/texts.txt' tilde (see starts path.expand('~')), e.g., '~/texts.txt' working directory (see getwd(), set setwd()), e.g., 'texts.txt' parent working directory, e.g., '../texts.txt' following examples, just relative path file shown. like set working directory folder containing files.","code":""},{"path":"https://miserman.github.io/lingmatch/articles/quickstart.html","id":"from-plain-text-files","dir":"Articles","previous_headings":"Loading your texts","what":"From plain-text files","title":"Lingmatch Quick Start","text":"one entry per line: want segment single file: want read multiple files folder: files just text, also enter path lingmatch functions, without first loading :","code":"texts = readLines('texts.txt') # with multiple lines between entries segs = read.segments('texts.txt')  # into 5 even segments segs = read.segments('texts.txt', 5)  # into 100 word chunks segs = read.segments('texts.txt', segment.size = 100)  # then get texts from segs texts = segs$text texts = read.segments('foldername')$text results = lingmatch('texts.txt')"},{"path":"https://miserman.github.io/lingmatch/articles/quickstart.html","id":"from-a-delimited-plain-text-file","dir":"Articles","previous_headings":"Loading your texts","what":"From a delimited plain-text file","title":"Lingmatch Quick Start","text":"texts column spreadsheet, stored plain-text file:","code":"# comma delimited data = read.csv('data.csv')  # tab delimited (sometimes with extension .tsv) data = read.delim('data.txt')  # Other delimiters; define with the sep argument. # might also need to change the quote or other arguments # depending on your file's format data = read.delim('data.txt', sep = 'delimiting character')  # then get texts from data texts = data$name_of_text_column"},{"path":"https://miserman.github.io/lingmatch/articles/quickstart.html","id":"from-a-microsoft-office-file","dir":"Articles","previous_headings":"Loading your texts","what":"From a Microsoft Office file","title":"Lingmatch Quick Start","text":"Install load readtext package: .doc .docx file: .xls .xlsx file:","code":"install.packages('readtext') library('readtext') texts = readtext('texts.docx')$text  # this returns all lines in one, so you could # use read.segments to split them up if needed texts = read.segments(texts)$text texts = readtext('data.xlsx')$name_of_text_column"},{"path":"https://miserman.github.io/lingmatch/articles/quickstart.html","id":"processing-text","dir":"Articles","previous_headings":"","what":"Processing text","title":"Lingmatch Quick Start","text":"Processing texts represents numerically, representation defines matching . example, matching structural features (e.g., number words average length) gives sense similar text texts: also look exact matching words making document-term matrix: raw texts next examples processed lma_dtm function, using defaults, also enter document-term matrix place texts, processed separately previous examples. structural features function word categories, give sense stylistically similar texts : get similarity something like tone, use sentiment dictionary: get similarity overall meaning, use content analysis focused dictionary like General Inquirer: set embeddings:","code":"structural_features = lma_meta(texts) # all words dtm = lma_dtm(texts)  # excluding stop words (function words) and rare words (those appearing in # fewer than 3 texts) dtm = lma_dtm(texts, exclude = 'function', dc.min = 2) function_cats = lma_termcat(texts, lma_dict()) sentiment = lma_termcat(texts, 'huliu', dir = '~/Dictionaries') inquirer_cats = lma_termcat(texts, 'inquirer', dir = '~/Dictionaries') glove_dimensions = lma_lspace(   lma_dtm(texts), 'glove', dir = '~/Latent Semantic Spaces' )"},{"path":"https://miserman.github.io/lingmatch/articles/quickstart.html","id":"measuring-matching","dir":"Articles","previous_headings":"","what":"Measuring matching","title":"Lingmatch Quick Start","text":"processed texts, can measure matching . calculate similarity different metrics: text average across texts, available metrics: just first second text:","code":"# Inverse Canberra distance can_sims = lma_simets(function_cats, metric = 'canberra')  # Cosine similarity cos_sims = lma_simets(function_cats, metric = 'cosine') sims_to_mean = lma_simets(function_cats, colMeans(function_cats)) lma_simets(function_cats[1,], function_cats[2,])"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Introduction to Text Classification","text":"First, need load text. using supervised classification methods, require text labeled way. Labels can range manual codings (human classifications ’re trying replicate) contextual features source. example, free-response text study participants asked recall time behaved either feminine masculine manner (osf.io/ra8zy). examples, can train classifier predict whether text describing feminine masculine behavior.","code":""},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"load-data","dir":"Articles","previous_headings":"Data","what":"Load data","title":"Introduction to Text Classification","text":"","code":"# download and load the data data <- read.csv(\"https://osf.io/download/vyste\")  # remove duplicate and empty texts data <- data[data$phase == \"Pre\" & data$Recall_Text != \"\", ]"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"make-a-train-test-split","dir":"Articles","previous_headings":"Data","what":"Make a train-test split","title":"Introduction to Text Classification","text":"ultimate goal classifier may automatically label new text future. well classifier able depends part similar texts training texts. best case new texts conceptually come population texts training sample. case, testing held-portion sample can make guess classifier’s future performance.","code":"# look at the class base-rates (by design, they are roughly even) table(data$Recall) / nrow(data) #>  #>  Feminine Masculine  #>  0.495283  0.504717  # define a train sample by drawing 70% of each class classes <- sort(unique(data$Recall)) train_ids <- unlist(lapply(classes, function(class) {   class_ids <- which(data$Recall == class)   sample(class_ids, length(class_ids) * .7) }), use.names = FALSE)  # now you have two datasets with similar class distributions data.frame(   rbind(table(data[-train_ids, \"Recall\"]), table(data[train_ids, \"Recall\"])),   total = c(nrow(data) - length(train_ids), length(train_ids)),   row.names = c(\"test\", \"train\") ) #>       Feminine Masculine total #> test        32        33    65 #> train       73        74   147"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"process-texts","dir":"Articles","previous_headings":"","what":"Process texts","title":"Introduction to Text Classification","text":"Processing text process deriving features . One straightforward ways extracting counting single words (unigrams), results document-term matrix. Words vary meaning quality cues, two common exclusion heuristics help address : Stop word removal excludes function words, high-frequency often less meaningful words, document-count minimum excludes words sufficiently rare given set.","code":"library(lingmatch)  # lma_dict() in the second position excludes function words # dc.min excludes terms with document-frequencies less than or equal to the set value dtm <- lma_dtm(data$Recall_Text, lma_dict(), dc.min = 5)  # extract the test rows # splitting after creating a dtm ensures features are consistent between splits, # but be cognizant of information passing between splits (such as if you were to # document weight before splitting) test_dtm <- dtm[-train_ids, ] dtm <- dtm[train_ids, ]"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"train-a-classifier","dir":"Articles","previous_headings":"","what":"Train a classifier","title":"Introduction to Text Classification","text":"Training classifier case generally involves learning class-associated weights terms, “learning” potentially simple calculating differences.","code":""},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"term-based-classifier","dir":"Articles","previous_headings":"Train a classifier","what":"Term-based classifier","title":"Introduction to Text Classification","text":"basic classifier use difference document counts (binary frequencies) classes associate terms class. results training dictionary category class, containing class-related terms. dictionary contains learned word-associations used later classify texts. can inspect sense terms important.","code":"# calculate differences in binary document counts (probabilities) between classes feminine_texts <- data[train_ids, \"Recall\"] == \"Feminine\" term_diffs <- colMeans(dtm[feminine_texts, ] != 0) -   colMeans(dtm[!feminine_texts, ] != 0)  # make a dictionary from the 40 most differing terms for each class dict <- list(   Feminine = sort(term_diffs, TRUE)[1:40],   Masculine = -sort(term_diffs)[1:40] ) # words sized by frequency, colored by associated gender, # and shaded by absolute difference library(wordcloud) library(splot)  terms <- unlist(lapply(dict, names), use.names = FALSE) wordcloud(   terms, sqrt(colSums(dtm[, terms])),   scale = c(4, 1),   min.freq = 0, random.order = FALSE,   random.color = FALSE, rot.per = 0, ordered.colors = TRUE, fixed.asp = FALSE,   colors = splot.color(     -abs(term_diffs[terms]),     rep(names(dict), vapply(dict, length, 0))   ) ) legend(   \"topleft\",   legend = names(dict), text.col = splot.color(),   text.font = 2, bty = \"n\" )"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"naïve-bayes-classifier","dir":"Articles","previous_headings":"Train a classifier","what":"Naïve Bayes classifier","title":"Introduction to Text Classification","text":"Bernoulli Naïve Bayes classifier uses information, often smooth probabilities.","code":"library(naivebayes) weights_nb <- bernoulli_naive_bayes(   (dtm != 0) * 1, data[train_ids, \"Recall\"],   laplace = 1 )  # what's returned is just the smoothed, conditional probability of each term laplace <- 1 rbind(   manual = tapply(dtm[, \"dress\"] != 0, data[train_ids, \"Recall\"], function(x) {     (sum(x) + laplace) / (length(x) + laplace * 2)   }),   package = weights_nb$prob1[\"dress\", ] ) #>          Feminine  Masculine #> manual  0.2266667 0.03947368 #> package 0.2266667 0.03947368"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"decision-tree-classifier","dir":"Articles","previous_headings":"Train a classifier","what":"Decision Tree classifier","title":"Introduction to Text Classification","text":"Decision trees (learned recursive partitioning algorithm) similar previous methods, consider conditional differences terms.","code":"library(rpart) library(rpart.plot)  weight_dtree <- rpart(data[train_ids, \"Recall\"] ~ ., as.data.frame(as.matrix(dtm)))  # default settings often select very few features, # making these models particularly interpretable rpart.plot(   weight_dtree,   extra = 2, mar = c(0, 0, 0, 0),   branch.col = \"white\", split.box.col = \"black\", split.col = \"white\",   nn.box.col = \"black\", nn.col = \"white\", space = 2 )"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"support-vector-machine-classifier","dir":"Articles","previous_headings":"Train a classifier","what":"Support Vector Machine classifier","title":"Introduction to Text Classification","text":"many many types classifiers, many based essentially information, potential pick different features. can easily apply range classifiers caret package (see topepo.github.io/caret), also adds parameter tuning. Many models also regularizing versions available (effectively select features), particularly useful many features, often case text.","code":"library(caret) weights_svm <- train(as.matrix(dtm), data[train_ids, \"Recall\"], \"svmLinear3\")"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"neural-network-classifiers","dir":"Articles","previous_headings":"Train a classifier","what":"Neural Network classifiers","title":"Introduction to Text Classification","text":"Neural Networks particularly flexible terms input, offer way use sequence information (getting away bag--words approach). Keras framework setting neural models. package interface Python, additional setup may required system, though handled automatically (see keras.rstudio.com).","code":""},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"basic-model","dir":"Articles","previous_headings":"Train a classifier > Neural Network classifiers","what":"Basic model","title":"Introduction to Text Classification","text":"","code":"library(keras)  # process texts in the same way, but add tokens.only = TRUE to preserve sequence # information tokenized <- lma_dtm(data$Recall_Text, lma_dict(), dc.min = 5, tokens.only = TRUE)  # specify your layers weights_neural <- keras_model_sequential(list(   # this set of layers translates input sequences of word indices to   # small embeddings, which are learned on the fly,   # then averages over the set of embeddings for a single text-centroid vector   layer_embedding(input_dim = length(tokenized$tokens) + 1, output_dim = 16),   layer_global_average_pooling_1d(),    # these are more or less arbitrary layers that can be played with more freely   layer_dense(units = 16, activation = \"relu\"),   layer_dropout(rate = 0.5),    # this is the final prediction layer, which converts the previous arbitrary layers   # to a binary classification   layer_dense(units = 1, activation = \"sigmoid\") )) #> Loaded Tensorflow version 2.9.1  # then compile the model (this changes the weights_neural object) compile(   weights_neural,   loss = \"binary_crossentropy\",   optimizer = \"adam\", metrics = \"accuracy\" )  # and train the model on the training sample fit(   weights_neural,   pad_sequences(tokenized$indices[train_ids], max(tokenized$WC)),   as.integer(feminine_texts),   epochs = 40, verbose = FALSE )"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"pre-trained-embeddings-model","dir":"Articles","previous_headings":"Train a classifier > Neural Network classifiers","what":"Pre-trained embeddings model","title":"Introduction to Text Classification","text":"can also use pre-trained embeddings, instead simply averaging words, can first use convolution layer calculated weighted sums word windows:","code":"# load a set of pre-trained embeddings, with unrecognized terms added with # weights of 0 space <- lma_lspace(names(tokenized$tokens), \"glove\", fill.missing = TRUE)  weights_neural_glove <- keras_model_sequential(list(   # this layer contains the loaded embeddings, which will not be adjusted   # during training   layer_embedding(     input_dim = nrow(space) + 1, output_dim = ncol(space),     input_length = max(tokenized$WC), trainable = FALSE,     weights = list(rbind(numeric(ncol(space)), space))   ),    # this layer convolves (calculates weighted sums) over the   # embeddings vectors within a window (set by kernel_size)   layer_conv_1d(filters = 16, kernel_size = 5, activation = \"relu\"),    # same final layers as the initial model   layer_global_average_pooling_1d(),   layer_dense(units = 16, activation = \"relu\"),   layer_dropout(rate = 0.5),   layer_dense(units = 1, activation = \"sigmoid\") )) compile(   weights_neural_glove,   loss = \"binary_crossentropy\",   optimizer = \"adam\", metrics = \"accuracy\" ) fit(   weights_neural_glove,   pad_sequences(tokenized$indices[train_ids], max(tokenized$WC)),   as.integer(feminine_texts),   epochs = 40, verbose = FALSE )"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"apply-and-assess-classifiers","dir":"Articles","previous_headings":"","what":"Apply and Assess classifiers","title":"Introduction to Text Classification","text":"Now, can feed previously trained weights classifiers (calculate predictions manually), measure accuracy (percent real test labels guessed correctly).","code":"# prepare the testset index sequences for the neural predictions test_sequences <- pad_sequences(tokenized$indices[-train_ids], max(tokenized$WC))  # get predicted labels from each model predictions <- data.frame(   terms = classes[max.col(lma_termcat(test_dtm, dict))],   bayes = predict(weights_nb, (test_dtm != 0) * 1),   dtree = predict(weight_dtree, as.data.frame(as.matrix(test_dtm)), \"class\"),   svm = predict(weights_svm, test_dtm),   neural = classes[(predict(weights_neural, test_sequences) < .5) + 1],   neural_glove = classes[(predict(weights_neural_glove, test_sequences) < .5) + 1] )  # when you have multiple classifiers, you can also add an ensemble classifier based # on them by, for example, taking the most-predicted class predictions$ensemble <- classes[(rowMeans(predictions == classes[1]) < .5) + 1]  # overall accuracies data.frame(accuracy = colMeans(predictions == data[-train_ids, \"Recall\"])) #>               accuracy #> terms        0.6615385 #> bayes        0.7076923 #> dtree        0.6923077 #> svm          0.6615385 #> neural       0.7384615 #> neural_glove 0.7230769 #> ensemble     0.7230769"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"testing-decisions","dir":"Articles","previous_headings":"","what":"Testing decisions","title":"Introduction to Text Classification","text":"examples far taken simple, default approach processing texts, often little reason behind processing decisions. cases, parameters can selected empirically trying option measuring accuracy model within sample.","code":""},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"weight-schemes","dir":"Articles","previous_headings":"Testing decisions","what":"Weight schemes","title":"Introduction to Text Classification","text":"example, might see keeping excluding stop words best different weighting schemes term-based model:","code":"# list every combination of term and document weights weights <- c(\"pmi\", \"ppmi\", as.character(outer(   c(\"binary\", \"log\", \"sqrt\", \"count\", \"amplify\"),   c(     \"dflog\", \"entropy\", \"ppois\", \"dpois\", \"dfmlog\", \"dfmax\", \"df\", \"idf\",     \"ridf\", \"normal\"   ),   paste,   sep = \"-\" )))  # set up an empty matrix for results weight_accuracies <- matrix(   numeric(length(weights) * 2),   ncol = 2,   dimnames = list(weights, c(\"keep\", \"exclude\")) )  # loop through each stop word option for (exclude in c(TRUE, FALSE)) {   # reprocess texts with or without stop words removed   tdtm <- lma_dtm(     data[train_ids, \"Recall_Text\"], if (exclude) lma_dict() else NULL,     dc.min = 5   )    # loop through each weighting scheme   for (weight in weights) {     # apply the weighting scheme     wdtm <- lma_weight(tdtm, weight)      # calculate mean differences between classes for each term     wterm_diffs <- colMeans(wdtm[feminine_texts, ]) - colMeans(wdtm[!feminine_texts, ])      # make dictionaries from the top 40 terms in each class     # then calculate category scores (applying the same weight     # scheme and taking the biggest as the class prediction)     wterm_preds <- classes[max.col(lma_termcat(wdtm, list(       Feminine = sort(wterm_diffs, TRUE)[1:40],       Masculine = -sort(wterm_diffs)[1:40]     )))]      # add the resulting accuracy to the set of results     weight_accuracies[weight, exclude + 1] <-       mean(wterm_preds == data[train_ids, \"Recall\"])   } }  # note that dark = TRUE makes the text white, so you might need to remove it splot(   weight_accuracies ~ weights,   type = \"point\", title = FALSE,   leg.title = \"Stop words\", laby = \"Overall Accuracy\", labx = \"Weight Scheme\",   lpos = \"bottomleft\", note = FALSE, sort = TRUE, mai = c(1.7, .5, 0, 0), dark = TRUE )"},{"path":"https://miserman.github.io/lingmatch/articles/text_classification.html","id":"train-test-splits","dir":"Articles","previous_headings":"Testing decisions","what":"Train-test splits","title":"Introduction to Text Classification","text":"general consideration using random train-test splits results probably vary bit resplits, close results may reliably different. example, might compare performance simpler models. Depending data model, can slow, ’ll also parallelize :","code":"library(parallel)  # make a full dtm full_dtm <- lma_dtm(data$Recall_Text, lma_dict(), dc.min = 5)  # make clusters cl <- makeCluster(detectCores() - 2)  # load objects into clusters clusterExport(cl, c(\"classes\", \"data\", \"full_dtm\"))  Model_Accuracies <- do.call(rbind, parLapply(cl, seq_len(100), function(i) {   # reload packages each time   library(lingmatch)   library(naivebayes)   library(rpart)   library(caret)    # draw a new set of training ids   ids <- unlist(lapply(classes, function(class) {     class_ids <- which(data$Recall == class)     sample(class_ids, length(class_ids) * .7)   }), use.names = FALSE)   train_dtm <- full_dtm[ids, ]   feminine_texts <- data[ids, \"Recall\"] == \"Feminine\"   test_dtm <- full_dtm[-ids, ]    # train models   terms <- colMeans(train_dtm[feminine_texts, ] != 0) -     colMeans(train_dtm[!feminine_texts, ] != 0)   bayes <- bernoulli_naive_bayes(     (train_dtm != 0) * 1, data[ids, \"Recall\"],     laplace = 1   )   dtree <- rpart(data[ids, \"Recall\"] ~ ., as.data.frame(as.matrix(train_dtm)))   svm <- train(as.matrix(train_dtm), data[ids, \"Recall\"], \"svmLinear3\")    # test models   colMeans(data.frame(     terms = classes[max.col(lma_termcat(       test_dtm, list(sort(terms, TRUE)[1:40], -sort(terms)[1:40])     ))],     bayes = predict(bayes, (test_dtm != 0) * 1),     dtree = predict(dtree, as.data.frame(as.matrix(test_dtm)), \"class\"),     svm = predict(svm, test_dtm)   ) == data[-ids, \"Recall\"]) })) stopCluster(cl)  data.frame(Average = colMeans(Model_Accuracies)) #>         Average #> terms 0.6860000 #> bayes 0.7310769 #> dtree 0.7318462 #> svm   0.7163077 splot(Model_Accuracies, title = FALSE, leg.title = \"Model\", dark = TRUE)"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"in-sample-vectors","dir":"Articles","previous_headings":"","what":"In-sample vectors","title":"Introduction to Word Vectors","text":"introduction mostly focused representation documents vectors within common document-term space. representation, can focus term vectors instead. , columns represent words terms occurrence 4 contexts (, sentences), rows provide co-occurrence information.","code":"library(splot) library(lingmatch)  texts <- c(   \"Frogs are small.\",   \"Crickets are small.\",   \"Horses are big.\",   \"Whales are big.\" ) (dtm <- lma_dtm(texts)) #> 4 x 7 sparse Matrix of class \"dgCMatrix\" #>      are big crickets frogs horses small whales #> [1,]   1   .        .     1      .     1      . #> [2,]   1   .        1     .      .     1      . #> [3,]   1   1        .     .      1     .      . #> [4,]   1   1        .     .      .     .      1"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"similarity-based-latent-dimensions","dir":"Articles","previous_headings":"In-sample vectors","what":"Similarity-based latent dimensions","title":"Introduction to Word Vectors","text":"Comparing word vectors can provide semantic information. example, can see animals co-occur different attributes:  similarity measures raw occurrence vectors form new, collapsed vectors—instead corresponding documents, columns new space represent attributes. new vectors can used make document comparisons flexible.","code":"# the t function transposes the document-term matrix # such that terms are in rows and documents in columns tdm <- t(dtm) (attribute_loadings <- lma_simets(tdm, tdm[c(\"small\", \"big\"), ], \"canberra\")) #> 7 x 2 sparse Matrix of class \"dgCMatrix\" #>          small  big #> are       0.50 0.50 #> big       0.00 1.00 #> crickets  0.75 0.25 #> frogs     0.75 0.25 #> horses    0.25 0.75 #> small     1.00 0.00 #> whales    0.25 0.75  # note that dark = TRUE makes the text white, so you might need to remove it splot(   attribute_loadings[, 1] ~ attribute_loadings[, 2],   type = \"scatter\",   lines = FALSE, laby = \"smallness\", labx = \"bigness\", dark = TRUE,   add = text(     y + .05 + c(0, 0, 0, -.09, 0, 0, -.09) ~ x,     labels = rownames(attribute_loadings)   ) ) # documents are already similar as expected # (with 1 <-> 2 and 3 <-> 4 being more, the rest less similar) # due to their overlap in the attribute words lma_simets(dtm, \"canberra\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                      #> [1,] I         .         .         . #> [2,] 0.7142857 I         .         . #> [3,] 0.4285714 0.4285714 I         . #> [4,] 0.4285714 0.4285714 0.7142857 I  # but mapping them to the attribute space # (dtm %*% space, forming a Document-Dimension Matrix) (ddm <- lma_lspace(dtm, space = attribute_loadings)) #> 4 x 2 sparse Matrix of class \"dgCMatrix\" #>      small  big #> [1,]  2.25 0.75 #> [2,]  2.25 0.75 #> [3,]  0.75 2.25 #> [4,]  0.75 2.25  # the similarity between animal words can also contribute lma_simets(ddm, \"canberra\") #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                  #> [1,] I   .   . . #> [2,] 1.0 I   . . #> [3,] 0.5 0.5 I . #> [4,] 0.5 0.5 1 I"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"pretrained-vectors","dir":"Articles","previous_headings":"","what":"Pretrained vectors","title":"Introduction to Word Vectors","text":"tidy narrow example terms texts semantic space creation, illustrates kind co-occurrence information used abstracted examples. get appropriate co-occurrence information larger set words, need many diverse observations use, large corpora usually used train embeddings. latent dimensions general-use spaces also usually similarities single seed words, instead optimized, form matrix factorization neural network. Pretrained spaces like available Open Science Framework (osf.io/489he), downloaded necessary run functions depend specify directory set directory options (can run lma_initdirs('~') initialize default locations).","code":""},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"embeddings-mapped-similarity","dir":"Articles","previous_headings":"Pretrained vectors","what":"Embeddings-mapped similarity","title":"Introduction to Word Vectors","text":"Similarity documents mapped embeddings sometimes called Latent Semantic Similarity (Landauer & Dumais, 1997) Word Centroid (inverse) Distance (Kusner et al., 2015). measures common parameter decisions associated . Latent Semantic Similarity, Document-Term Matrix usually initially weighted (Term-Frequency Inverse Document Frequency Pointwise Mutual Information schemes), stop words removed (though isn’t relevant case, since “” appears documents), mapped Singular Value Decomposition space, similarity measured cosine similarity. settings default 'lsa' type (using “100k_lsa” space): Word Centroid Distance, DTM usually also stop words removed, might frequency weighted (normalized), mapped word2vec space, distances measured Euclidean distance (inverted similarity): parameter decisions often left unexamined, potential affect ultimate characterization texts. One lingmatch’s goals make parameters easy play . See text classification vignette example .","code":"lingmatch(texts, type = \"lsa\")$sim #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                     #> [1,] I         .         .        . #> [2,] 0.9958587 I         .        . #> [3,] 0.3360165 0.3182474 I        . #> [4,] 0.4092065 0.3831958 0.751225 I lingmatch(   texts,   exclude = \"function\", weight = \"freq\",   space = \"google\", metric = \"euclidean\" )$sim #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                         #> [1,] I          .          .          . #> [2,] 0.01980396 I          .          . #> [3,] 0.01233601 0.01213053 I          . #> [4,] 0.01169420 0.01111124 0.01187304 I"},{"path":[]},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"term-coverage","dir":"Articles","previous_headings":"Pretrained vectors > Choosing a space","what":"Term coverage","title":"Introduction to Word Vectors","text":"One simple thing look choosing space coverage terms appear set texts. can without downloading spaces looking common term map, select.lspace function terms entered first argument:","code":"# some relatively rare terms for their formatting or narrow uses terms <- c(\"part-time\", \"i/o\", \"'cause\", \"brexit\", \"debuffs\")  # select.lspace returns more information about each space, # as well as the full common term map spaces <- select.lspace(terms)  # the \"selected\" entry contains the top 5 spaces in terms of coverage spaces$selected[, c(\"terms\", \"coverage\")] #>                   terms coverage #> CoNLL17_skipgram 459818      1.0 #> facebook_crawl    81653      0.8 #> glove_crawl      467538      0.8 #> paragram_sl999   456295      0.8 #> paragram_ws353   456295      0.8"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"term-similarity","dir":"Articles","previous_headings":"Pretrained vectors > Choosing a space","what":"Term similarity","title":"Introduction to Word Vectors","text":"Another possible, though intensive thing look might similarities words particular interest. initial set texts, example, looking animals relation attributes, might look relationships whatever spaces ’ve downloaded:","code":"# retrieve names of downloaded spaces space_names <- unique(sub(\"\\\\..*$\", \"\", list.files(\"~/Latent Semantic Spaces\", \"dat\")))  # define a few terms of interest terms <- c(\"small\", \"big\", \"frogs\", \"crickets\", \"horses\", \"whales\")  # retrieve the terms from each space, # and calculate their similarities if all are found similarities <- do.call(rbind, lapply(space_names, function(name) {   lss <- tryCatch(lma_lspace(terms, name), error = function(e) NULL)   if (all(terms %in% rownames(lss))) {     lma_simets(lss, \"canberra\")@x   } else {     numeric(length(terms) * (length(terms) - 1) / 2)   } })) dimnames(similarities) <- list(   space_names,   outer(terms, terms, paste, sep = \" <-> \")[lower.tri(diag(length(terms)))] )  # sort by relationships of interest, # where, e.g., horses <-> big and frogs <-> small should be big, # and frogs <-> horses should be small weights <- structure(   c(-1, 1, 1, -.5, -.5, -.5, -.5, 1, 1, 1, -.5, -.5, -.5, -.5, 1),   names = colnames(similarities) ) similarities <- similarities[order(-similarities %*% weights), ] t(similarities[1, weights == 1, drop = FALSE]) #>                         100k #> frogs <-> small    0.3177957 #> crickets <-> small 0.3011583 #> horses <-> big     0.2848507 #> whales <-> big     0.2696833 #> crickets <-> frogs 0.4812112 #> whales <-> horses  0.3230661  # retrieve the attribute similarities in the best space sims_best <- cbind(   smallness = similarities[1, grep(\"small\", colnames(similarities))[-1]],   bigness = similarities[1, grep(\"big\", colnames(similarities))[-1]] )  # and look at where the animal words fall within that attribute space splot(   smallness ~ bigness, sims_best,   type = \"scatter\", dark = TRUE,   lines = FALSE, add = text(y + .0017 ~ I(x + 3e-4), labels = terms[-(1:2)]) )"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"concept-capture","dir":"Articles","previous_headings":"Pretrained vectors > Choosing a space","what":"Concept capture","title":"Introduction to Word Vectors","text":"One problems embeddings try solve crispiness single terms—may many terms strictly different similar meanings. mind, might also make sense fuzzify target dimensions example something like synonym centroids concept vectors: best way learn animal attributes general probably come training texts, might start look like research personality psychology clusters words used describe people (e.g., Thurstone, 1934).","code":"# now our attributes will be sets of terms concepts <- list(   small = c(\"small\", \"little\", \"tiny\", \"miniscule\", \"diminutive\"),   big = c(\"big\", \"large\", \"huge\", \"massive\", \"enormous\", \"gigantic\") )  # extract all single terms from the space lss_exp <- lma_lspace(c(terms, unlist(concepts)), rownames(similarities)[1])  # then replace the initial seed word vectors with their aggregated versions lss_exp[\"small\", ] <- colMeans(lss_exp[rownames(lss_exp) %in% concepts$small, ]) lss_exp[\"big\", ] <- colMeans(lss_exp[rownames(lss_exp) %in% concepts$big, ])  # calculate similarities to those new aggregate vectors sims_best_fuz <- lma_simets(   lss_exp[terms, ], \"canberra\",   symmetrical = TRUE )[-(1:2), c(\"small\", \"big\")]  # and see if animal relationships are improved splot(   sims_best_fuz[, \"small\"] ~ sims_best_fuz[, \"big\"],   type = \"scatter\",   lines = FALSE, laby = \"smallness\", labx = \"bigness\", dark = TRUE,   add = text(y + .0014 ~ x, labels = rownames(sims_best_fuz)) )"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"dimensions-as-topics","dir":"Articles","previous_headings":"","what":"Dimensions as topics","title":"Introduction to Word Vectors","text":"Topic extraction tends use models fit within sample, considers fewer dimensions. goal often interpret dimensions directly inspecting highest loading terms, can result something like dictionary—sets topics categories containing associated terms. loading full space (opposed just selection vectors, ’ve ), can look thing pretrained embeddings:    can also interesting use term select dimensions look :","code":"# load in the full \"100k\" space lss <- lma_lspace(\"100k\") structure(dim(lss), names = c(\"terms\", \"dimensions\")) #>      terms dimensions  #>      99188        300  # make a function to more easily retrieve terms top_terms <- function(dims, space, nterms = 30) {   terms <- sort(rowMeans(space[, dims, drop = FALSE]), TRUE)[seq_len(nterms)]   splot(     terms ~ names(terms),     colorby = terms, leg = FALSE, sort = TRUE, dark = TRUE,     type = \"bar\", laby = FALSE, labx = FALSE, title = paste(       \"dimension\",       if (length(dims) != 1 && sum(dims) == sum(seq_along(dims))) {         paste0(dims[1], \"-\", dims[length(dims)])       } else {         paste(dims, collapse = \", \")       }     )   ) } # now we can see the top-loading terms on single dimensions for (d in 1:3) top_terms(d, lss) # or top-loading terms on aggregated dimensions top_terms(1:5, lss) # even across all dimensions top_terms(seq_len(ncol(lss)), lss) # this selects the 10 dimensions on which \"whales\" loads most top_terms(order(-lss[\"whales\", ])[1:10], lss)"},{"path":"https://miserman.github.io/lingmatch/articles/word_vectors.html","id":"dictionaries-as-embeddings","dir":"Articles","previous_headings":"","what":"Dictionaries as embeddings","title":"Introduction to Word Vectors","text":"can also kind interesting think dictionaries forms embeddings, either binary weighted regions sharp drop-offs: simple space, can solve analogies like can pretrained spaces (Mikolov et al., 2013):","code":"# take a small, weighted, sexed family dictionary family <- list(   female = c(mother = 1, wife = .9, sister = .8, grandma = .7, aunt = .6),   male = c(father = 1, husband = .9, brother = .8, grandpa = .7, uncle = .6) ) family_terms <- unlist(lapply(family, names), use.names = FALSE)  # then make a latent space out of it (family_space <- cbind(   family = structure(unlist(family), names = family_terms),   female = family_terms %in% names(family$female),   male = family_terms %in% names(family$male) )) #>         family female male #> mother     1.0      1    0 #> wife       0.9      1    0 #> sister     0.8      1    0 #> grandma    0.7      1    0 #> aunt       0.6      1    0 #> father     1.0      0    1 #> husband    0.9      0    1 #> brother    0.8      0    1 #> grandpa    0.7      0    1 #> uncle      0.6      0    1 # function to abstract the process guess_analog <- function(x, y, asx, space, metric = \"canberra\") {   sort(lma_simets(     space, space[y, ] - space[x, ] + space[asx, ], metric   ), TRUE) }  ## i.e., sister is to brother as aunt is to _____. guess_analog(\"sister\", \"brother\", \"aunt\", family_space)[1:3] #>    uncle  grandpa  brother  #> 1.000000 0.974359 0.952381  # compare with a pretrained space in the same restricted term set guess_analog(   \"sister\", \"brother\", \"aunt\",   lma_lspace(family_terms, \"google\") )[1:3] #>     uncle      aunt    father  #> 0.5902633 0.5521642 0.5453535"},{"path":[]},{"path":"https://miserman.github.io/lingmatch/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Micah Iserman. Maintainer.","code":""},{"path":"https://miserman.github.io/lingmatch/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Iserman M (2022). lingmatch: Linguistic Matching Accommodation. R package version 1.0.4, https://github.com/miserman/lingmatch.","code":"@Manual{,   title = {lingmatch: Linguistic Matching and Accommodation},   author = {Micah Iserman},   year = {2022},   note = {R package version 1.0.4},   url = {https://github.com/miserman/lingmatch}, }"},{"path":"https://miserman.github.io/lingmatch/index.html","id":"lingmatch","dir":"","previous_headings":"","what":"Linguistic Matching and Accommodation","title":"Linguistic Matching and Accommodation","text":"--one R package assessment linguistic matching /accommodation.","code":""},{"path":"https://miserman.github.io/lingmatch/index.html","id":"features","dir":"","previous_headings":"","what":"features","title":"Linguistic Matching and Accommodation","text":"Input raw text, document-term matrix (DTM), LIWC output. Apply various weighting functions DTM. Measure similarity /accommodation various metrics. Calculate standard forms Language Style Matching (LSM) Latent Semantic Similarity (LSS).","code":""},{"path":"https://miserman.github.io/lingmatch/index.html","id":"resources","dir":"","previous_headings":"","what":"resources","title":"Linguistic Matching and Accommodation","text":"Quick start Comparisons Introduction text analysis Word vectors Text Classification Dictionary repository: osf.io/y6g5b Latent semantic space repository: osf.io/489he","code":""},{"path":"https://miserman.github.io/lingmatch/index.html","id":"installation","dir":"","previous_headings":"","what":"installation","title":"Linguistic Matching and Accommodation","text":"Download R r-project.org, install package R console: Release (version 1.0.3) Development (version 1.0.4) load package:","code":"install.packages('lingmatch') # install.packages('remotes') remotes::install_github('miserman/lingmatch') library(lingmatch)"},{"path":"https://miserman.github.io/lingmatch/index.html","id":"examples","dir":"","previous_headings":"","what":"examples","title":"Linguistic Matching and Accommodation","text":"Can make quick comparison two bits text; default give cosine similarity raw word-count vectors: , given vector texts: Process texts one step: process texts step step, measure similarity : within single function call: , want standard form (example), specify default:","code":"lingmatch('First text to look at.', 'Text to compare that text with.') text = c(   \"Why, hello there! How are you this evening?\",   \"I am well, thank you for your inquiry!\",   \"You are a most good at social interactions person!\",   \"Why, thank you! You're not all bad yourself!\" ) # with a dictionary inquirer_cats = lma_process(text, dict = 'inquirer', dir = '~/Dictionaries')  # with a latent semantic space glove_vectors = lma_process(text, space = 'glove', dir = '~/Latent Semantic Spaces') dtm = lma_dtm(text) dtm_weighted = lma_weight(dtm) dtm_categorized = lma_termcat(dtm_weighted, lma_dict(1:9)) similarity = lma_simets(dtm_categorized, metric = 'canberra') similarity = lingmatch(   text, weight = 'frequency', dict = lma_dict(1:9), metric = 'canberra' )$sim similarity = lingmatch(text, type = 'lsm')$sim"},{"path":"https://miserman.github.io/lingmatch/reference/download.dict.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Dictionaries — download.dict","title":"Download Dictionaries — download.dict","text":"Downloads specified dictionaries osf.io/y6g5b.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/download.dict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Dictionaries — download.dict","text":"","code":"download.dict(dict = \"lusi\", check.md5 = TRUE, mode = \"wb\",   dir = getOption(\"lingmatch.dict.dir\"))"},{"path":"https://miserman.github.io/lingmatch/reference/download.dict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Dictionaries — download.dict","text":"dict One names dictionaries download, '' available. See osf.io/y6g5b/wiki information, list available dictionaries. check.md5 Logical; TRUE (default), retrieves MD5 checksum OSF, compares calculated downloaded file check integrity. mode character specifying file write mode; default 'wb'. See download.file. dir Directory save dictionary;  default getOption('lingmatch.dict.dir').  must specified, option must set -- use lma_initdirs initialize directory.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/download.dict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Dictionaries — download.dict","text":"Path downloaded dictionary, list multiple downloaded.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/download.dict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Dictionaries — download.dict","text":"","code":"if (FALSE) {  download.dict(\"lusi\", dir = \"~/Dictionaries\") }"},{"path":"https://miserman.github.io/lingmatch/reference/download.lspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Latent Semantic Spaces — download.lspace","title":"Download Latent Semantic Spaces — download.lspace","text":"Downloads specified semantic space osf.io/489he.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/download.lspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Latent Semantic Spaces — download.lspace","text":"","code":"download.lspace(space = \"100k_lsa\", include.terms = TRUE,   decompress = TRUE, check.md5 = TRUE, mode = \"wb\",   dir = getOption(\"lingmatch.lspace.dir\"))"},{"path":"https://miserman.github.io/lingmatch/reference/download.lspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Latent Semantic Spaces — download.lspace","text":"space Name one spaces want download, '' available. '100k_lsa' default, common options might 'google', 'facebook', 'glove'. See osf.io/489he/wiki information, full list spaces. include.terms Logical; FALSE, .dat.bz2 file downloaded (numeric vectors). decompress Logical; TRUE (default), decompresses downloaded file bunzip2 system command assuming available  (indicated Sys.('bunzip2')). check.md5 Logical; TRUE (default), retrieves MD5 checksum OSF, compares calculated downloaded file check integrity. mode character specifying file write mode; default 'wb'. See download.file. dir Directory save space. Specify , set lspace directory option (e.g., options(lingmatch.lspace.dir = '~/Latent Semantic Spaces')), use lma_initdirs initialize directory.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/download.lspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Latent Semantic Spaces — download.lspace","text":"character vector paths [1] data [2] term files.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/download.lspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Latent Semantic Spaces — download.lspace","text":"","code":"if (FALSE) {  download.lspace(\"glove_crawl\", dir = \"~/Latent Semantic Spaces\") }"},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":null,"dir":"Reference","previous_headings":"","what":"Linguistic Matching and Accommodation — lingmatch","title":"Linguistic Matching and Accommodation — lingmatch","text":"Offers variety methods assess linguistic matching accommodation, matching general similarity (sometimes called homophily), accommodation form conditional similarity (accounting base-rate precedent; sometimes called alignment).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linguistic Matching and Accommodation — lingmatch","text":"","code":"lingmatch(input = NULL, comp = mean, data = NULL, group = NULL, ...,   comp.data = NULL, comp.group = NULL, order = NULL, drop = FALSE,   all.levels = FALSE, type = \"lsm\")"},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linguistic Matching and Accommodation — lingmatch","text":"input Texts compared; vector, document-term matrix (dtm; terms column names), path file (.txt .csv, texts separated one lines/rows). comp Defines comparison made: function, applied input within group (overall     group; .e., apply(input, 2, comp); e.g., comp = mean compare text     mean profile group). character length 1 spaces: partially matches one lsm_profiles's rownames, row used comparison. partially matches 'auto', highest correlating lsm_profiles row used. partially matches 'pairwise', text compared one another. partially matches 'sequential', last variable group treated         speaker ID (see Grouping Comparisons section). character vector, processed way input. vector, either () logical factor-like (n levels < length) length     nrow(input), (b) numeric logical length less nrow(input), used     select subset input (e.g., comp = 1:10 treat first 10 rows input     comparison; comp = type == 'prompt' make logical vector identifying prompts, assuming \"type\"     name column data, variable global environment, value \"prompt\" marked     prompts). matrix-like object (multiple rows columns), named vector,     treated sort dtm, assuming common (column) names input     comp (e.g., prompt response texts already processed separately). data matrix-like object reference column names, variables referred arguments (e.g., lingmatch(text, data = data) lingmatch(data$text). group logical factor-like vector length NROW(input), used defined groups. ... Passes arguments lma_dtm, lma_weight, lma_termcat, /lma_lspace (depending input comp), lma_simets. comp.data matrix-like object source comp variables. comp.group column name grouping variable(s) comp.data; group contains references column names, comp.group specified, group variables looked comp.data. order numeric vector length nrow(input) indicating order texts grouping variables type comparison sequential. necessary texts already ordered desired. drop logical; FALSE, columns sum 0 retained. .levels logical; FALSE, multiple groups combined. See Grouping Comparisons section. type character least partially matching 'lsm' 'lsa'; applies default settings aligning standard calculations type:","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linguistic Matching and Accommodation — lingmatch","text":"list processed components input, information comparison, results comparison: dtm: sparse matrix; raw count-dtm, version original input     processed. processed: matrix-like object; processed version input     (e.g., weighted categorized). comp.type: string describing comparison applicable. comp: vector matrix-like object; comparison data applicable. group: string describing group applicable. sim: Result lma_simets.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Linguistic Matching and Accommodation — lingmatch","text":"great many points decision assessment linguistic similarity /accommodation, partly inherited great many point decision inherent numerical representation language. Two general types matching implemented sets defaults: Language/Linguistic Style Matching (LSM; Niederhoffer & Pennebaker, 2002; Ireland & Pennebaker, 2010), Latent Semantic Analysis/Similarity (LSA; Landauer & Dumais, 1997; Babcock, Ta, & Ickes, 2014). See type argument specifics.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"grouping-and-comparisons","dir":"Reference","previous_headings":"","what":"Grouping and Comparisons","title":"Linguistic Matching and Accommodation — lingmatch","text":"Defining groups comparisons can sometimes bit complicated, requires dataset specific knowledge, always (readily) done automatically. Variables entered group argument treated differently depending position arguments: Splitting default, groups treated define separate chunks data     comparisons calculated. Functions used calculated comparisons,     pairwise comparisons performed separately groups. example,     wanted compare text mean texts condition, group     variable identify split condition. Given multiple grouping variables,     calculations either done split (.levels = TRUE; applied     sequence groups become smaller smaller), splits made (    .levels = FALSE). makes 'one many' comparisons either calculated     preexisting standards (.e., profile current data, precalculated profile,     respectively). Comparison ID comparison data identified comp, groups assumed     apply input comp (either data, separately     data comp.data, case comp.group may needed     grouping variable different names data comp.data).     case, multiple grouping variables combined single factor assumed     uniquely identify comparison. makes 'one many' comparisons specific texts     (case manipulated prompts text-based conditions). Speaker ID comp matches 'sequential', last grouping variable     entered assumed identify something like speakers (.e., factor two     levels multiple observations per level). case, data assumed ordered     (ordered sorted order specified). additional grouping variables     last treated splitting groups. can set probabilistic     accommodation metrics. moment, sequential comparisons made within groups,     similarity scores speakers averaged, resulting mean matching speakers     within group.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Linguistic Matching and Accommodation — lingmatch","text":"Babcock, M. J., Ta, V. P., & Ickes, W. (2014). Latent semantic similarity language style   matching initial dyadic interactions. Journal Language Social Psychology, 33,   78-88. Ireland, M. E., & Pennebaker, J. W. (2010). Language style matching writing: synchrony   essays, correspondence, poetry. Journal Personality Social Psychology, 99,   549. Landauer, T. K., & Dumais, S. T. (1997). solution Plato's problem: latent semantic   analysis theory acquisition, induction, representation knowledge.   Psychological Review, 104, 211. Niederhoffer, K. G., & Pennebaker, J. W. (2002). Linguistic style matching social interaction.   Journal Language Social Psychology, 21, 337-360.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/lingmatch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Linguistic Matching and Accommodation — lingmatch","text":"","code":"# compare single strings lingmatch(\"Compare this sentence.\", \"With this other sentence.\") #> $dtm #> 2 x 5 sparse Matrix of class \"dgCMatrix\" #>      compare other sentence this with #> [1,]       .     1        1    1    1 #> [2,]       1     .        1    1    . #>  #> $processed #> 1 x 5 sparse Matrix of class \"dgCMatrix\" #>      compare other sentence this with #> [1,]       1     .        1    1    . #>  #> $comp.type #> [1] \"text\" #>  #> $comp #>  compare    other sentence     this     with  #>        0        1        1        1        1  #>  #> $group #> NULL #>  #> $sim #>    cosine  #> 0.5773503  #> attr(,\"time\") #> simets  #>      0  #>   # compare each entry in a character vector with... texts <- c(   \"One bit of text as an entry...\",   \"Maybe multiple sentences in an entry. Maybe essays or posts or a book.\",   \"Could be lines or a column from a read-in file...\" )  ## one another lingmatch(texts) #> $dtm #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $processed #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $comp.type #> [1] \"pairwise\" #>  #> $comp #> NULL #>  #> $group #> NULL #>  #> $sim #> 3 x 3 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                           #> [1,] I         .        . #> [2,] 0.1833397 I        . #> [3,] 0.0000000 0.280056 I #>   ## the first lingmatch(texts, 1) #> $dtm #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $processed #> 2 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [2,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $comp.type #> [1] \"1\" #>  #> $comp #>         a        an        as        be       bit      book    column     could  #>         0         1         1         0         1         0         0         0  #>     entry    essays      file      from        in     lines     maybe  multiple  #>         1         0         0         0         0         0         0         0  #>        of       one        or     posts   read-in sentences      text  #>         1         1         0         0         0         0         1  #>  #> $group #> NULL #>  #> $sim #> [1] 0.1833397 0.0000000 #> attr(,\"time\") #> simets  #>      0  #>   ## the next lingmatch(texts, \"seq\") #> $dtm #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $processed #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $comp.type #> [1] \"sequential\" #>  #> $comp #> NULL #>  #> $group #> NULL #>  #> $sim #>            cosine #> 1 <-> 2 0.1833397 #> 2 <-> 3 0.2800560 #>   ## the set average lingmatch(texts, mean) #> $dtm #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $processed #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $comp.type #> [1] \"mean\" #>  #> $comp #>         a        an        as        be       bit      book    column     could  #> 1.0000000 0.6666667 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333  #>     entry    essays      file      from        in     lines     maybe  multiple  #> 0.6666667 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.6666667 0.3333333  #>        of       one        or     posts   read-in sentences      text  #> 0.3333333 0.3333333 1.0000000 0.3333333 0.3333333 0.3333333 0.3333333  #>  #> $group #> NULL #>  #> $sim #> [1] 0.4909903 0.8051610 0.6666667 #> attr(,\"time\") #> simets  #>      0  #>   ## other entries in a group lingmatch(texts, group = c(\"a\", \"a\", \"b\")) #> $dtm #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $processed #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $comp.type #> [1] \"group mean\" #>  #> $comp #>     a an  as be bit book column could entry essays file from  in lines maybe #> a 0.5  1 0.5  0 0.5  0.5      0     0     1    0.5    0    0 0.5     0     1 #> b 2.0  0 0.0  1 0.0  0.0      1     1     0    0.0    1    1 0.0     1     0 #>   multiple  of one or posts read-in sentences text #> a      0.5 0.5 0.5  1   0.5       0       0.5  0.5 #> b      0.0 0.0 0.0  1   0.0       1       0.0  0.0 #>  #> $group #> [1] \"c('a', 'a', 'b')\" #>  #> $sim #>   g1    cosine #> 1  a 0.6428571 #> 2  a 0.8708636 #> 3  b 1.0000000 #>   ## one another, without stop words lingmatch(texts, exclude = \"function\") #> $dtm #> 3 x 10 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 10 column names 'book', 'column', 'entry' ... ]] #>                          #> [1,] . . 1 . . . . . . 1 #> [2,] 1 . 1 1 . . 1 . 1 . #> [3,] . 1 . . 1 1 . 1 . . #>  #> $processed #> 3 x 10 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 10 column names 'book', 'column', 'entry' ... ]] #>                          #> [1,] . . 1 . . . . . . 1 #> [2,] 1 . 1 1 . . 1 . 1 . #> [3,] . 1 . . 1 1 . 1 . . #>  #> $comp.type #> [1] \"pairwise\" #>  #> $comp #> NULL #>  #> $group #> NULL #>  #> $sim #> 3 x 3 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                    #> [1,] I         . . #> [2,] 0.3162278 I . #> [3,] 0.0000000 0 I #>   ## a standard average (based on function words) lingmatch(texts, \"auto\", dict = lma_dict(1:9)) #> $dtm #> 3 x 23 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 23 column names 'a', 'an', 'as' ... ]] #>                                                    #> [1,] . 1 1 . 1 . . . 1 . . . . . . . 1 1 . . . . 1 #> [2,] 1 1 . . . 1 . . 1 1 . . 1 . 2 1 . . 2 1 . 1 . #> [3,] 2 . . 1 . . 1 1 . . 1 1 . 1 . . . . 1 . 1 . . #>  #> $processed #>      ppron ipron article adverb conj prep auxverb negate quant #> [1,]     0     0       1      0    0    2       0      0     1 #> [2,]     0     0       2      2    2    1       0      0     1 #> [3,]     0     0       2      0    1    1       2      0     0 #> attr(,\"WC\") #> [1]  7 13 10 #> attr(,\"time\") #>     dtm termcat  #>       0       0  #> attr(,\"type\") #> [1] \"count\" #>  #> $comp.type #> [1] \"auto: nytimes\" #>  #> $comp #>         ppron ipron article adverb conj  prep auxverb negate quant #> nytimes  3.56  3.84    9.08   2.76 4.85 14.27    5.11   0.62  1.94 #>  #> $group #> NULL #>  #> $sim #> [1] 0.8341107 0.6844995 0.7757765 #> attr(,\"time\") #> simets  #>      0  #>"},{"path":"https://miserman.github.io/lingmatch/reference/lma_dict.html","id":null,"dir":"Reference","previous_headings":"","what":"English Function Word Category and Special Character Lists — lma_dict","title":"English Function Word Category and Special Character Lists — lma_dict","text":"Returns list function words based Linguistic Inquiry Word Count 2015 dictionary (terms category names -- words selected independently), list special characters patterns.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"English Function Word Category and Special Character Lists — lma_dict","text":"","code":"lma_dict(..., as.regex = TRUE, as.function = FALSE)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_dict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"English Function Word Category and Special Character Lists — lma_dict","text":"... Numbers letters corresponding category names: ppron, ipron, article, adverb, conj, prep, auxverb, negate, quant, interrog, number, interjection, special. .regex Logical: FALSE, lists returned without regular expression. .function Logical function: specified .regex TRUE, selected dictionary collapsed regex string (terms separated |), function matching characters string returned. regex string passed matching function (grepl default) 'pattern' argument, first argument returned function passed 'x' argument. See examples.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"English Function Word Category and Special Character Lists — lma_dict","text":"list vector terms category, (.function = TRUE) function accepts initial \"terms\" argument (character vector), additional arguments determined function entered .function (grepl default).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dict.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"English Function Word Category and Special Character Lists — lma_dict","text":"special category returned unless specifically requested. list regular expression strings attempting capture special things like ellipses emojis, sets special characters (outside Basic Latin range; [^\\u0020-\\u007F]), can used character conversions. special part returned list, .regex set TRUE. special list always used lma_dtm lma_termcat. creating dtm, special used clean original input (, default, punctuation involved ellipses emojis treated different -- ellipses emojis rather periods parens colons ). categorizing dtm, input dictionary passed special lists sure terms dtm match dictionary (, example, \": (\" replaced \"repfrown\" text dictionary).","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/lma_dict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"English Function Word Category and Special Character Lists — lma_dict","text":"","code":"# return the full dictionary (excluding special) lma_dict() #> $ppron #>  [1] \"^dae$\"        \"^dem$\"        \"^eir$\"        \"^eirself$\"    \"^em$\"         #>  [6] \"^he$\"         \"^he'\"         \"^her$\"        \"^hers$\"       \"^herself$\"    #> [11] \"^hes$\"        \"^him$\"        \"^himself$\"    \"^hir$\"        \"^hirs$\"       #> [16] \"^hirself$\"    \"^his$\"        \"^hisself$\"    \"^i$\"          \"^i'\"          #> [21] \"^id$\"         \"^idc$\"        \"^idgaf$\"      \"^idk$\"        \"^idontknow$\"  #> [26] \"^idve$\"       \"^iirc$\"       \"^iknow$\"      \"^ikr$\"        \"^ill$\"        #> [31] \"^ily$\"        \"^im$\"         \"^ima$\"        \"^imean$\"      \"^imma$\"       #> [36] \"^ive$\"        \"^lets$\"       \"^let's$\"      \"^me$\"         \"^methinks$\"   #> [41] \"^mine$\"       \"^my$\"         \"^myself$\"     \"^omfg$\"       \"^omg$\"        #> [46] \"^oneself$\"    \"^our$\"        \"^ours\"        \"^she$\"        \"^she'\"        #> [51] \"^shes$\"       \"^thee$\"       \"^their$\"      \"^their'\"      \"^theirs\"      #> [56] \"^them$\"       \"^thems\"       \"^they$\"       \"^they'\"       \"^theyd$\"      #> [61] \"^theyll$\"     \"^theyve$\"     \"^thine$\"      \"^thou$\"       \"^thoust$\"     #> [66] \"^thy$\"        \"^thyself$\"    \"^u$\"          \"^u'\"          \"^ud$\"         #> [71] \"^ull$\"        \"^ur$\"         \"^ure$\"        \"^us$\"         \"^we$\"         #> [76] \"^we'\"         \"^weve$\"       \"^y'\"          \"^ya'\"         \"^yall\"        #> [81] \"^yins$\"       \"^yinz$\"       \"^you$\"        \"^you'\"        \"^youd$\"       #> [86] \"^youll$\"      \"^your$\"       \"^youre$\"      \"^yours$\"      \"^yourself$\"   #> [91] \"^yourselves$\" \"^youve$\"      \"^zer$\"        \"^zir$\"        \"^zirs$\"       #> [96] \"^zirself$\"    \"^zis$\"        #>  #> $ipron #>  [1] \"^another$\"   \"^anybo\"      \"^anyone\"     \"^anything\"   \"^dat$\"       #>  [6] \"^de+z$\"      \"^dis$\"       \"^everyb\"     \"^everyone\"   \"^everything\" #> [11] \"^few$\"       \"^it$\"        \"^it'$\"       \"^it'\"        \"^itd$\"       #> [16] \"^itll$\"      \"^its$\"       \"^itself$\"    \"^many$\"      \"^nobod\"      #> [21] \"^nothing$\"   \"^other$\"     \"^others$\"    \"^same$\"      \"^somebo\"     #> [26] \"^somebody'\"  \"^someone\"    \"^something\"  \"^stuff$\"     \"^that$\"      #> [31] \"^that'\"      \"^thatd$\"     \"^thatll$\"    \"^thats$\"     \"^these$\"     #> [36] \"^these'\"     \"^thesed$\"    \"^thesell$\"   \"^thesere$\"   \"^thing\"      #> [41] \"^this$\"      \"^this'\"      \"^thisd$\"     \"^thisll$\"    \"^those$\"     #> [46] \"^those'\"     \"^thosed$\"    \"^thosell$\"   \"^thosere$\"   \"^what$\"      #> [51] \"^what'\"      \"^whatd$\"     \"^whatever$\"  \"^whatll$\"    \"^whats$\"     #> [56] \"^which\"      \"^who$\"       \"^who'\"       \"^whod$\"      \"^whoever$\"   #> [61] \"^wholl$\"     \"^whom$\"      \"^whomever$\"  \"^whos$\"      \"^whose$\"     #> [66] \"^whosever$\"  \"^whosoever$\" #>  #> $article #> [1] \"^a$\"   \"^an$\"  \"^da$\"  \"^teh$\" \"^the$\" #>  #> $adverb #>   [1] \"^absolutely$\"      \"^actively$\"        \"^actually$\"        #>   [4] \"^afk$\"             \"^again$\"           \"^ago$\"             #>   [7] \"^ahead$\"           \"^almost$\"          \"^already$\"         #>  [10] \"^altogether$\"      \"^always$\"          \"^angrily$\"         #>  [13] \"^anxiously$\"       \"^any$\"             \"^anymore$\"         #>  [16] \"^anyway$\"          \"^anywhere$\"        \"^apparently$\"      #>  [19] \"^automatically$\"   \"^away$\"            \"^awhile$\"          #>  [22] \"^back$\"            \"^badly$\"           \"^barely$\"          #>  [25] \"^basically$\"       \"^below$\"           \"^brietermsy$\"      #>  [28] \"^carefully$\"       \"^causiously$\"      \"^certainly$\"       #>  [31] \"^clearly$\"         \"^closely$\"         \"^coldly$\"          #>  [34] \"^commonly$\"        \"^completely$\"      \"^constantly$\"      #>  [37] \"^continually$\"     \"^correctly$\"       \"^coz$\"             #>  [40] \"^currently$\"       \"^daily$\"           \"^deeply$\"          #>  [43] \"^definitely$\"      \"^definitly$\"       \"^deliberately$\"    #>  [46] \"^desperately$\"     \"^differently$\"     \"^directly$\"        #>  [49] \"^early$\"           \"^easily$\"          \"^effectively$\"     #>  [52] \"^elsewhere$\"       \"^enough$\"          \"^entirely$\"        #>  [55] \"^equally$\"         \"^especially$\"      \"^essentially$\"     #>  [58] \"^etc$\"             \"^even$\"            \"^eventually$\"      #>  [61] \"^ever$\"            \"^every$\"           \"^everyday$\"        #>  [64] \"^everywhere\"       \"^exactly$\"         \"^exclusively$\"     #>  [67] \"^extremely$\"       \"^fairly$\"          \"^far$\"             #>  [70] \"^finally$\"         \"^fortunately$\"     \"^frequently$\"      #>  [73] \"^fully$\"           \"^further$\"         \"^generally$\"       #>  [76] \"^gently$\"          \"^genuinely$\"       \"^good$\"            #>  [79] \"^greatly$\"         \"^hardly$\"          \"^heavily$\"         #>  [82] \"^hence$\"           \"^henceforth$\"      \"^hereafter$\"       #>  [85] \"^herein$\"          \"^heretofore$\"      \"^hesitantly$\"      #>  [88] \"^highly$\"          \"^hither$\"          \"^hopefully$\"       #>  [91] \"^hotly$\"           \"^however$\"         \"^immediately$\"     #>  [94] \"^importantly$\"     \"^increasingly$\"    \"^incredibly$\"      #>  [97] \"^indeed$\"          \"^initially$\"       \"^instead$\"         #> [100] \"^intensely$\"       \"^jus$\"             \"^just$\"            #> [103] \"^largely$\"         \"^lately$\"          \"^least$\"           #> [106] \"^legitimately$\"    \"^less$\"            \"^lightly$\"         #> [109] \"^likely$\"          \"^literally$\"       \"^loudly$\"          #> [112] \"^luckily$\"         \"^mainly$\"          \"^maybe$\"           #> [115] \"^meanwhile$\"       \"^merely$\"          \"^more$\"            #> [118] \"^moreover$\"        \"^most$\"            \"^mostly$\"          #> [121] \"^much$\"            \"^namely$\"          \"^naturally$\"       #> [124] \"^nearly$\"          \"^necessarily$\"     \"^nervously$\"       #> [127] \"^never$\"           \"^nevertheless$\"    \"^no$\"              #> [130] \"^nonetheless$\"     \"^normally$\"        \"^not$\"             #> [133] \"^notwithstanding$\" \"^obviously$\"       \"^occasionally$\"    #> [136] \"^often$\"           \"^once$\"            \"^only$\"            #> [139] \"^originally$\"      \"^otherwise$\"       \"^overall$\"         #> [142] \"^particularly$\"    \"^passionately$\"    \"^perfectly$\"       #> [145] \"^perhaps$\"         \"^personally$\"      \"^physically$\"      #> [148] \"^please$\"          \"^possibly$\"        \"^potentially$\"     #> [151] \"^practically$\"     \"^presently$\"       \"^previously$\"      #> [154] \"^primarily$\"       \"^probability$\"     \"^probably$\"        #> [157] \"^profoundly$\"      \"^prolly$\"          \"^properly$\"        #> [160] \"^quickly$\"         \"^quietly$\"         \"^quite$\"           #> [163] \"^randomly$\"        \"^rarely$\"          \"^rather$\"          #> [166] \"^readily$\"         \"^really$\"          \"^recently$\"        #> [169] \"^regularly$\"       \"^relatively$\"      \"^respectively$\"    #> [172] \"^right$\"           \"^roughly$\"         \"^sadly$\"           #> [175] \"^seldomly$\"        \"^seriously$\"       \"^shortly$\"         #> [178] \"^significantly$\"   \"^similarly$\"       \"^simply$\"          #> [181] \"^slightly$\"        \"^slowly$\"          \"^so$\"              #> [184] \"^some$\"            \"^somehow$\"         \"^sometimes$\"       #> [187] \"^somewhat$\"        \"^somewhere$\"       \"^soon$\"            #> [190] \"^specifically$\"    \"^still$\"           \"^strongly$\"        #> [193] \"^subsequently$\"    \"^successfully$\"    \"^such$\"            #> [196] \"^suddenly$\"        \"^supposedly$\"      \"^surely$\"          #> [199] \"^surprisingly$\"    \"^technically$\"     \"^terribly$\"        #> [202] \"^thence$\"          \"^thereafter$\"      \"^therefor$\"        #> [205] \"^therefore$\"       \"^thither$\"         \"^thoroughly$\"      #> [208] \"^thus$\"            \"^thusfar$\"         \"^thusly$\"          #> [211] \"^together$\"        \"^too$\"             \"^totally$\"         #> [214] \"^truly$\"           \"^typically$\"       \"^ultimately$\"      #> [217] \"^uncommonly$\"      \"^unfortunately$\"   \"^unfortunatly$\"    #> [220] \"^usually$\"         \"^vastly$\"          \"^very$\"            #> [223] \"^virtually$\"       \"^well$\"            \"^whence$\"          #> [226] \"^where\"            \"^wherefor\"         \"^whither$\"         #> [229] \"^wholly$\"          \"^why$\"             \"^why'\"             #> [232] \"^whyd$\"            \"^whys$\"            \"^widely$\"          #> [235] \"^wither$\"          \"^yet$\"             #>  #> $conj #>  [1] \"^also$\"     \"^altho$\"    \"^although$\" \"^and$\"      \"^b/c$\"      #>  [6] \"^bc$\"       \"^because$\"  \"^besides$\"  \"^both$\"     \"^but$\"      #> [11] \"^'cause$\"   \"^cos$\"      \"^cuz$\"      \"^either$\"   \"^else$\"     #> [16] \"^except$\"   \"^for$\"      \"^how$\"      \"^how'\"      \"^howd$\"     #> [21] \"^howll$\"    \"^hows$\"     \"^if$\"       \"^neither$\"  \"^nor$\"      #> [26] \"^or$\"       \"^than$\"     \"^tho$\"      \"^though$\"   \"^unless$\"   #> [31] \"^unlike$\"   \"^versus$\"   \"^vs$\"       \"^when$\"     \"^when'\"     #> [36] \"^whenever$\" \"^whereas$\"  \"^whether$\"  \"^while$\"    \"^whilst$\"   #>  #> $prep #>  [1] \"^about$\"      \"^above$\"      \"^abt$\"        \"^across$\"     \"^acrost$\"     #>  [6] \"^afk$\"        \"^after$\"      \"^against$\"    \"^along$\"      \"^amid\"        #> [11] \"^among\"       \"^around$\"     \"^as$\"         \"^at$\"         \"^atop$\"       #> [16] \"^before$\"     \"^behind$\"     \"^beneath$\"    \"^beside$\"     \"^betwe\"       #> [21] \"^beyond$\"     \"^by$\"         \"^despite$\"    \"^down$\"       \"^during$\"     #> [26] \"^excluding$\"  \"^from$\"       \"^here$\"       \"^here'\"       \"^heres$\"      #> [31] \"^in$\"         \"^including$\"  \"^inside$\"     \"^into$\"       \"^minus$\"      #> [36] \"^near$\"       \"^now$\"        \"^of$\"         \"^off$\"        \"^on$\"         #> [41] \"^onto$\"       \"^out$\"        \"^outside$\"    \"^over$\"       \"^plus$\"       #> [46] \"^regarding$\"  \"^sans$\"       \"^since$\"      \"^then$\"       \"^there$\"      #> [51] \"^there'\"      \"^thered$\"     \"^therell$\"    \"^theres$\"     \"^through$\"    #> [56] \"^throughout$\" \"^thru$\"       \"^til$\"        \"^till$\"       \"^to$\"         #> [61] \"^toward\"      \"^under$\"      \"^underneath$\" \"^until$\"      \"^untill$\"     #> [66] \"^unto$\"       \"^up$\"         \"^upon$\"       \"^via$\"        \"^with$\"       #> [71] \"^within$\"     \"^without$\"    \"^worth$\"      #>  #> $auxverb #>  [1] \"^am$\"        \"^are$\"       \"^arent$\"     \"^aren't$\"    \"^be$\"        #>  [6] \"^been$\"      \"^bein$\"      \"^being$\"     \"^brb$\"       \"^can$\"       #> [11] \"^could$\"     \"^could'\"     \"^couldnt$\"   \"^couldn't$\"  \"^couldve$\"   #> [16] \"^did$\"       \"^didnt$\"     \"^didn't$\"    \"^do$\"        \"^does$\"      #> [21] \"^doesnt$\"    \"^doesn't$\"   \"^doing$\"     \"^dont$\"      \"^don't$\"     #> [26] \"^had$\"       \"^hadnt$\"     \"^hadn't$\"    \"^has$\"       \"^hasnt$\"     #> [31] \"^hasn't$\"    \"^have$\"      \"^havent$\"    \"^haven't$\"   \"^having$\"    #> [36] \"^is$\"        \"^isnt$\"      \"^isn't$\"     \"^may$\"       \"^might$\"     #> [41] \"^might'\"     \"^mightnt$\"   \"^mightn't$\"  \"^mightve$\"   \"^must$\"      #> [46] \"^mustnt$\"    \"^mustn't$\"   \"^mustve$\"    \"^ought\"      \"^shant$\"     #> [51] \"^shan't$\"    \"^sha'nt$\"    \"^shall$\"     \"^should$\"    \"^shouldnt$\"  #> [56] \"^shouldn't$\" \"^shouldve$\"  \"^was$\"       \"^wasnt$\"     \"^wasn't$\"    #> [61] \"^were$\"      \"^werent$\"    \"^weren't$\"   \"^will$\"      \"^would$\"     #> [66] \"^would'\"     \"^wouldnt\"    \"^wouldn't\"   \"^wouldve$\"   #>  #> $negate #>  [1] \"^ain't$\"     \"^aint$\"      \"^aren't$\"    \"^arent$\"     \"^can't$\"     #>  [6] \"^cannot$\"    \"^cant$\"      \"^couldn't$\"  \"^couldnt$\"   \"^didn't$\"    #> [11] \"^didnt$\"     \"^doesn't$\"   \"^doesnt$\"    \"^don't$\"     \"^dont$\"      #> [16] \"^hadn't$\"    \"^hadnt$\"     \"^hasn't$\"    \"^hasnt$\"     \"^haven't$\"   #> [21] \"^havent$\"    \"^idk$\"       \"^isn't$\"     \"^isnt$\"      \"^must'nt$\"   #> [26] \"^mustn't$\"   \"^mustnt$\"    \"^nah\"        \"^need'nt$\"   \"^needn't$\"   #> [31] \"^neednt$\"    \"^negat\"      \"^neither$\"   \"^never$\"     \"^no$\"        #> [36] \"^nobod\"      \"^noes$\"      \"^none$\"      \"^nope$\"      \"^nor$\"       #> [41] \"^not$\"       \"^nothing$\"   \"^nowhere$\"   \"^np$\"        \"^ought'nt$\"  #> [46] \"^oughtn't$\"  \"^oughtnt$\"   \"^shant$\"     \"^shan't$\"    \"^sha'nt$\"    #> [51] \"^should'nt$\" \"^shouldn't$\" \"^shouldnt$\"  \"^uh-uh$\"     \"^wasn't$\"    #> [56] \"^wasnt$\"     \"^weren't$\"   \"^werent$\"    \"^without$\"   \"^won't$\"     #> [61] \"^wont$\"      \"^wouldn't$\"  \"^wouldnt$\"   #>  #> $quant #>  [1] \"^add$\"       \"^added$\"     \"^adding$\"    \"^adds$\"      \"^all$\"       #>  [6] \"^allot$\"     \"^alot$\"      \"^amount$\"    \"^amounts$\"   \"^another$\"   #> [11] \"^any$\"       \"^approximat\" \"^average$\"   \"^bit$\"       \"^bits$\"      #> [16] \"^both$\"      \"^bunch$\"     \"^chapter$\"   \"^couple$\"    \"^doubl\"      #> [21] \"^each$\"      \"^either$\"    \"^entire\"     \"^equal\"      \"^every$\"     #> [26] \"^extra$\"     \"^few$\"       \"^fewer$\"     \"^fewest$\"    \"^group\"      #> [31] \"^inequal\"    \"^least$\"     \"^less$\"      \"^lot$\"       \"^lotof$\"     #> [36] \"^lots$\"      \"^lotsa$\"     \"^lotta$\"     \"^majority$\"  \"^many$\"      #> [41] \"^mo$\"        \"^mo'\"        \"^more$\"      \"^most$\"      \"^much$\"      #> [46] \"^mucho$\"     \"^multiple$\"  \"^nada$\"      \"^none$\"      \"^part$\"      #> [51] \"^partly$\"    \"^percent\"    \"^piece$\"     \"^pieces$\"    \"^plenty$\"    #> [56] \"^remaining$\" \"^sampl\"      \"^scarce$\"    \"^scarcer$\"   \"^scarcest$\"  #> [61] \"^section$\"   \"^segment\"    \"^series$\"    \"^several\"    \"^single$\"    #> [66] \"^singles$\"   \"^singly$\"    \"^some$\"      \"^somewhat$\"  \"^ton$\"       #> [71] \"^tons$\"      \"^total$\"     \"^triple\"     \"^tripling$\"  \"^variety$\"   #> [76] \"^various$\"   \"^whole$\"     #>  #> $interrog #>  [1] \"^how$\"       \"^how'd$\"     \"^how're$\"    \"^how's$\"     \"^howd$\"      #>  [6] \"^howre$\"     \"^hows$\"      \"^wat$\"       \"^wattt\"      \"^what$\"      #> [11] \"^what'd$\"    \"^what'll$\"   \"^what're$\"   \"^what's$\"    \"^whatd$\"     #> [16] \"^whatever$\"  \"^whatll$\"    \"^whatre$\"    \"^whatt\"      \"^when$\"      #> [21] \"^when'\"      \"^whence$\"    \"^whenever$\"  \"^where$\"     \"^where'd$\"   #> [26] \"^where's$\"   \"^wherefore$\" \"^wherever$\"  \"^whether$\"   \"^which$\"     #> [31] \"^whichever$\" \"^whither$\"   \"^who$\"       \"^who'd$\"     \"^who'll$\"    #> [36] \"^who's$\"     \"^whoever$\"   \"^wholl$\"     \"^whom$\"      \"^whomever$\"  #> [41] \"^whos$\"      \"^whose$\"     \"^whosever$\"  \"^whoso\"      \"^why$\"       #> [46] \"^why'\"       \"^whyever$\"   \"^wut$\"       #>  #> $number #>  [1] \"^billion\"  \"^doubl\"    \"^dozen\"    \"^eight\"    \"^eleven$\"  \"^fift\"     #>  [7] \"^first$\"   \"^firstly$\" \"^firsts$\"  \"^five$\"    \"^four\"     \"^half$\"    #> [13] \"^hundred\"  \"^infinit\"  \"^million\"  \"^nine\"     \"^once$\"    \"^one$\"     #> [19] \"^quarter\"  \"^second$\"  \"^seven\"    \"^single$\"  \"^six\"      \"^ten$\"     #> [25] \"^tenth$\"   \"^third$\"   \"^thirt\"    \"^thousand\" \"^three$\"   \"^trillion\" #> [31] \"^twel\"     \"^twent\"    \"^twice$\"   \"^two$\"     \"^zero$\"    \"^zillion\"  #>  #> $interjection #>  [1] \"^a+h+$\"       \"^a+w+$\"       \"^allas$\"      \"^alright\"     \"^anyhoo$\"     #>  [6] \"^anyway[ysz]\" \"^bl[eh]+$\"    \"^g+[eah]+$\"   \"^h[ah]+$\"     \"^h[hu]+$\"     #> [11] \"^h[mh]+$\"     \"^l[ol]+$\"     \"^m[hm]+$\"     \"^meh$\"        \"^o+h+$\"       #> [16] \"^o+k+$\"       \"^okie\"        \"^oo+f+$\"      \"^soo+$\"       \"^u[uh]+$\"     #> [21] \"^u+g+h+$\"     \"^w[ow]+$\"     \"^wee+ll+$\"    \"^y[aes]+$\"    \"^ya+h+$\"      #> [26] \"^yeah$\"       \"^yus+$\"       #>   # return the standard 7 category lsm categories lma_dict(1:7) #> $ppron #>  [1] \"^dae$\"        \"^dem$\"        \"^eir$\"        \"^eirself$\"    \"^em$\"         #>  [6] \"^he$\"         \"^he'\"         \"^her$\"        \"^hers$\"       \"^herself$\"    #> [11] \"^hes$\"        \"^him$\"        \"^himself$\"    \"^hir$\"        \"^hirs$\"       #> [16] \"^hirself$\"    \"^his$\"        \"^hisself$\"    \"^i$\"          \"^i'\"          #> [21] \"^id$\"         \"^idc$\"        \"^idgaf$\"      \"^idk$\"        \"^idontknow$\"  #> [26] \"^idve$\"       \"^iirc$\"       \"^iknow$\"      \"^ikr$\"        \"^ill$\"        #> [31] \"^ily$\"        \"^im$\"         \"^ima$\"        \"^imean$\"      \"^imma$\"       #> [36] \"^ive$\"        \"^lets$\"       \"^let's$\"      \"^me$\"         \"^methinks$\"   #> [41] \"^mine$\"       \"^my$\"         \"^myself$\"     \"^omfg$\"       \"^omg$\"        #> [46] \"^oneself$\"    \"^our$\"        \"^ours\"        \"^she$\"        \"^she'\"        #> [51] \"^shes$\"       \"^thee$\"       \"^their$\"      \"^their'\"      \"^theirs\"      #> [56] \"^them$\"       \"^thems\"       \"^they$\"       \"^they'\"       \"^theyd$\"      #> [61] \"^theyll$\"     \"^theyve$\"     \"^thine$\"      \"^thou$\"       \"^thoust$\"     #> [66] \"^thy$\"        \"^thyself$\"    \"^u$\"          \"^u'\"          \"^ud$\"         #> [71] \"^ull$\"        \"^ur$\"         \"^ure$\"        \"^us$\"         \"^we$\"         #> [76] \"^we'\"         \"^weve$\"       \"^y'\"          \"^ya'\"         \"^yall\"        #> [81] \"^yins$\"       \"^yinz$\"       \"^you$\"        \"^you'\"        \"^youd$\"       #> [86] \"^youll$\"      \"^your$\"       \"^youre$\"      \"^yours$\"      \"^yourself$\"   #> [91] \"^yourselves$\" \"^youve$\"      \"^zer$\"        \"^zir$\"        \"^zirs$\"       #> [96] \"^zirself$\"    \"^zis$\"        #>  #> $ipron #>  [1] \"^another$\"   \"^anybo\"      \"^anyone\"     \"^anything\"   \"^dat$\"       #>  [6] \"^de+z$\"      \"^dis$\"       \"^everyb\"     \"^everyone\"   \"^everything\" #> [11] \"^few$\"       \"^it$\"        \"^it'$\"       \"^it'\"        \"^itd$\"       #> [16] \"^itll$\"      \"^its$\"       \"^itself$\"    \"^many$\"      \"^nobod\"      #> [21] \"^nothing$\"   \"^other$\"     \"^others$\"    \"^same$\"      \"^somebo\"     #> [26] \"^somebody'\"  \"^someone\"    \"^something\"  \"^stuff$\"     \"^that$\"      #> [31] \"^that'\"      \"^thatd$\"     \"^thatll$\"    \"^thats$\"     \"^these$\"     #> [36] \"^these'\"     \"^thesed$\"    \"^thesell$\"   \"^thesere$\"   \"^thing\"      #> [41] \"^this$\"      \"^this'\"      \"^thisd$\"     \"^thisll$\"    \"^those$\"     #> [46] \"^those'\"     \"^thosed$\"    \"^thosell$\"   \"^thosere$\"   \"^what$\"      #> [51] \"^what'\"      \"^whatd$\"     \"^whatever$\"  \"^whatll$\"    \"^whats$\"     #> [56] \"^which\"      \"^who$\"       \"^who'\"       \"^whod$\"      \"^whoever$\"   #> [61] \"^wholl$\"     \"^whom$\"      \"^whomever$\"  \"^whos$\"      \"^whose$\"     #> [66] \"^whosever$\"  \"^whosoever$\" #>  #> $article #> [1] \"^a$\"   \"^an$\"  \"^da$\"  \"^teh$\" \"^the$\" #>  #> $adverb #>   [1] \"^absolutely$\"      \"^actively$\"        \"^actually$\"        #>   [4] \"^afk$\"             \"^again$\"           \"^ago$\"             #>   [7] \"^ahead$\"           \"^almost$\"          \"^already$\"         #>  [10] \"^altogether$\"      \"^always$\"          \"^angrily$\"         #>  [13] \"^anxiously$\"       \"^any$\"             \"^anymore$\"         #>  [16] \"^anyway$\"          \"^anywhere$\"        \"^apparently$\"      #>  [19] \"^automatically$\"   \"^away$\"            \"^awhile$\"          #>  [22] \"^back$\"            \"^badly$\"           \"^barely$\"          #>  [25] \"^basically$\"       \"^below$\"           \"^brietermsy$\"      #>  [28] \"^carefully$\"       \"^causiously$\"      \"^certainly$\"       #>  [31] \"^clearly$\"         \"^closely$\"         \"^coldly$\"          #>  [34] \"^commonly$\"        \"^completely$\"      \"^constantly$\"      #>  [37] \"^continually$\"     \"^correctly$\"       \"^coz$\"             #>  [40] \"^currently$\"       \"^daily$\"           \"^deeply$\"          #>  [43] \"^definitely$\"      \"^definitly$\"       \"^deliberately$\"    #>  [46] \"^desperately$\"     \"^differently$\"     \"^directly$\"        #>  [49] \"^early$\"           \"^easily$\"          \"^effectively$\"     #>  [52] \"^elsewhere$\"       \"^enough$\"          \"^entirely$\"        #>  [55] \"^equally$\"         \"^especially$\"      \"^essentially$\"     #>  [58] \"^etc$\"             \"^even$\"            \"^eventually$\"      #>  [61] \"^ever$\"            \"^every$\"           \"^everyday$\"        #>  [64] \"^everywhere\"       \"^exactly$\"         \"^exclusively$\"     #>  [67] \"^extremely$\"       \"^fairly$\"          \"^far$\"             #>  [70] \"^finally$\"         \"^fortunately$\"     \"^frequently$\"      #>  [73] \"^fully$\"           \"^further$\"         \"^generally$\"       #>  [76] \"^gently$\"          \"^genuinely$\"       \"^good$\"            #>  [79] \"^greatly$\"         \"^hardly$\"          \"^heavily$\"         #>  [82] \"^hence$\"           \"^henceforth$\"      \"^hereafter$\"       #>  [85] \"^herein$\"          \"^heretofore$\"      \"^hesitantly$\"      #>  [88] \"^highly$\"          \"^hither$\"          \"^hopefully$\"       #>  [91] \"^hotly$\"           \"^however$\"         \"^immediately$\"     #>  [94] \"^importantly$\"     \"^increasingly$\"    \"^incredibly$\"      #>  [97] \"^indeed$\"          \"^initially$\"       \"^instead$\"         #> [100] \"^intensely$\"       \"^jus$\"             \"^just$\"            #> [103] \"^largely$\"         \"^lately$\"          \"^least$\"           #> [106] \"^legitimately$\"    \"^less$\"            \"^lightly$\"         #> [109] \"^likely$\"          \"^literally$\"       \"^loudly$\"          #> [112] \"^luckily$\"         \"^mainly$\"          \"^maybe$\"           #> [115] \"^meanwhile$\"       \"^merely$\"          \"^more$\"            #> [118] \"^moreover$\"        \"^most$\"            \"^mostly$\"          #> [121] \"^much$\"            \"^namely$\"          \"^naturally$\"       #> [124] \"^nearly$\"          \"^necessarily$\"     \"^nervously$\"       #> [127] \"^never$\"           \"^nevertheless$\"    \"^no$\"              #> [130] \"^nonetheless$\"     \"^normally$\"        \"^not$\"             #> [133] \"^notwithstanding$\" \"^obviously$\"       \"^occasionally$\"    #> [136] \"^often$\"           \"^once$\"            \"^only$\"            #> [139] \"^originally$\"      \"^otherwise$\"       \"^overall$\"         #> [142] \"^particularly$\"    \"^passionately$\"    \"^perfectly$\"       #> [145] \"^perhaps$\"         \"^personally$\"      \"^physically$\"      #> [148] \"^please$\"          \"^possibly$\"        \"^potentially$\"     #> [151] \"^practically$\"     \"^presently$\"       \"^previously$\"      #> [154] \"^primarily$\"       \"^probability$\"     \"^probably$\"        #> [157] \"^profoundly$\"      \"^prolly$\"          \"^properly$\"        #> [160] \"^quickly$\"         \"^quietly$\"         \"^quite$\"           #> [163] \"^randomly$\"        \"^rarely$\"          \"^rather$\"          #> [166] \"^readily$\"         \"^really$\"          \"^recently$\"        #> [169] \"^regularly$\"       \"^relatively$\"      \"^respectively$\"    #> [172] \"^right$\"           \"^roughly$\"         \"^sadly$\"           #> [175] \"^seldomly$\"        \"^seriously$\"       \"^shortly$\"         #> [178] \"^significantly$\"   \"^similarly$\"       \"^simply$\"          #> [181] \"^slightly$\"        \"^slowly$\"          \"^so$\"              #> [184] \"^some$\"            \"^somehow$\"         \"^sometimes$\"       #> [187] \"^somewhat$\"        \"^somewhere$\"       \"^soon$\"            #> [190] \"^specifically$\"    \"^still$\"           \"^strongly$\"        #> [193] \"^subsequently$\"    \"^successfully$\"    \"^such$\"            #> [196] \"^suddenly$\"        \"^supposedly$\"      \"^surely$\"          #> [199] \"^surprisingly$\"    \"^technically$\"     \"^terribly$\"        #> [202] \"^thence$\"          \"^thereafter$\"      \"^therefor$\"        #> [205] \"^therefore$\"       \"^thither$\"         \"^thoroughly$\"      #> [208] \"^thus$\"            \"^thusfar$\"         \"^thusly$\"          #> [211] \"^together$\"        \"^too$\"             \"^totally$\"         #> [214] \"^truly$\"           \"^typically$\"       \"^ultimately$\"      #> [217] \"^uncommonly$\"      \"^unfortunately$\"   \"^unfortunatly$\"    #> [220] \"^usually$\"         \"^vastly$\"          \"^very$\"            #> [223] \"^virtually$\"       \"^well$\"            \"^whence$\"          #> [226] \"^where\"            \"^wherefor\"         \"^whither$\"         #> [229] \"^wholly$\"          \"^why$\"             \"^why'\"             #> [232] \"^whyd$\"            \"^whys$\"            \"^widely$\"          #> [235] \"^wither$\"          \"^yet$\"             #>  #> $conj #>  [1] \"^also$\"     \"^altho$\"    \"^although$\" \"^and$\"      \"^b/c$\"      #>  [6] \"^bc$\"       \"^because$\"  \"^besides$\"  \"^both$\"     \"^but$\"      #> [11] \"^'cause$\"   \"^cos$\"      \"^cuz$\"      \"^either$\"   \"^else$\"     #> [16] \"^except$\"   \"^for$\"      \"^how$\"      \"^how'\"      \"^howd$\"     #> [21] \"^howll$\"    \"^hows$\"     \"^if$\"       \"^neither$\"  \"^nor$\"      #> [26] \"^or$\"       \"^than$\"     \"^tho$\"      \"^though$\"   \"^unless$\"   #> [31] \"^unlike$\"   \"^versus$\"   \"^vs$\"       \"^when$\"     \"^when'\"     #> [36] \"^whenever$\" \"^whereas$\"  \"^whether$\"  \"^while$\"    \"^whilst$\"   #>  #> $prep #>  [1] \"^about$\"      \"^above$\"      \"^abt$\"        \"^across$\"     \"^acrost$\"     #>  [6] \"^afk$\"        \"^after$\"      \"^against$\"    \"^along$\"      \"^amid\"        #> [11] \"^among\"       \"^around$\"     \"^as$\"         \"^at$\"         \"^atop$\"       #> [16] \"^before$\"     \"^behind$\"     \"^beneath$\"    \"^beside$\"     \"^betwe\"       #> [21] \"^beyond$\"     \"^by$\"         \"^despite$\"    \"^down$\"       \"^during$\"     #> [26] \"^excluding$\"  \"^from$\"       \"^here$\"       \"^here'\"       \"^heres$\"      #> [31] \"^in$\"         \"^including$\"  \"^inside$\"     \"^into$\"       \"^minus$\"      #> [36] \"^near$\"       \"^now$\"        \"^of$\"         \"^off$\"        \"^on$\"         #> [41] \"^onto$\"       \"^out$\"        \"^outside$\"    \"^over$\"       \"^plus$\"       #> [46] \"^regarding$\"  \"^sans$\"       \"^since$\"      \"^then$\"       \"^there$\"      #> [51] \"^there'\"      \"^thered$\"     \"^therell$\"    \"^theres$\"     \"^through$\"    #> [56] \"^throughout$\" \"^thru$\"       \"^til$\"        \"^till$\"       \"^to$\"         #> [61] \"^toward\"      \"^under$\"      \"^underneath$\" \"^until$\"      \"^untill$\"     #> [66] \"^unto$\"       \"^up$\"         \"^upon$\"       \"^via$\"        \"^with$\"       #> [71] \"^within$\"     \"^without$\"    \"^worth$\"      #>  #> $auxverb #>  [1] \"^am$\"        \"^are$\"       \"^arent$\"     \"^aren't$\"    \"^be$\"        #>  [6] \"^been$\"      \"^bein$\"      \"^being$\"     \"^brb$\"       \"^can$\"       #> [11] \"^could$\"     \"^could'\"     \"^couldnt$\"   \"^couldn't$\"  \"^couldve$\"   #> [16] \"^did$\"       \"^didnt$\"     \"^didn't$\"    \"^do$\"        \"^does$\"      #> [21] \"^doesnt$\"    \"^doesn't$\"   \"^doing$\"     \"^dont$\"      \"^don't$\"     #> [26] \"^had$\"       \"^hadnt$\"     \"^hadn't$\"    \"^has$\"       \"^hasnt$\"     #> [31] \"^hasn't$\"    \"^have$\"      \"^havent$\"    \"^haven't$\"   \"^having$\"    #> [36] \"^is$\"        \"^isnt$\"      \"^isn't$\"     \"^may$\"       \"^might$\"     #> [41] \"^might'\"     \"^mightnt$\"   \"^mightn't$\"  \"^mightve$\"   \"^must$\"      #> [46] \"^mustnt$\"    \"^mustn't$\"   \"^mustve$\"    \"^ought\"      \"^shant$\"     #> [51] \"^shan't$\"    \"^sha'nt$\"    \"^shall$\"     \"^should$\"    \"^shouldnt$\"  #> [56] \"^shouldn't$\" \"^shouldve$\"  \"^was$\"       \"^wasnt$\"     \"^wasn't$\"    #> [61] \"^were$\"      \"^werent$\"    \"^weren't$\"   \"^will$\"      \"^would$\"     #> [66] \"^would'\"     \"^wouldnt\"    \"^wouldn't\"   \"^wouldve$\"   #>   # return just a few categories without regular expression lma_dict(neg, ppron, aux, as.regex = FALSE) #> $ppron #>  [1] \"dae\"        \"dem\"        \"eir\"        \"eirself\"    \"em\"         #>  [6] \"he\"         \"he'*\"       \"her\"        \"hers\"       \"herself\"    #> [11] \"hes\"        \"him\"        \"himself\"    \"hir\"        \"hirs\"       #> [16] \"hirself\"    \"his\"        \"hisself\"    \"i\"          \"i'*\"        #> [21] \"id\"         \"idc\"        \"idgaf\"      \"idk\"        \"idontknow\"  #> [26] \"idve\"       \"iirc\"       \"iknow\"      \"ikr\"        \"ill\"        #> [31] \"ily\"        \"im\"         \"ima\"        \"imean\"      \"imma\"       #> [36] \"ive\"        \"lets\"       \"let's\"      \"me\"         \"methinks\"   #> [41] \"mine\"       \"my\"         \"myself\"     \"omfg\"       \"omg\"        #> [46] \"oneself\"    \"our\"        \"ours*\"      \"she\"        \"she'*\"      #> [51] \"shes\"       \"thee\"       \"their\"      \"their'*\"    \"theirs*\"    #> [56] \"them\"       \"thems*\"     \"they\"       \"they'*\"     \"theyd\"      #> [61] \"theyll\"     \"theyve\"     \"thine\"      \"thou\"       \"thoust\"     #> [66] \"thy\"        \"thyself\"    \"u\"          \"u'*\"        \"ud\"         #> [71] \"ull\"        \"ur\"         \"ure\"        \"us\"         \"we\"         #> [76] \"we'*\"       \"weve\"       \"y'*\"        \"ya'*\"       \"yall*\"      #> [81] \"yins\"       \"yinz\"       \"you\"        \"you'*\"      \"youd\"       #> [86] \"youll\"      \"your\"       \"youre\"      \"yours\"      \"yourself\"   #> [91] \"yourselves\" \"youve\"      \"zer\"        \"zir\"        \"zirs\"       #> [96] \"zirself\"    \"zis\"        #>  #> $auxverb #>  [1] \"am\"        \"are\"       \"arent\"     \"aren't\"    \"be\"        \"been\"      #>  [7] \"bein\"      \"being\"     \"brb\"       \"can\"       \"could\"     \"could'*\"   #> [13] \"couldnt\"   \"couldn't\"  \"couldve\"   \"did\"       \"didnt\"     \"didn't\"    #> [19] \"do\"        \"does\"      \"doesnt\"    \"doesn't\"   \"doing\"     \"dont\"      #> [25] \"don't\"     \"had\"       \"hadnt\"     \"hadn't\"    \"has\"       \"hasnt\"     #> [31] \"hasn't\"    \"have\"      \"havent\"    \"haven't\"   \"having\"    \"is\"        #> [37] \"isnt\"      \"isn't\"     \"may\"       \"might\"     \"might'*\"   \"mightnt\"   #> [43] \"mightn't\"  \"mightve\"   \"must\"      \"mustnt\"    \"mustn't\"   \"mustve\"    #> [49] \"ought*\"    \"shant\"     \"shan't\"    \"sha'nt\"    \"shall\"     \"should\"    #> [55] \"shouldnt\"  \"shouldn't\" \"shouldve\"  \"was\"       \"wasnt\"     \"wasn't\"    #> [61] \"were\"      \"werent\"    \"weren't\"   \"will\"      \"would\"     \"would'*\"   #> [67] \"wouldnt*\"  \"wouldn't*\" \"wouldve\"   #>  #> $negate #>  [1] \"ain't\"     \"aint\"      \"aren't\"    \"arent\"     \"can't\"     \"cannot\"    #>  [7] \"cant\"      \"couldn't\"  \"couldnt\"   \"didn't\"    \"didnt\"     \"doesn't\"   #> [13] \"doesnt\"    \"don't\"     \"dont\"      \"hadn't\"    \"hadnt\"     \"hasn't\"    #> [19] \"hasnt\"     \"haven't\"   \"havent\"    \"idk\"       \"isn't\"     \"isnt\"      #> [25] \"must'nt\"   \"mustn't\"   \"mustnt\"    \"nah*\"      \"need'nt\"   \"needn't\"   #> [31] \"neednt\"    \"negat*\"    \"neither\"   \"never\"     \"no\"        \"nobod*\"    #> [37] \"noes\"      \"none\"      \"nope\"      \"nor\"       \"not\"       \"nothing\"   #> [43] \"nowhere\"   \"np\"        \"ought'nt\"  \"oughtn't\"  \"oughtnt\"   \"shant\"     #> [49] \"shan't\"    \"sha'nt\"    \"should'nt\" \"shouldn't\" \"shouldnt\"  \"uh-uh\"     #> [55] \"wasn't\"    \"wasnt\"     \"weren't\"   \"werent\"    \"without\"   \"won't\"     #> [61] \"wont\"      \"wouldn't\"  \"wouldnt\"   #>   # return special specifically lma_dict(special) #> $special #> $special$ELLIPSIS #> [1] \"\\\\.{3, }|\\\\. +\\\\. +[. ]+\" #>  #> $special$SMILE #> [1] \"\\\\s(?:[[{(<qd]+[\\\\s<-]*[;:8=]|[;:8=][\\\\s>-]*[]})>Dpb]+|[uUnwWmM^=+-]_[uUnwWmM^=+-])(?=\\\\s)\" #>  #> $special$FROWN #> [1] \"\\\\s(?:[]D)}>]+[\\\\s.,<-]*[;:8=]|[;:8=][\\\\s.,>-]*[[{(<]+|[Tt:;]_[Tt;:]|[uUtT;:][mMn][uUtT;:])(?=\\\\s)\" #>  #> $special$LIKE #>  [1] \"(?<=could not) like\\\\b\"  \"(?<=did not) like\\\\b\"    #>  [3] \"(?<=did) like\\\\b\"        \"(?<=didn't) like\\\\b\"     #>  [5] \"(?<=do not) like\\\\b\"     \"(?<=do) like\\\\b\"         #>  [7] \"(?<=does not) like\\\\b\"   \"(?<=does) like\\\\b\"       #>  [9] \"(?<=doesn't) like\\\\b\"    \"(?<=don't) like\\\\b\"      #> [11] \"(?<=i) like\\\\b\"          \"(?<=should not) like\\\\b\" #> [13] \"(?<=they) like\\\\b\"       \"(?<=we) like\\\\b\"         #> [15] \"(?<=will not) like\\\\b\"   \"(?<=will) like\\\\b\"       #> [17] \"(?<=won't) like\\\\b\"      \"(?<=would not) like\\\\b\"  #> [19] \"(?<=you) like\\\\b\"        #>  #> $special$CHARACTERS #>                                                           #>                                                    \"\\\\s\"  #>                                                        '  #>                                      \"[´‘’‚‛′‵ʹʻʾʿˈˊˋ˴̡̢̨̛̦̩̀́̍̒̓̔̀́̓͑͗̕]\"  #>                                                        \"  #>                                       \"[“”„‟″‴‶‷⁗ʺ˝ˮ˵˶̋̏]\"  #>                                                      ...  #>                                                      \"…\"  #>                                                        -  #>                                          \"[־᠆‐‑–﹘﹣－]\"  #>                                                       -   #>                                            \"[‒—―⸺⸻]|--+\"  #>                                                        a  #>                    \"[ÀÁÂÃÄÅàáâãäåĀāĂăĄąȀȁȂȃȦȧɅɐɑɒɕͣΆΑАа]\"  #>                                                       ae  #>                                                \"[ÆæŒœɶ]\"  #>                                                        b  #>                           \"[ßƀƁƂƃƄƅƆƇƈƉƊƋƌɃɓʙБВбвѢѣҔҕℬ]\"  #>                                                        c  #>                                      \"[ÇçĆćĈĉƆƇƈɔʗͨСсℂ℃]\"  #>                                                        d  #>                                   \"[ÐÞþčĎďĐđƉȡɖɖɗͩΒдԀⅅⅆ]\"  #>                                                        e  #> \"[ÈÉÊËèéêëĒēĔĕĖėĘęĚěƎƏƐȄȅȆȇȨȩɆɇɘəͤΈΕЀЁЄЕЗезѐёєҘҙℇ℈ℨ℮ℯℰⅇ]\"  #>                                                        f  #>                                             \"[ƑƒҒғ℉∱Ⅎⅎ]\"  #>                                                        g  #>                                      \"[ĜĝĞğĠġĢģƓȢɠɡɢℊ⅁]\"  #>                                                        h  #>                                       \"[ĤĥħƕɦɧΉΗђℋℌℍℎℏ]\"  #>                                                        i  #>                         \"[ÌÍÎÏìíîïĨĩĪīĬĭĮįİıƗƚȈȉͥΐΙІЇії]\"  #>                                                        j  #>                                           \"[ĵȶȷɈɉЈј℩ℹⅉ]\"  #>                                                        k  #>                                                \"[ķĸƘƙK]\"  #>                                                        l  #>                                          \"[ĹĺĻļĽľĿŀŁłȴ]\"  #>                                                        m  #>                                                  \"[ɱѠℳ]\"  #>                                                        n  #>                             \"[ÑñŃńŅņŇňŉŊŋȠȵɲɳɴͶͷИЙийℕℵ]\"  #>                                                        h  #>                                                      \"ŉ\"  #>                                                        o  #>                      \"[ÒÓÔÕÖØðòóôõöøŌōŎŏŐőŐőȰȱɵʘͦΘФфѲѳℴ]\"  #>                                                        p  #>                                                \"[Рр℗℘ℙ]\"  #>                                                        q  #>                                                  \"[ƍℚ℺]\"  #>                                                        r  #>                                \"[ŔŕŖŗŘřȑȒȓɹʀʁгѓҐґℛℜℝ℟ℾ]\"  #>                                                        s  #>                                        \"[ŚŜŝŞşŠšŠšȘșЅѕ]\"  #>                                                        t  #>                                           \"[ŢţŤťŦŧͱͳТт]\"  #>                                                        u  #>                \"[ÙÚÛÜùúûüüŨũŪūŬŭŮůŰűŲųǓǔǕǖǗǘǙǚǛǜȔȗɄʉͧЦц]\"  #>                                                        v  #>                                                 \"[ѴѵѶѷ]\"  #>                                                        w  #>                                             \"[ŴŵɰШЩшщѡ]\"  #>                                                        y  #>                                         \"[ÝýÿŶŷŸȲȳУЧуч]\"  #>                                                        z  #>                                         \"[ŹźŻżžȤȥɀʐʑΖℤ]\"  #>                                                        x  #>                                              \"[×ЖХжхҖҗ]\"  #>  #> $special$SYMBOLS #>   (cc) number     sm    tel   (tm)  omega  alpha    fax     pi  sigma  #>    \"©\"    \"№\"    \"℠\"    \"℡\"    \"™\"    \"Ω\"    \"℧\"    \"℻\" \"[ℼℿ]\"    \"⅀\"  #>  #>   # returning a function is.ppron <- lma_dict(ppron, as.function = TRUE) is.ppron(c(\"i\", \"am\", \"you\", \"were\")) #> [1]  TRUE FALSE  TRUE FALSE  in.lsmcat <- lma_dict(1:7, as.function = TRUE) in.lsmcat(c(\"a\", \"frog\", \"for\", \"me\")) #> [1]  TRUE FALSE  TRUE  TRUE  ## use as a stopword filter is.stopword <- lma_dict(as.function = TRUE) dtm <- lma_dtm(\"Most of these words might not be all that relevant.\") dtm[, !is.stopword(colnames(dtm))] #> relevant    words  #>        1        1   ## use to replace special characters clean <- lma_dict(special, as.function = gsub) clean(c(   \"\\u201Ccurly quotes\\u201D\", \"na\\u00EFve\", \"typographer\\u2019s apostrophe\",   \"en\\u2013dash\", \"em\\u2014dash\" )) #> [1] \"\\\"curly quotes\\\"\"         \"naive\"                    #> [3] \"typographer's apostrophe\" \"en-dash\"                  #> [5] \"em - dash\""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dtm.html","id":null,"dir":"Reference","previous_headings":"","what":"Document-Term Matrix Creation — lma_dtm","title":"Document-Term Matrix Creation — lma_dtm","text":"Creates document-term matrix (dtm) set texts.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dtm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Document-Term Matrix Creation — lma_dtm","text":"","code":"lma_dtm(text, exclude = NULL, context = NULL, replace.special = FALSE,   numbers = FALSE, punct = FALSE, urls = TRUE, emojis = FALSE,   to.lower = TRUE, word.break = \" +\", dc.min = 0, dc.max = Inf,   sparse = TRUE, tokens.only = FALSE)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_dtm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Document-Term Matrix Creation — lma_dtm","text":"text Texts processed. can vector (column data frame) list. exclude character vector words excluded. exclude single string matching 'function', lma_dict(1:9) used. context character vector used reformat text based look- ahead/behind. example, might attempt disambiguate like reformatting certain likes (e.g., context = c('() like*', '() like*', '() like'), words parentheses context target word, asterisks denote partial matching). converted regular expression (.e., '(? <= ) like\\\\b') , matched, replaced coded version word (e.g., \"Hey, like !\" become \"Hey, -like !\"). probably useful categorization, dictionary include one another version word (e.g., LIWC 2015 dictionary something like like, LIWC 2007 something like kind (), try clean posemo category). replace.special Logical: TRUE, special characters replaced regular equivalents using lma_dict special function. numbers Logical: TRUE, numbers preserved. punct Logical: TRUE, punctuation preserved. urls Logical: FALSE, attempts replace urls \"repurl\". emojis Logical: TRUE, attempts replace emojis (e.g., \":(\" replaced \"repfrown\"). .lower Logical: FALSE, words different capitalization treated different terms. word.break regular expression string determining way words split. Default ' +' breaks words one blank spaces. may also like break dashes slashes ('[ /-]+'), depending text. dc.min Numeric: excludes terms appearing set number fewer documents. Default 0 (limit). dc.max Numeric: excludes terms appearing set number . Default Inf (limit). sparse Logical: FALSE, regular dense matrix returned. tokens.Logical: TRUE, returns list rather matrix, entries:","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dtm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Document-Term Matrix Creation — lma_dtm","text":"sparse matrix (regular matrix sparse = FALSE), row per text, column per term, list tokens.= TRUE. Includes attribute options (opts), attributes word count (WC) column sums (colsums) tokens.= FALSE.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dtm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Document-Term Matrix Creation — lma_dtm","text":"relatively simple way make dtm. calculate (less) standard forms LSM LSS, somewhat raw dtm fine, processes essentially use dictionaries (obviating stemming) weighting categorization (largely obviating 'stop word' removal). exact effect additional processing depend dictionary/semantic space weighting scheme used (particularly LSA). function also processing may matter plan categorizing categories terms look- ahead/behind assertions (like LIWC dictionaries). Otherwise, methods may faster, memory efficient, /featureful.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_dtm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Document-Term Matrix Creation — lma_dtm","text":"","code":"text <- c(   \"Why, hello there! How are you this evening?\",   \"I am well, thank you for your inquiry!\",   \"You are a most good at social interactions person!\",   \"Why, thank you! You're not all bad yourself!\" )  lma_dtm(text) #> 4 x 27 sparse Matrix of class \"dgCMatrix\" #>    [[ suppressing 27 column names 'a', 'all', 'am' ... ]] #>                                                            #> [1,] . . . 1 . . 1 . . 1 1 . . . . . . . . 1 1 . 1 1 . . . #> [2,] . . 1 . . . . 1 . . . 1 1 . . . . . 1 . . 1 . 1 . 1 . #> [3,] 1 . . 1 1 . . . 1 . . . . 1 1 . 1 1 . . . . . 1 . . . #> [4,] . 1 . . . 1 . . . . . . . . . 1 . . 1 . . . 1 1 1 . 1"},{"path":"https://miserman.github.io/lingmatch/reference/lma_initdirs.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize Directories for Dictionaries and Latent Semantic Spaces — lma_initdirs","title":"Initialize Directories for Dictionaries and Latent Semantic Spaces — lma_initdirs","text":"Creates directories dictionaries latent semantic spaces needed, sets lingmatch.dict.dir lingmatch.lspace.dir options already set, creates links expected locations ('~/Dictionaries' '~/Latent Semantic Spaces') default applicable.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_initdirs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize Directories for Dictionaries and Latent Semantic Spaces — lma_initdirs","text":"","code":"lma_initdirs(base = \"\", dict = \"Dictionaries\",   lspace = \"Latent Semantic Spaces\", link = TRUE)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_initdirs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize Directories for Dictionaries and Latent Semantic Spaces — lma_initdirs","text":"base Path directory create dict lspace subdirectories. dict Path dictionaries directory relative base. lspace Path latent semantic spaces directory relative base. link Logical; TRUE (default), full dict /lspace paths exist (potentially created), '~/Dictionaries' '~/Latent Semantic Spaces' respectively, junctions (Windows) symbolic links created: ~/Dictionaries <<===>> dict ~/Latent Semantic Spaces <<===>> lspace.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_initdirs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize Directories for Dictionaries and Latent Semantic Spaces — lma_initdirs","text":"Paths [1] dictionaries [2] latent semantic space directories, single path dict lspace specified.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_initdirs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize Directories for Dictionaries and Latent Semantic Spaces — lma_initdirs","text":"","code":"if (FALSE) {  # set up the expected dictionary and latent semantic space directories lma_initdirs(\"~\")  # set up directories elsewhere, and links to the expected locations lma_initdirs(\"d:\")  # point options and create links to preexisting directories lma_initdirs(\"~/NLP_Resources\", \"Dicts\", \"Dicts/Embeddings\")  # create just a dictionaries directory and set the # lingmatch.dict.dir option without creating a link lma_initdirs(dict = \"z:/external_dictionaries\", link = FALSE) }"},{"path":"https://miserman.github.io/lingmatch/reference/lma_lspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Latent Semantic Space (Embeddings) Operations — lma_lspace","title":"Latent Semantic Space (Embeddings) Operations — lma_lspace","text":"Map document-term matrix onto latent semantic space, extract terms latent semantic space (dtm character vector, map.space = FALSE), perform singular value decomposition document-term matrix (dtm matrix space missing).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_lspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Latent Semantic Space (Embeddings) Operations — lma_lspace","text":"","code":"lma_lspace(dtm = \"\", space, map.space = TRUE, fill.missing = FALSE,   term.map = NULL, dim.cutoff = 0.5, keep.dim = FALSE,   use.scan = FALSE, dir = getOption(\"lingmatch.lspace.dir\"))"},{"path":"https://miserman.github.io/lingmatch/reference/lma_lspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Latent Semantic Space (Embeddings) Operations — lma_lspace","text":"dtm matrix terms column names, character vector terms extracted specified space. length 1 space missing, treated space. space matrix terms rownames. missing, right singular vectors singular value decomposition dtm. character, file matching character searched dir (e.g., space = 'google'). file found character matches one available spaces, given option download , handled download.lspace. dtm missing, entire space loaded returned. map.space Logical: FALSE, original vectors space terms found dtm returned. Otherwise dtm %*% space returned, excluding uncommon columns dtm rows space. fill.missing Logical: TRUE terms extracted space, includes terms found space rows 0s, returned matrix row every requested term. term.map matrix space column name, terms row names, indices terms given space values, numeric vector indices terms names, character vector terms corresponding rows space. used instead reading \"_terms.txt\" file corresponding space entered character (name space file). dim.cutoff space calculated, used decide number dimensions retained: cumsum(d) / sum(d) < dim.cutoff, d vector singular values dtm (.e., svd(dtm)$d). default .5; lower cutoffs result fewer dimensions. keep.dim Logical: TRUE, space calculated input, matrix dimensions dtm returned. Otherwise, matrix terms rows dimensions columns returned. use.scan Logical: TRUE, reads rows space scan. dir Path folder containing spaces.  Set session default options(lingmatch.lspace.dir = 'desired/path').","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_lspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Latent Semantic Space (Embeddings) Operations — lma_lspace","text":"matrix sparse matrix either () row per term column per latent dimension (latent space, either calculated input, retrieved map.space = FALSE), (b) row per document column per latent dimension (dtm mapped space), (c) row per document column per term (space calculated keep.dim = TRUE).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_lspace.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Latent Semantic Space (Embeddings) Operations — lma_lspace","text":"traditional latent semantic space selection right singular vectors singular value decomposition dtm (svd(dtm)$v[, 1:k], k selected number dimensions, decided dim.cutoff). Mapping new dtm latent semantic space consists multiplying common terms: dtm[, ct] %*% space[ct, ], ct = colnames(dtm)[colnames(dtm) %% rownames(space)] -- terms common dtm space. results matrix documents rows, dimensions columns, replacing terms.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/lma_lspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Latent Semantic Space (Embeddings) Operations — lma_lspace","text":"","code":"text <- c(   paste(     \"Hey, I like kittens. I think all kinds of cats really are just the\",     \"best pet ever.\"   ),   paste(     \"Oh year? Well I really like cars. All the wheels and the turbos...\",     \"I think that's the best ever.\"   ),   paste(     \"You know what? Poo on you. Cats, dogs, rabbits -- you know, living\",     \"creatures... to think you'd care about anything else!\"   ),   paste(     \"You can stick to your opinion. You can be wrong if you want. You know\",     \"what life's about? Supercharging, diesel guzzling, exhaust spewing,\",     \"piston moving ignitions.\"   ) )  dtm <- lma_dtm(text)  # calculate a latent semantic space from the example text lss <- lma_lspace(dtm)  # show that document similarities between the truncated and full space are the same spaces <- list(   full = lma_lspace(dtm, keep.dim = TRUE),   truncated = lma_lspace(dtm, lss) ) sapply(spaces, lma_simets, metric = \"cosine\") #> $full #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                         #> [1,] I            .          .        . #> [2,] 0.999420475  I          .        . #> [3,] 0.140738442  0.10695580 I        . #> [4,] 0.001947292 -0.03209365 0.990319 I #>  #> $truncated #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                         #> [1,] I            .          .        . #> [2,] 0.999420475  I          .        . #> [3,] 0.140738442  0.10695580 I        . #> [4,] 0.001947292 -0.03209365 0.990319 I #>   if (FALSE) {  # specify a directory containing spaces, # or where you would like to download spaces space_dir <- \"~/Latent Semantic Spaces\"  # map to a pretrained space ddm <- lma_lspace(dtm, \"100k\", dir = space_dir)  # load the matching subset of the space # without mapping lss_100k_part <- lma_lspace(colnames(dtm), \"100k\", dir = space_dir)  ## or lss_100k_part <- lma_lspace(dtm, \"100k\", map.space = FALSE, dir = space_dir)  # load the full space lss_100k <- lma_lspace(\"100k\", dir = space_dir)  ## or lss_100k <- lma_lspace(space = \"100k\", dir = space_dir) }"},{"path":"https://miserman.github.io/lingmatch/reference/lma_meta.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Text-Based Metastatistics — lma_meta","title":"Calculate Text-Based Metastatistics — lma_meta","text":"Calculate simple descriptive statistics text.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_meta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Text-Based Metastatistics — lma_meta","text":"","code":"lma_meta(text)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_meta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Text-Based Metastatistics — lma_meta","text":"text character vector texts.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_meta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Text-Based Metastatistics — lma_meta","text":"data.frame:  characters: Total number characters. syllables: Total number syllables, estimated split length '+[eu]*|e+*|+|o+[ui]*|u+|y+[aeiou]*' - 1. words: Total number words (raw word count). unique_words: Number unique words (binary word count). clauses: Number clauses, marked commas, colons, semicolons, dashes, brackets       within sentences. sentences: Number sentences, marked periods, question marks, exclamation points,       new line characters. words_per_clause: Average number words per clause. words_per_sentence: Average number words per sentence. sixltr: Number words 6 characters long. characters_per_word: Average number characters per word       (characters / words). syllables_per_word: Average number syllables per word       (syllables / words). type_token_ratio: Ratio unique total words: unique_words / words. reading_grade: Flesch-Kincaid grade level: .39 * words / sentences +       11.8 * syllables / words - 15.59. numbers: Number terms starting numbers. punct: Number terms starting non-alphanumeric characters. periods: Number periods. commas: Number commas. qmarks: Number question marks. exclams: Number exclamation points. quotes: Number quotation marks (single double). apostrophes: Number apostrophes, defined modified letter apostrophe, backtick       single straight curly quote surrounded letters. brackets: Number bracketing characters (including parentheses, square,       curly, angle brackets). orgmarks: Number characters used organization structuring (including       dashes, foreword slashes, colons, semicolons).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_meta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Text-Based Metastatistics — lma_meta","text":"","code":"text <- c(   succinct = \"It is here.\",   verbose = \"Hear me now. I shall tell you about it. It is here. Do you hear?\",   couched = \"I might be wrong, but it seems to me that it might be here.\",   bigwords = \"Object located thither.\",   excited = \"It's there! It's there! It's there!\",   drippy = \"It's 'there', right? Not 'here'? 'there'? Are you Sure?\",   struggly = \"It's here -- in that place where it is. Like... the 1st place (here).\" ) lma_meta(text) #>          characters syllables words unique_words clauses sentences #> succinct          8         3     3            3       1         1 #> verbose          46        16    15           12       4         4 #> couched          44        14    14           11       2         1 #> bigwords         20         7     3            3       1         1 #> excited          27         6     6            2       3         3 #> drippy           36         9     9            8       5         4 #> struggly         44        12    12           10       3         2 #>          words_per_clause words_per_sentence sixltr characters_per_word #> succinct             3.00               3.00      0            2.666667 #> verbose              3.75               3.75      0            3.066667 #> couched              7.00              14.00      0            3.142857 #> bigwords             3.00               3.00      3            6.666667 #> excited              2.00               2.00      0            4.500000 #> drippy               1.80               2.25      0            4.000000 #> struggly             4.00               6.00      0            3.666667 #>          syllables_per_word type_token_ratio reading_grade numbers puncts #> succinct           1.000000        1.0000000     -2.620000       0      1 #> verbose            1.066667        0.8000000     -1.540833       0      4 #> couched            1.000000        0.7857143      1.670000       0      2 #> bigwords           2.333333        1.0000000     13.113333       0      1 #> excited            1.000000        0.3333333     -3.010000       0      3 #> drippy             1.000000        0.8888889     -2.912500       0     11 #> struggly           1.000000        0.8333333     -1.450000       1      8 #>          periods commas qmarks exclams quotes apostrophes brackets orgmarks #> succinct       1      0      0       0      0           0        0        0 #> verbose        3      0      1       0      0           0        0        0 #> couched        1      1      0       0      0           0        0        0 #> bigwords       1      0      0       0      0           0        0        0 #> excited        0      0      0       3      0           3        0        0 #> drippy         0      1      4       0      6           1        0        0 #> struggly       5      0      0       0      0           1        2        1"},{"path":"https://miserman.github.io/lingmatch/reference/lma_patcat.html","id":null,"dir":"Reference","previous_headings":"","what":"Categorize Texts — lma_patcat","title":"Categorize Texts — lma_patcat","text":"Categorize raw texts using pattern-based dictionary.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_patcat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Categorize Texts — lma_patcat","text":"","code":"lma_patcat(text, dict = NULL, pattern.weights = \"weight\",   pattern.categories = \"category\", bias = NULL, to.lower = TRUE,   return.dtm = FALSE, drop.zeros = FALSE, exclusive = TRUE,   boundary = NULL, fixed = TRUE, globtoregex = FALSE,   name.map = c(intname = \"_intercept\", term = \"term\"),   dir = getOption(\"lingmatch.dict.dir\"))"},{"path":"https://miserman.github.io/lingmatch/reference/lma_patcat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Categorize Texts — lma_patcat","text":"text vector text categorized. Texts padded 2 spaces, potentially lowercased. dict least vector terms (patterns), usually matrix-like object columns terms, categories, weights. pattern.weights vector weights corresponding terms dict, column name weights found dict. pattern.categories vector category names corresponding terms dict, column name category names found dict. bias constant add category weighting summing. Can vector names corresponding unique values dict[, category], usually extracted dict based intercept included category (defined name.map['intname']). .lower Logical indicating whether text converted lowercase processing. return.dtm Logical; TRUE, document-term matrix returned, rather weighted, summed, biased category values. drop.zeros logical; TRUE, categories terms matches removed. exclusive Logical; FALSE, dictionary term searched original text. Otherwise (default), terms sorted length (longer terms searched first), matches removed text (avoiding subsequent matches matched patterns). boundary string add beginning end dictionary term. TRUE, boundary set ' ', avoiding pattern matches within words. default, dictionary terms left entered. fixed Logical; FALSE, patterns treated regular expressions. globtoregex Logical; TRUE, initial terminal asterisks replaced \\\\b\\\\w* \\\\w*\\\\b respectively. also set fixed FALSE unless fixed specified. name.map named character vector: intname: term identifying category biases within term list;     defaults '_intercept' term: name column containing terms dict; defaults 'term' Missing names added, names can specified positional (e.g., c('_int', 'terms')), can specified name (e.g., c(term = 'patterns')), leaving rest default. dir Path folder look dict name file passed read.dic.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_patcat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Categorize Texts — lma_patcat","text":"matrix row per text columns per dictionary category, (return.dtm = TRUE) sparse matrix row per text column per term. Includes WC attribute original word counts, categories attribute row indices associated category return.dtm = TRUE.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/lma_patcat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Categorize Texts — lma_patcat","text":"","code":"# example text text <- c(   paste(     \"Oh, what youth was! What I had and gave away.\",     \"What I took and spent and saw. What I lost. And now? Ruin.\"   ),   paste(     \"God, are you so bored?! You just want what's gone from us all?\",     \"I miss the you that was too. I love that you.\"   ),   paste(     \"Tomorrow! Tomorrow--nay, even tonight--you wait, as I am about to change.\",     \"Soon I will off to revert. Please wait.\"   ) )  # make a document-term matrix with pre-specified terms only lma_patcat(text, c(\"bored?!\", \"i lo\", \". \"), return.dtm = TRUE) #> 3 x 3 sparse Matrix of class \"dgTMatrix\" #>      bored?! i lo .  #> [1,]       .    1  4 #> [2,]       1    1  2 #> [3,]       .    .  3  # get counts of sets of letter lma_patcat(text, list(c(\"a\", \"b\", \"c\"), c(\"d\", \"e\", \"f\"))) #>      cat1 cat2 #> [1,]   14    7 #> [2,]    8    8 #> [3,]   10    9 #> attr(,\"WC\") #> [1] 21 16 19 #> attr(,\"time\") #> patcat  #>      0   # same thing with regular expressions lma_patcat(text, list(\"[abc]\", \"[def]\"), fixed = FALSE) #>      cat1 cat2 #> [1,]   14    7 #> [2,]    8    8 #> [3,]   10    9 #> attr(,\"WC\") #> [1] 21 16 19 #> attr(,\"time\") #> patcat  #>   0.02   # match only words lma_patcat(text, list(\"i\"), boundary = TRUE) #>      category #> [1,]        3 #> [2,]        2 #> [3,]        2 #> attr(,\"WC\") #> [1] 3 2 2 #> attr(,\"time\") #> patcat  #>      0   # match only words, ignoring punctuation lma_patcat(   text, c(\"you\", \"tomorrow\", \"was\"),   fixed = FALSE,   boundary = \"\\\\b\", return.dtm = TRUE ) #> 3 x 3 sparse Matrix of class \"dgTMatrix\" #>      tomorrow you was #> [1,]        .   .   1 #> [2,]        .   4   1 #> [3,]        2   1   .  if (FALSE) {  # read in the temporal orientation lexicon from the World Well-Being Project tempori <- read.csv(   \"https://wwbp.org/downloads/public_data/temporalOrientationLexicon.csv\" )  lma_patcat(text, tempori)  # or use the standardized version tempori_std <- read.dic(\"wwbp_prospection\", dir = \"~/Dictionaries\")  lma_patcat(text, tempori_std)  ## get scores on the same scale by adjusting the standardized values tempori_std[, -1] <- tempori_std[, -1] / 100 *   select.dict(\"wwbp_prospection\")$selected[, \"original_max\"]  lma_patcat(text, tempori_std)[, unique(tempori$category)] }"},{"path":"https://miserman.github.io/lingmatch/reference/lma_process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Text — lma_process","title":"Process Text — lma_process","text":"wrapper pre-processing functions, potentially read.segments, lma_dtm lma_patcat, lma_weight, lma_termcat lma_lspace, optionally including lma_meta output.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Text — lma_process","text":"","code":"lma_process(input = NULL, ..., meta = TRUE)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Text — lma_process","text":"input vector text, path text file folder. ... arguments passed lma_dtm, lma_patcat, lma_weight, lma_termcat, /lma_lspace. arguments must named. meta Logical; FALSE, metastatistics included. applies raw text available. included, meta categories added last columns, names starting \"meta_\".","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Text — lma_process","text":"matrix texts represented rows, features columns, unless multiple rows per output  (e.g., latent semantic space applied without terms mapped) case special output  returned (e.g., matrix terms rows latent dimensions columns).","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/lma_process.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process Text — lma_process","text":"","code":"# starting with some texts in a vector texts <- c(   \"Firstly, I would like to say, and with all due respect...\",   \"Please, proceed. I hope you feel you can speak freely...\",   \"Oh, of course, I just hope to be clear, and not cause offense...\",   \"Oh, no, don't monitor yourself on my account...\" )  # by default, term counts and metastatistics are returned lma_process(texts) #>                                                               text account all #> 1        Firstly, I would like to say, and with all due respect...       0   1 #> 2         Please, proceed. I hope you feel you can speak freely...       0   0 #> 3 Oh, of course, I just hope to be clear, and not cause offense...       0   0 #> 4                  Oh, no, don't monitor yourself on my account...       1   0 #>   and be can cause clear course don't due feel firstly freely hope i just like #> 1   1  0   0     0     0      0     0   1    0       1      0    0 1    0    1 #> 2   0  0   1     0     0      0     0   0    1       0      1    1 1    0    0 #> 3   1  1   0     1     1      1     0   0    0       0      0    1 1    1    0 #> 4   0  0   0     0     0      0     1   0    0       0      0    0 0    0    0 #>   monitor my no not of offense oh on please proceed respect say speak to with #> 1       0  0  0   0  0       0  0  0      0       0       1   1     0  1    1 #> 2       0  0  0   0  0       0  0  0      1       1       0   0     1  0    0 #> 3       0  0  0   1  1       1  1  0      0       0       0   0     0  1    0 #> 4       1  1  1   0  0       0  1  1      0       0       0   0     0  0    0 #>   would you yourself meta_characters meta_syllables meta_words #> 1     1   0        0              42             12         11 #> 2     0   2        0              42             11         10 #> 3     0   0        0              46             14         13 #> 4     0   0        1              35             12          8 #>   meta_unique_words meta_clauses meta_sentences meta_words_per_clause #> 1                11            3              1              3.666667 #> 2                 9            3              2              3.333333 #> 3                13            4              1              3.250000 #> 4                 8            3              1              2.666667 #>   meta_words_per_sentence meta_sixltr meta_characters_per_word #> 1                      11           2                 3.818182 #> 2                       5           3                 4.200000 #> 3                      13           2                 3.538462 #> 4                       8           3                 4.375000 #>   meta_syllables_per_word meta_type_token_ratio meta_reading_grade meta_numbers #> 1                1.090909                   1.0           1.572727            0 #> 2                1.100000                   0.9          -0.660000            0 #> 3                1.076923                   1.0           2.187692            0 #> 4                1.500000                   1.0           5.230000            0 #>   meta_puncts meta_periods meta_commas meta_qmarks meta_exclams meta_quotes #> 1           5            3           2           0            0           0 #> 2           5            4           1           0            0           0 #> 3           6            3           3           0            0           0 #> 4           5            3           2           0            0           0 #>   meta_apostrophes meta_brackets meta_orgmarks #> 1                0             0             0 #> 2                0             0             0 #> 3                0             0             0 #> 4                1             0             0  # add dictionary and percent arguments for standard dictionary-based results lma_process(texts, dict = lma_dict(), percent = TRUE) #>                                                               text     ppron #> 1        Firstly, I would like to say, and with all due respect...  9.090909 #> 2         Please, proceed. I hope you feel you can speak freely... 30.000000 #> 3 Oh, of course, I just hope to be clear, and not cause offense...  7.692308 #> 4                  Oh, no, don't monitor yourself on my account... 25.000000 #>   ipron article   adverb     conj     prep   auxverb    negate    quant #> 1     0       0  0.00000 9.090909 18.18182  9.090909  0.000000 9.090909 #> 2     0       0 10.00000 0.000000  0.00000 10.000000  0.000000 0.000000 #> 3     0       0 15.38462 7.692308 15.38462  7.692308  7.692308 0.000000 #> 4     0       0 12.50000 0.000000 12.50000 12.500000 25.000000 0.000000 #>   interrog   number interjection meta_characters meta_syllables meta_words #> 1        0 9.090909     0.000000              42             12         11 #> 2        0 0.000000     0.000000              42             11         10 #> 3        0 0.000000     7.692308              46             14         13 #> 4        0 0.000000    12.500000              35             12          8 #>   meta_unique_words meta_clauses meta_sentences meta_words_per_clause #> 1                11            3              1              3.666667 #> 2                 9            3              2              3.333333 #> 3                13            4              1              3.250000 #> 4                 8            3              1              2.666667 #>   meta_words_per_sentence meta_sixltr meta_characters_per_word #> 1                      11    18.18182                 3.818182 #> 2                       5    30.00000                 4.200000 #> 3                      13    15.38462                 3.538462 #> 4                       8    37.50000                 4.375000 #>   meta_syllables_per_word meta_type_token_ratio meta_reading_grade meta_numbers #> 1                1.090909                   1.0           1.572727            0 #> 2                1.100000                   0.9          -0.660000            0 #> 3                1.076923                   1.0           2.187692            0 #> 4                1.500000                   1.0           5.230000            0 #>   meta_puncts meta_periods meta_commas meta_qmarks meta_exclams meta_quotes #> 1    45.45455     27.27273    18.18182           0            0           0 #> 2    50.00000     40.00000    10.00000           0            0           0 #> 3    46.15385     23.07692    23.07692           0            0           0 #> 4    62.50000     37.50000    25.00000           0            0           0 #>   meta_apostrophes meta_brackets meta_orgmarks #> 1              0.0             0             0 #> 2              0.0             0             0 #> 3              0.0             0             0 #> 4             12.5             0             0  # add space and weight arguments for standard word-centroid vectors lma_process(texts, space = lma_lspace(texts), weight = \"tfidf\") #>                                                               text          V1 #> 1        Firstly, I would like to say, and with all due respect... -0.07509574 #> 2         Please, proceed. I hope you feel you can speak freely... -0.08118897 #> 3 Oh, of course, I just hope to be clear, and not cause offense... -0.09709788 #> 4                  Oh, no, don't monitor yourself on my account... -0.01913927 #>            V2 meta_characters meta_syllables meta_words meta_unique_words #> 1  0.05737681              42             12         11                11 #> 2 -0.16696471              42             11         10                 9 #> 3  0.03907020              46             14         13                13 #> 4  0.01993145              35             12          8                 8 #>   meta_clauses meta_sentences meta_words_per_clause meta_words_per_sentence #> 1            3              1              3.666667                      11 #> 2            3              2              3.333333                       5 #> 3            4              1              3.250000                      13 #> 4            3              1              2.666667                       8 #>   meta_sixltr meta_characters_per_word meta_syllables_per_word #> 1   0.1818182                 3.818182                1.090909 #> 2   0.3000000                 4.200000                1.100000 #> 3   0.1538462                 3.538462                1.076923 #> 4   0.3750000                 4.375000                1.500000 #>   meta_type_token_ratio meta_reading_grade meta_numbers meta_puncts #> 1                   1.0           1.572727            0   0.4545455 #> 2                   0.9          -0.660000            0   0.5000000 #> 3                   1.0           2.187692            0   0.4615385 #> 4                   1.0           5.230000            0   0.6250000 #>   meta_periods meta_commas meta_qmarks meta_exclams meta_quotes #> 1    0.2727273   0.1818182           0            0           0 #> 2    0.4000000   0.1000000           0            0           0 #> 3    0.2307692   0.2307692           0            0           0 #> 4    0.3750000   0.2500000           0            0           0 #>   meta_apostrophes meta_brackets meta_orgmarks #> 1            0.000             0             0 #> 2            0.000             0             0 #> 3            0.000             0             0 #> 4            0.125             0             0"},{"path":"https://miserman.github.io/lingmatch/reference/lma_simets.html","id":null,"dir":"Reference","previous_headings":"","what":"Similarity Calculations — lma_simets","title":"Similarity Calculations — lma_simets","text":"Enter numerical matrix, set vectors, set matrices calculate similarity per vector.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_simets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Similarity Calculations — lma_simets","text":"","code":"lma_simets(a, b = NULL, metric = NULL, group = NULL, lag = 0,   agg = TRUE, agg.mean = TRUE, pairwise = TRUE, symmetrical = FALSE,   mean = FALSE, return.list = FALSE)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_simets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Similarity Calculations — lma_simets","text":"vector matrix. vector, b must also provided. matrix b missing, row compared. matrix b missing, row compared b row b. b vector matrix compared rows . metric character vector characters least partially matching one available metric names ('' explicitly include metrics), number vector numbers indicating metric index: jaccard: sum(& b) / sum(| b) euclidean: 1 / (1 + sqrt(sum((- b) ^ 2))) canberra: mean(1 - abs(- b) / (+ b)) cosine: sum(* b) / sqrt(sum(^ 2 * sum(b ^ 2))) pearson: (mean(* b) - (mean() * mean(b))) / sqrt(mean(^ 2) - mean() ^ 2) / sqrt(mean(b ^ 2) - mean(b) ^ 2) group b missing multiple rows, used make comparisons rows , modified agg agg.mean. lag Amount adjust b index; either rows b multiple rows (e.g., lag = 1, [1, ] compared b[2, ]), values otherwise (e.g., lag = 1, [1] compared b[2]). b supplied, b copy , resulting lagged self-comparisons autocorrelations. agg Logical: FALSE, boundary rows groups compared, see example. agg.mean Logical: FALSE aggregated rows summed instead averaged. pairwise Logical: FALSE b matrices number rows, paired rows compared. Otherwise (supplied), pairwise comparisons made. symmetrical Logical: TRUE pairwise comparisons rows made, results lower triangle copied upper triangle. mean Logical: TRUE, single mean metric returned per row . return.list Logical: TRUE, list-like object always returned, entry metric, even one metric requested.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_simets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Similarity Calculations — lma_simets","text":"Output varies based dimensions b:  : vector value per metric. : b vectors. : vector value per row. : time single value expected per row: b vector,       b matrices number rows pairwise = FALSE, group       specified, mean = TRUE, one metric requested. : data.frame column per metric. : multiple metrics requested previous case. : sparse matrix metric attribute metric name. : Pairwise comparisons within matrix       b matrix, 1 metric requested. : list sparse matrix per metric. : multiple metrics requested previous case.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_simets.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Similarity Calculations — lma_simets","text":"Use setThreadOptions change parallelization options; e.g., run RcppParallel::setThreadOptions(4) call lma_simets set number CPU threads 4.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_simets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Similarity Calculations — lma_simets","text":"","code":"text <- c(   \"words of speaker A\", \"more words from speaker A\",   \"words from speaker B\", \"more words from speaker B\" ) (dtm <- lma_dtm(text)) #> 4 x 7 sparse Matrix of class \"dgCMatrix\" #>      a b from more of speaker words #> [1,] 1 .    .    .  1       1     1 #> [2,] 1 .    1    1  .       1     1 #> [3,] . 1    1    .  .       1     1 #> [4,] . 1    1    1  .       1     1  # compare each entry lma_simets(dtm) #> $jaccard #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                #> [1,] I         .         .   . #> [2,] 0.5000000 I         .   . #> [3,] 0.3333333 0.5000000 I   . #> [4,] 0.2857143 0.6666667 0.8 I #>  #> $euclidean #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                #> [1,] I         .         .   . #> [2,] 0.3660254 I         .   . #> [3,] 0.3333333 0.3660254 I   . #> [4,] 0.3090170 0.4142136 0.5 I #>  #> $canberra #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                      #> [1,] I         .         .         . #> [2,] 0.5714286 I         .         . #> [3,] 0.4285714 0.5714286 I         . #> [4,] 0.2857143 0.7142857 0.8571429 I #>  #> $cosine #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                      #> [1,] I         .         .         . #> [2,] 0.6708204 I         .         . #> [3,] 0.5000000 0.6708204 I         . #> [4,] 0.4472136 0.8000000 0.8944272 I #>  #> $pearson #> 4 x 4 sparse Matrix of class \"dtCMatrix\" (unitriangular) #>                                         #> [1,]  I          .          .         . #> [2,]  0.09128709 I          .         . #> [3,] -0.16666667 0.09128709 I         . #> [4,] -0.54772256 0.30000000 0.7302967 I #>  #> attr(,\"time\") #> simets  #>      0   # compare each entry with the mean of all entries lma_simets(dtm, colMeans(dtm)) #>     jaccard euclidean  canberra    cosine   pearson #> 1 0.5714286 0.4220645 0.4380952 0.7484552 0.1964186 #> 2 0.7142857 0.5166852 0.5986395 0.9128709 0.6454972 #> 3 0.5714286 0.5166852 0.5034014 0.8845380 0.7463905 #> 4 0.7142857 0.5166852 0.5986395 0.9128709 0.6454972  # compare by group (corresponding to speakers and turns in this case) speaker <- c(\"A\", \"A\", \"B\", \"B\")  ## by default, consecutive rows from the same group are averaged: lma_simets(dtm, group = speaker) #>                 jaccard euclidean  canberra    cosine    pearson #> 1, 2 <-> 3, 4 0.5714286 0.3874259 0.5238095 0.6888467 -0.1324532  ## with agg = FALSE, only the rows at the boundary between ## groups (rows 2 and 3 in this case) are used: lma_simets(dtm, group = speaker, agg = FALSE) #>         jaccard euclidean  canberra    cosine    pearson #> 2 <-> 3     0.5 0.3660254 0.5714286 0.6708204 0.09128709"},{"path":"https://miserman.github.io/lingmatch/reference/lma_termcat.html","id":null,"dir":"Reference","previous_headings":"","what":"Document-Term Matrix Categorization — lma_termcat","title":"Document-Term Matrix Categorization — lma_termcat","text":"Reduces dimensions document-term matrix dictionary-based categorization.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_termcat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Document-Term Matrix Categorization — lma_termcat","text":"","code":"lma_termcat(dtm, dict, term.weights = NULL, bias = NULL,   bias.name = \"_intercept\", escape = TRUE, partial = FALSE,   glob = TRUE, term.filter = NULL, term.break = 20000,   to.lower = FALSE, dir = getOption(\"lingmatch.dict.dir\"))"},{"path":"https://miserman.github.io/lingmatch/reference/lma_termcat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Document-Term Matrix Categorization — lma_termcat","text":"dtm matrix terms column names. dict name provided dictionary (osf.io/y6g5b/wiki) file found dir, list object named character vectors word lists, path file read read.dic. term.weights list object named numeric vectors lining character vectors dict, used weight terms dict vector. category dict specified term.weights, dict term.weights vectors length, weights category 1. bias list named vector specifying constant add named category. term matching bias.name included category, associated weight used bias category. bias.name character specifying term used category bias; default '_intercept'. escape Logical indicating whether terms dict treated plain text (including asterisk wild cards). TRUE, regular expression related characters escaped. Set TRUE get PCRE compilation errors. partial Logical; TRUE terms partially matched (padded ^ $). glob Logical; TRUE (default), convert initial terminal asterisks partial matches. term.filter regular expression string used format text term (passed gsub). example, terms part--speech tagged (e.g., 'a_DT'), '_.*' remove tag. term.break category term.break characters, processed chunks. Reduce 20000 get PCRE compilation error. .lower Logical; TRUE lowercase dictionary terms. Otherwise, dictionary terms converted match terms single-cased. Set FALSE always keep dictionary terms entered. dir Path folder look dict;  look '~/Dictionaries' default.  Set session default options(lingmatch.dict.dir = 'desired/path').","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_termcat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Document-Term Matrix Categorization — lma_termcat","text":"matrix row per dtm row columns per dictionary category, WC attribute original word counts.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/lma_termcat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Document-Term Matrix Categorization — lma_termcat","text":"","code":"if (FALSE) {  # Score texts with the NRC Affect Intensity Lexicon  dict <- readLines(\"https://saifmohammad.com/WebDocs/NRC-AffectIntensity-Lexicon.txt\") dict <- read.table(   text = dict[-seq_len(grep(\"term\\tscore\", dict, fixed = TRUE)[[1]])],   col.names = c(\"term\", \"weight\", \"category\") )  text <- c(   angry = paste(     \"We are outraged by their hateful brutality,\",     \"and by the way they terrorize us with their hatred.\"   ),   fearful = paste(     \"The horrific torture of that terrorist was tantamount\",     \"to the terrorism of terrorists.\"   ),   joyous = \"I am jubilant to be celebrating the bliss of this happiest happiness.\",   sad = paste(     \"They are nearly suicidal in their mourning after\",     \"the tragic and heartbreaking holocaust.\"   ) )  emotion_scores <- lma_termcat(text, dict) if (require(\"splot\")) splot(emotion_scores ~ names(text), leg = \"out\")  ## or use the standardized version (which includes more categories)  emotion_scores <- lma_termcat(text, \"nrc_eil\", dir = \"~/Dictionaries\") emotion_scores <- emotion_scores[, c(\"anger\", \"fear\", \"joy\", \"sadness\")] if (require(\"splot\")) splot(emotion_scores ~ names(text), leg = \"out\") }"},{"path":"https://miserman.github.io/lingmatch/reference/lma_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Document-Term Matrix Weighting — lma_weight","title":"Document-Term Matrix Weighting — lma_weight","text":"Weight document-term matrix.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Document-Term Matrix Weighting — lma_weight","text":"","code":"lma_weight(dtm, weight = \"count\", normalize = TRUE, wc.complete = TRUE,   log.base = 10, alpha = 1, pois.x = 1L, doc.only = FALSE,   percent = FALSE)"},{"path":"https://miserman.github.io/lingmatch/reference/lma_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Document-Term Matrix Weighting — lma_weight","text":"dtm matrix words column names. weight string referring least partially one (combination; see note)   available weighting methods: Term weights (applied uniquely cell) binary (dtm > 0) * 1      Convert frequencies 1s 0s; remove differences frequencies. log log(dtm + 1, log.base)      Log frequencies. sqrt sqrt(dtm)      Square root frequencies. count dtm      Unaltered; sometimes called term frequencies (tf). amplify dtm ^ alpha      Amplify difference frequencies. Document weights (applied column) dflog log(colSums(dtm > 0), log.base)      Log binary term sum. entropy 1 - rowSums(x * log(x + 1, log.base) / log(ncol(x), log.base), na.rm = TRUE)      x = t(dtm) / colSums(dtm > 0); entropy term-conditional term distribution. ppois 1 - ppois(pois.x, colSums(dtm) / nrow(dtm))      Poisson-predicted term distribution. dpois 1 - dpois(pois.x, colSums(dtm) / nrow(dtm))      Poisson-predicted term density. dfmlog log(diag(dtm[max.col(t(dtm)), ]), log.base)      Log maximum term frequency. dfmax diag(dtm[max.col(t(dtm)), ])      Maximum term frequency. df colSums(dtm > 0)      Sum binary term occurrence across documents. idf log(nrow(dtm) / colSums(dtm > 0), log.base)      Inverse document frequency. ridf idf - log(dpois, log.base)      Residual inverse document frequency. normal sqrt(1 / colSums(dtm ^ 2))      Normalized document frequency. Alternatively, 'pmi' 'ppmi' apply pointwise mutual information weighting scheme ('ppmi' setting negative values 0). normalize Logical: FALSE, dtm divided document word-count weighted. wc.complete dtm made lma_dtm ('WC' attribute), word counts frequencies can based raw count (default; wc.complete = TRUE). wc.complete = FALSE, dtm 'WC' attribute, rowSums(dtm) used word count. log.base base logs, applied weight using log. Default 10. alpha scaling factor applied document frequency part pointwise mutual information weighting, amplify's power (dtm ^ alpha, defaults 1.1). pois.x integer; quantile probability poisson distribution (dpois(pois.x, colSums(x, na.rm = TRUE) / nrow(x))). doc.Logical: TRUE, document weights returned (single value term). percent Logical; TRUE, frequencies multiplied 100.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_weight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Document-Term Matrix Weighting — lma_weight","text":"weighted version dtm, type attribute added (attr(dtm, 'type')).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_weight.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Document-Term Matrix Weighting — lma_weight","text":"Term weights works adjust differences counts within documents, differences meaning increasingly binary log sqrt count amplify. Document weights work treat words differently based -document overall frequency. term frequencies constant, dpois, idf, ridf, normal give less common words increasingly weight, dfmax, dfmlog, ppois, df, dflog, entropy give less common words increasingly less weight. weight can either vector two characters, corresponding term weight document weight (e.g., c('count', 'idf')), can string term document weights separated :\\*_/; ,- (e.g., 'count-idf'). 'tf' also acceptable 'count', 'tfidf' parsed c('count', 'idf'), though special case. weight, term document weights can entered individually; term weights alone apply document weight, document weights alone apply 'count' term weight (unless doc.= TRUE, case term-named vector document weights returned instead weighted dtm).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/lma_weight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Document-Term Matrix Weighting — lma_weight","text":"","code":"# visualize term and document weights  ## term weights term_weights <- c(\"binary\", \"log\", \"sqrt\", \"count\", \"amplify\") Weighted <- sapply(term_weights, function(w) lma_weight(1:20, w, FALSE)) if (require(splot)) splot(Weighted ~ 1:20, labx = \"Raw Count\", lines = \"co\") #> Loading required package: splot   ## document weights doc_weights <- c(   \"df\", \"dflog\", \"dfmax\", \"dfmlog\", \"idf\", \"ridf\",   \"normal\", \"dpois\", \"ppois\", \"entropy\" ) weight_range <- function(w, value = 1) {   m <- diag(20)   m[upper.tri(m, TRUE)] <- if (is.numeric(value)) {     value   } else {     unlist(lapply(       1:20, function(v) rep(if (value == \"inverted\") 21 - v else v, v)     ))   }   lma_weight(m, w, FALSE, doc.only = TRUE) }  if (require(splot)) {   category <- rep(c(\"df\", \"idf\", \"normal\", \"poisson\", \"entropy\"), c(4, 2, 1, 2, 1))   op <- list(     laby = \"Relative (Scaled) Weight\", labx = \"Document Frequency\",     leg = \"outside\", lines = \"connected\", mv.scale = TRUE, note = FALSE   )   splot(     sapply(doc_weights, weight_range) ~ 1:20,     options = op, title = \"Same Term, Varying Document Frequencies\",     sud = \"All term frequencies are 1.\",     colorby = list(category, grade = TRUE)   )   splot(     sapply(doc_weights, weight_range, value = \"sequence\") ~ 1:20,     options = op, title = \"Term as Document Frequencies\",     sud = \"Non-zero terms are the number of non-zero terms.\",     colorby = list(category, grade = TRUE)   )   splot(     sapply(doc_weights, weight_range, value = \"inverted\") ~ 1:20,     options = op, title = \"Term Opposite of Document Frequencies\",     sud = \"Non-zero terms are the number of zero terms + 1.\",     colorby = list(category, grade = TRUE)   ) }"},{"path":"https://miserman.github.io/lingmatch/reference/read.dic.html","id":null,"dir":"Reference","previous_headings":"","what":"Read/Write Dictionary Files — read.dic","title":"Read/Write Dictionary Files — read.dic","text":"Read write dictionary files Comma-Separated Values (.csv; weighted) Linguistic Inquiry Word Count (.dic; non-weighted) format.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/read.dic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read/Write Dictionary Files — read.dic","text":"","code":"read.dic(path, cats, type = \"asis\", as.weighted = FALSE,   dir = getOption(\"lingmatch.dict.dir\"), ..., term.name = \"term\",   category.name = \"category\", raw = FALSE)  write.dic(dict, filename, type = \"asis\", as.weighted = FALSE,   save = TRUE)"},{"path":"https://miserman.github.io/lingmatch/reference/read.dic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read/Write Dictionary Files — read.dic","text":"path Path file, name corresponding file getOption('lingmatch.dict.dir') ('~/Dictionaries') one dictionaries available osf.io/y6g5b, matrix-like object categorized, list formatted. cats character vector category names returned. categories returned default. type character indicating whether terms altered. Unspecified matching 'asis' leaves terms . options change wildcards regular expressions: 'pattern' ('^[poi]') replaces initial asterisks '\\\\b\\\\w*', terminal asterisks '\\\\w*\\\\b', match terms within raw text; anything else, terms padded ^ $, bounding marks removed asterisk present, match tokenized terms. .weighted Logical; TRUE, prevents weighted dictionaries converted unweighted versions, converts unweighted dictionaries binary weighted version -- data.frame \"term\" column unique terms, column category. dir Path folder containing dictionaries, like dictionaries downloaded; passed select.dict /download.dict. ... Passes arguments readLines. term.name, category.name Strings identifying column names path containing terms categories respectively. raw Logical character. logical, indicates path treated raw dictionary (might read .dic file). character, replaces path read file. dict list named entry terms category, data.frame terms one column, categories weights rest. filename name file saved. save Logical: FALSE, write file.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/read.dic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read/Write Dictionary Files — read.dic","text":"read.dic: list (unweighted) entry category containing   character vectors terms, data.frame (weighted) columns terms (first, \"term\")   weights (subsequent, category labels names). write.dic: version written dictionary -- raw character vector   unweighted dictionaries, data.frame weighted dictionaries.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/read.dic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read/Write Dictionary Files — read.dic","text":"","code":"# make a small murder related dictionary dict <- list(   kill = c(\"kill*\", \"murd*\", \"wound*\", \"die*\"),   death = c(\"death*\", \"dying\", \"die*\", \"kill*\") )  # convert it to a weighted format (dict_weighted <- read.dic(dict, as.weighted = TRUE)) #>     term kill death #> 1  kill*    1     1 #> 2  murd*    1     0 #> 3 wound*    1     0 #> 4   die*    1     1 #> 5 death*    0     1 #> 6  dying    0     1  # categorize it back read.dic(dict_weighted) #> $kill #> [1] \"kill*\"  \"murd*\"  \"wound*\" \"die*\"   #>  #> $death #> [1] \"kill*\"  \"die*\"   \"death*\" \"dying\"  #>   # convert it to a string without writing to a file cat(raw_dict <- write.dic(dict, save = FALSE)) #> % #> 1\tkill #> 2\tdeath #> % #> kill*\t1\t2 #> murd*\t1 #> wound*\t1 #> die*\t1\t2 #> death*\t2 #> dying\t2  # parse it back in read.dic(raw = raw_dict) #> $kill #> [1] \"kill*\"  \"murd*\"  \"wound*\" \"die*\"   #>  #> $death #> [1] \"kill*\"  \"die*\"   \"death*\" \"dying\"  #>   if (FALSE) {  # save it as a .dic file write.dic(dict, \"murder\")  # read it back in as a list read.dic(\"murder.dic\")  # read in the Moral Foundations or LUSI dictionaries from urls moral_dict <- read.dic(\"https://osf.io/download/whjt2\") lusi_dict <- read.dic(\"https://www.depts.ttu.edu/psy/lusi/files/lusi_dict.txt\")  # save and read in a version of the General Inquirer dictionary inquirer <- read.dic(\"inquirer\", dir = \"~/Dictionaries\") }"},{"path":"https://miserman.github.io/lingmatch/reference/read.segments.html","id":null,"dir":"Reference","previous_headings":"","what":"Read and Segment Multiple Texts — read.segments","title":"Read and Segment Multiple Texts — read.segments","text":"Split texts word count specific characters. Input texts directly, read files.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/read.segments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read and Segment Multiple Texts — read.segments","text":"","code":"read.segments(path = \".\", segment = NULL, ext = \".txt\", subdir = FALSE,   segment.size = -1, bysentence = FALSE, end_in_quotes = TRUE,   preclean = FALSE, text = NULL)"},{"path":"https://miserman.github.io/lingmatch/reference/read.segments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read and Segment Multiple Texts — read.segments","text":"path Path folder containing files, vector paths files. folders files recognized path, treated text. segment Specifies text file segmented. character, split character; '\\n' default. number, texts broken many segments, roughly equal number words. ext extension files want read . '.txt' default. subdir Logical; TRUE, files folders path also included. segment.size Logical; specified, segment ignored, texts broken segments containing roughly segment.size number words. bysentence Logical; TRUE, segment number segment.size specified, sentences kept together, rather potentially broken across segments. end_in_quotes Logical; FALSE, sentence-ending marks (.?!) considered immediately followed quotation mark. example, '\"Word.\" Word.' considered one sentence. preclean Logical; TRUE, text cleaned lma_dict(special) segmentation. text character vector text split, used place path. entry treated file.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/read.segments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read and Segment Multiple Texts — read.segments","text":"data.frame columns file names (input), segment number within file (segment), word count segment (WC), text segment (text).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/read.segments.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read and Segment Multiple Texts — read.segments","text":"","code":"# split preloaded text read.segments(\"split this text into two segments\", 2) #>   input segment WC              text #> 1     1       1  3   split this text #> 2     1       2  3 into two segments  if (FALSE) {  # read in all files from the package directory texts <- read.segments(path.package(\"lingmatch\"), ext = \"\") texts[, -4]  # segment .txt files in dir in a few ways: dir <- \"path/to/files\"  ## into 1 line segments texts_lines <- read.segments(dir)  ## into 5 even segments each texts_5segs <- read.segments(dir, 5)  ## into 50 word segments texts_50words <- read.segments(dir, segment.size = 50)  ## into 1 sentence segments texts_1sent <- read.segments(dir, segment.size = 1, bysentence = TRUE) }"},{"path":"https://miserman.github.io/lingmatch/reference/select.dict.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Dictionaries — select.dict","title":"Select Dictionaries — select.dict","text":"Retrieve information links dictionaries (lexicons/word lists) available osf.io/y6g5b.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/select.dict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Dictionaries — select.dict","text":"","code":"select.dict(query = NULL, dir = getOption(\"lingmatch.dict.dir\"),   check.md5 = TRUE, mode = \"wb\")"},{"path":"https://miserman.github.io/lingmatch/reference/select.dict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Dictionaries — select.dict","text":"query character matching dictionary name, set keywords search dictionary information. dir Path folder containing dictionaries, want saved. look getOption('lingmatch.dict.dir') '~/Dictionaries' default. check.md5 Logical; TRUE (default), retrieves MD5 checksum OSF, compares calculated downloaded file check integrity. mode Passed download.file downloading files.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/select.dict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Dictionaries — select.dict","text":"list varying entries:  info: version osf.io/kjqb8 stored internally;       data.frame  dictionary names row names, information dictionary columns.          Also described         osf.io/y6g5b/wiki/dict_variables,         short (corresponding file name [{short}.(csv|dic)]         wiki urls [https://osf.io/y6g5b/wiki/{short}]) set row names removed: name: Full name dictionary. description: Description dictionary, relating purpose             development. note: Notes processing decisions additionally alter original. constructor: dictionary constructed: algorithm: Terms selected automated process, potentially                 learned data resources. crowd: Several individuals rated terms, aggregate ratings                 translate categories weights. mixed: combination methods, usually iterative                 process. team: One individuals make decisions term inclusions,                 categories, weights. subject: Broad, rough subject purpose dictionary: emotion: Terms relate emotions, potentially exemplifying expressing                 . general: large range categories, aiming capture content                 text. impression: Terms categorized weighted based impression                 might give. language: Terms categorized weighted based linguistic                 features, part speech, specificity, area use. social: Terms relate social phenomena, characteristics concerns                 social entities. terms: Number unique terms across categories. term_type: Format terms: glob: Include asterisks denote inclusion characters                 word boundary. glob+: Glob-style asterisks regular expressions within terms. ngram: Includes number words term, separated spaces. pattern: string characters, potentially within words,                 spanning words. regex: Regular expressions. stem: Unigrams common endings removed. unigram: Complete single words. weighted: Indicates whether weights associated terms.             determines file type dictionary: dictionaries weights stored             .csv, without stored .dic files. regex_characters: Logical indicating whether special regular expression             characters present term, might need escaped terms used             regular expressions. Glob-type terms allow complete parens (least one open one             closed, indicating preceding following words), initial terminal asterisks.             terms, [](){}*.^$+?\\| counted regex characters.             escaped R gsub('([][)(}{*.^$+?\\\\|])', '\\\\\\1', terms) terms             character vector, Python (importing re)             [re.sub(r'([][(){}*.^$+?\\|])', r'\\\\1', term) term terms] terms             list. categories: Category names order appear dictionary             file, separated commas. ncategories: Number categories. original_max: Maximum value original dictionary standardization:             original values / max(original values) * 100. Dictionaries weights             considered max 1. osf: ID file OSF, translating file's URL:             https://osf.io/osf. wiki: URL dictionary's wiki. downloaded: Path file downloaded, '' otherwise. selected: subset info selected query.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/select.dict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Dictionaries — select.dict","text":"","code":"# just retrieve information about available dictionaries dicts <- select.dict()$info dicts[1:10, 4:9] #>                  constructor    subject terms term_type weighted #> adicat_function         team   language   759     glob+    FALSE #> adict                  mixed     social 12168     ngram    FALSE #> afinn                   team    emotion  3381   unigram     TRUE #> agency_communion        team     social   447      glob    FALSE #> allslang                team   language 10109     ngram    FALSE #> anew                   crowd impression  1034   unigram     TRUE #> anew_emotion           crowd    emotion  1034   unigram     TRUE #> banbuilder              team impression   199   unigram    FALSE #> banned                  team impression    77     ngram    FALSE #> cost_benefit            team     social   154      glob    FALSE #>                  regex_characters #> adicat_function              TRUE #> adict                        TRUE #> afinn                       FALSE #> agency_communion            FALSE #> allslang                     TRUE #> anew                        FALSE #> anew_emotion                 TRUE #> banbuilder                  FALSE #> banned                      FALSE #> cost_benefit                FALSE  # select all dictionaries mentioning sentiment or emotion sentiment_dicts <- select.dict(\"sentiment emotion\")$selected sentiment_dicts[1:10, 4:9] #>              constructor    subject  terms term_type weighted regex_characters #> afinn               team    emotion   3381   unigram     TRUE            FALSE #> anew_emotion       crowd    emotion   1034   unigram     TRUE             TRUE #> depechemood    algorithm    emotion 114000   unigram     TRUE            FALSE #> emolex         algorithm    emotion  28480   unigram     TRUE            FALSE #> emosenticnet   algorithm    emotion  13175     ngram    FALSE             TRUE #> emote              crowd impression   2197   unigram     TRUE            FALSE #> galc                team    emotion    274      glob    FALSE            FALSE #> huliu               team    emotion   6789   unigram    FALSE             TRUE #> inquirer            team    general   8624   unigram    FALSE             TRUE #> labmt              crowd    emotion   3934   unigram     TRUE            FALSE"},{"path":"https://miserman.github.io/lingmatch/reference/select.lspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Select Latent Semantic Spaces — select.lspace","title":"Select Latent Semantic Spaces — select.lspace","text":"Retrieve information links latent semantic spaces (sets word vectors/embeddings) available osf.io/489he, optionally download term mappings (osf.io/xr7jv).","code":""},{"path":"https://miserman.github.io/lingmatch/reference/select.lspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select Latent Semantic Spaces — select.lspace","text":"","code":"select.lspace(query = NULL, dir = getOption(\"lingmatch.lspace.dir\"),   get.map = FALSE, check.md5 = TRUE, mode = \"wb\")"},{"path":"https://miserman.github.io/lingmatch/reference/select.lspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select Latent Semantic Spaces — select.lspace","text":"query character matching space name, character vector terms, used select spaces. length 1, get.map set TRUE. dir Path directory containing lma_term_map.rda downloaded spaces;  look getOption('lingmatch.lspace.dir') '~/Latent Semantic Spaces' default. get.map Logical; TRUE lma_term_map.rda found dir, term map (lma_term_map.rda) downloaded decompressed. check.md5 Logical; TRUE (default), retrieves MD5 checksum OSF, compares calculated downloaded file check integrity. mode Passed download.file downloading term map.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/select.lspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select Latent Semantic Spaces — select.lspace","text":"list varying entries:  info: version osf.io/9yzca stored internally;       data.frame  spaces row names, information space columns: terms: number terms space corpus: corpus(es) space trained model: model space trained dimensions: number dimensions model (columns space) model_info: parameter details model original_max: maximum value used normalize space; original             space (vectors * original_max) / 100 osf_dat: OSF id .dat files; URL             https://osf.io/osf_dat osf_terms: OSF id _terms.txt files; URL             https://osf.io/osf_terms wiki: link wiki space downloaded: path .dat file downloaded,             '' otherwise. selected: subset info selected query. term_map: get.map TRUE lma_term_map.rda found       dir, copy osf.io/xr7jv, space names       column names, terms row names, indices values, 0 indicating term       present associated space.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/select.lspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select Latent Semantic Spaces — select.lspace","text":"","code":"# just retrieve information about available spaces spaces <- select.lspace() spaces$info[1:10, c(\"terms\", \"dimensions\", \"original_max\")] #>                   terms dimensions original_max #> 100k              99188        300  129.8630950 #> 100k_cbow         99186        300    7.3177360 #> 100k_lsa          99188        300 1144.5951390 #> blogs             27277        300    0.4780927 #> CoNLL17_skipgram 459818        100    4.1434050 #> dcp_cbow         215142        400    1.8966740 #> dcp_svd          215142        500    0.6560584 #> eigenwords       159908        200    1.0000000 #> eigenwords_tscca  55879        200    1.5136493 #> facebook_crawl    81653        300    4.5530000  # retrieve all spaces that used word2vec w2v_spaces <- select.lspace(\"word2vec\")$selected w2v_spaces[, c(\"terms\", \"dimensions\", \"original_max\")] #>                   terms dimensions original_max #> 100k_cbow         99186        300    7.3177360 #> CoNLL17_skipgram 459818        100    4.1434050 #> dcp_cbow         215142        400    1.8966740 #> google           345655        300    2.8437500 #> nasari           151776        300    0.7317708 #> paragram_sl999   456295        300    4.0129000 #> paragram_ws353   456295        300    4.0129000 #> sensembed        409078        400    1.7170870 #> ukwac_cbow       171187        400    6.6162160  if (FALSE) {  # select spaces by terms select.lspace(c(   \"part-time\", \"i/o\", \"'cause\", \"brexit\", \"debuffs\" ))$selected[, c(\"terms\", \"coverage\")] }"},{"path":"https://miserman.github.io/lingmatch/reference/standardize.lspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Standardize a Latent Semantic Space — standardize.lspace","title":"Standardize a Latent Semantic Space — standardize.lspace","text":"Reformat .rda file matrix terms row names, plain-text embeddings file term start line, consistent delimiting characters. Plain-text files processed line--line, large spaces can reformatted RAM-conservatively.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/standardize.lspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standardize a Latent Semantic Space — standardize.lspace","text":"","code":"standardize.lspace(infile, name, sep = \" \", digits = 9,   dir = getOption(\"lingmatch.lspace.dir\"), outdir = dir, remove = \"\",   term_check = \"^[a-zA-Z]+$|^['a-zA-Z][a-zA-Z.'\\\\/-]*[a-zA-Z.]$\",   verbose = FALSE)"},{"path":"https://miserman.github.io/lingmatch/reference/standardize.lspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standardize a Latent Semantic Space — standardize.lspace","text":"infile Name .rda plain-text file relative dir,  e.g., \"default.rda\" \"glove/glove.6B.300d.txt\". name Base name reformatted file term file; e.g., \"glove\" result glove.dat glove_terms.txt outdir. sep Delimiting character values line, e.g., \" \" \"\\t\". applies plain-text files. digits Number digits round values ; default 9. dir Path folder containing infiles.  Default getOption('lingmatch.lspace.dir'), must set current session. specified infile full path, dir set infile's parent directory. outdir Path folder save standardized files; default dir. remove string regex pattern removed term names  (.e., gsub(remove, \"\", term)); default \"\", ignored. term_check string regex pattern filter terms; .e., lines fully matched terms written reformatted file. default attempts retain regular words, including dashes, foreword slashes, periods. Set empty string (\"\") write lines regardless term. verbose Logical: TRUE, prints current line number term console every 1,000 lines. applies plain-text files.","code":""},{"path":"https://miserman.github.io/lingmatch/reference/standardize.lspace.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Standardize a Latent Semantic Space — standardize.lspace","text":"Path standardized [1] data file [2] terms file applicable.","code":""},{"path":[]},{"path":"https://miserman.github.io/lingmatch/reference/standardize.lspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Standardize a Latent Semantic Space — standardize.lspace","text":"","code":"if (FALSE) {  # from https://sites.google.com/site/fritzgntr/software-resources/semantic_spaces standardize.lspace(\"EN_100k_lsa.rda\", \"100k_lsa\")  # from https://fasttext.cc/docs/en/english-vectors.html standardize.lspace(\"crawl-300d-2M.vec\", \"facebook_crawl\")  # Standardized versions of these spaces can also be downloaded with download.lspace. }"},{"path":[]},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"improvements-1-0-4","dir":"Changelog","previous_headings":"","what":"Improvements","title":"version 1.0.4","text":"Avoids checking long texts files. Improves feedback directory specified. Changes results singular pairwise comparisons 1 NA.","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"version-103","dir":"Changelog","previous_headings":"","what":"version 1.0.3","title":"version 1.0.3","text":"CRAN release: 2022-01-25","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"bug-fixes-1-0-3","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"version 1.0.3","text":"Fixes lma_simets vector matrix comparisons cases. Fixes lma_dtm tokens.one text ends empty.","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"version-102","dir":"Changelog","previous_headings":"","what":"version 1.0.2","title":"version 1.0.2","text":"CRAN release: 2021-04-22","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"features-1-0-2","dir":"Changelog","previous_headings":"","what":"Features","title":"version 1.0.2","text":"Adds options specify term category names read.dic. Adds option remove unmatched categories terms lma_patcat.","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"improvements-1-0-2","dir":"Changelog","previous_headings":"Features","what":"Improvements","title":"version 1.0.2","text":"Improves input-handling dictionaries. Enables term exclusions token-lma_dtm. Makes output formats consistent. Handles special-character conversion misencoded text. Better handles unrecognized weight metric names. Handles inconsistently named dictionaries.","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"bug-fixes-1-0-2","dir":"Changelog","previous_headings":"Features","what":"Bug Fixes","title":"version 1.0.2","text":"Fixes lma_dtm token dtm conversion final token indices entries empty. Appropriately allows read.dic read urls. Term weights correctly default count given empty weight. Adds pois.x argument lma_weight allow separately specified augment alphas d/ppois quantiles probabilities. Corrects handling empty texts cases. Corrects lma_process routing argument passing cases.","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"version-101","dir":"Changelog","previous_headings":"","what":"version 1.0.1","title":"version 1.0.1","text":"CRAN release: 2021-02-25","code":""},{"path":"https://miserman.github.io/lingmatch/news/index.html","id":"bug-fixes-1-0-1","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"version 1.0.1","text":"Avoids compilation issue older macOS platforms. Avoids factor-related issues stringsAsFactors option TRUE.","code":""}]
