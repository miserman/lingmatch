---
title: "Introduction to Text Analysis"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Exploring Data with splot}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This introduction uses lingmatch to illustrate a few of the many branching paths of text analysis.

*Built with R 
`r getRversion()`
on 
`r format(Sys.time(),'%B %d %Y')`*

***
# Introduction

First, we need some text -- anything that results in a vector of text will do.
```{r}
text = c(
  "Hey, I like kittens. I think all kinds of cats really are just the best pet ever.",
  "Oh yeah? Well I really like cars. All the wheels and the turbos... I think that's the best
   ever.",
  "You know what? Poo on you. Cats, dogs, rabbits -- you know, living creatures... to think
   you'd care about anything else!",
  "You can stick to your opinion. You can not be right if you want. You know what life's about?
   Supercharging, diesel guzzling, exhaust spewing, piston moving ignitions."
)
```

Even before processing, we can do some fun things.
The grepl function, for example, allows you to turn string searches into logical vectors;
you can use this to dummy code based on word use/pattern appearance.
```{r}
grepl(' I | me | my ',text)
```

For more thorough processing, we can break these strings into words, then count them.
This is what the lma_dtm function from the lingmatch package does:
```{r}
# load in the lingmatch package
library(lingmatch)

dtm = lma_dtm(text)
dtm[,1:10]
```

With even this simple level of processing, we can get some fun information:
```{r}
# a count of words across documents
colSums(dtm)[1:5]
# a count of words in each document
rowSums(dtm)
# correlation between words
cor(dtm)[1:5,1:5]
cor(t(dtm))
```

This is getting into the fundamentals of information extraction/retrieval,
where, for example, term similarity can be used to enhance searches,
and document similarity can be the search criteria.
Document similarity is also secretly linguistic similarity.

# Document/Term Weighting

Using just raw word counts, common words have more weight -- potentially many times more.
Often, we're more interested in less common words, as those tend to be more meaningful
(they tend to be content words, as opposed to function words, and can better distinguish
relevant texts to a search).

The most simple, straightforward way to adjust the weight of words is to weight them
(adjust their values).

Weighting can take place on either or both axes of the dtm; term weighting adjusts the
effect of term frequency, and document weighting adjusts the effect of term commonness.

Different weighting schemes work to either amplify or dampen these differences:
```{r}
#load in the splot package
library(splot)

# set up plotting options
op = list(y='term_frequency~document_frequency', line='connected', leg='outside')

# and the range of document frequency
document_frequency = seq_len(20)
```

Comparing term weights:
```{r, fig.height=5, fig.width=10, dev='CairoSVG'}
term_weights = c('binary','log','sqrt','count','amplify')
term_frequency = sapply(term_weights,function(w)lma_weight(matrix(document_frequency,1),w,FALSE))
splot(myl=c(0,25),options=op)
```

Comparing document weights:
```{r, fig.height=5, fig.width=10, dev='CairoSVG'}
doc_weights = c('df','dfmax','idf','normal','poisson','ridf','entropy')
term_frequency = sapply(doc_weights,function(w)
  lma_weight(sapply(document_frequency,function(i)sample(0:i,5000,TRUE)),w,FALSE,doc.only=TRUE)
)
splot(myl=c(-3,3),mv.scale=TRUE,options=op)
```

We can weight our example dtm and see how it affects the correlation between documents:
```{r}
# idf dampens frequent words
cor(t(lma_weight(dtm,'idf')))

# whereas entropy amplifies frequent words
cor(t(lma_weight(dtm,'entropy')))
```

For now, we'll just apply standard word term count (frequency) weighting
```{r}
wdtm = lma_weight(dtm)
```

# Dimention Reduction

A more thorough way to change the dtm space is by reducing its dimensions.
This is effectively what LIWC (or any dictionary-based approach) does --
reducing the number of terms by throwing them into categories:
```{r}
cwdtm = lma_termcat(wdtm)
cwdtm[,1:7]
```

# Language Style Matching

The way we've set things up allows us to look at the standard LSM calculation:
```{r}
lma_simets(cwdtm,metric='canberra')
```

What makes language style matching language style matching is our choice of weighting,
dimention reduction, and similarity metric.

***
<div style='text-align:center'>
![Brought to you by the <a href="https://www.depts.ttu.edu/psy/lusi/resources.php" target="_blank">Language Use and Social Interaction</a> lab at Texas Tech University](https://www.depts.ttu.edu/psy/lusi/files/lusi.jpg)
</div>
