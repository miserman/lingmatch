---
title: "Introduction to Text Analysis"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Exploring Data with splot}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This introduction uses lingmatch to illustrate a few of the many branching paths of text analysis.

*Built with R 
`r getRversion()`
on 
`r format(Sys.time(),'%B %d %Y')`*

***
# Setup

First, we need some text -- anything that results in a vector of text will do.
```{r}
text = c(
  "Hey, I like kittens. I think all kinds of cats really are just the best pet ever.",
  "Oh yeah? Well I really like cars. All the wheels and the turbos... I think that's the best
   ever.",
  "You know what? Poo on you. Cats, dogs, rabbits -- you know, living creatures... to think
   you'd care about anything else!",
  "You can stick to your opinion. You can not be right if you want. You know what life's about?
   Supercharging, diesel guzzling, exhaust spewing, piston moving ignitions."
)
```

## Manual Processing

We'll be taking a 'bag of words' approach to this text, meaning we don't care about sentence structure or the like,
so we'll want to standardize the texts and split them into words.

The first step can be to convert all of the characters to lower case:
```{r}
# we'll call the processed text words since it will eventually be single words
words = tolower(text)
words[1]
```

Just to simplify things, we can also just remove punctuation, which will involve some regular expression.

[Regular expression](https://en.wikipedia.org/wiki/Regular_expression) is a way to parse strings of characters
in a refined way. Forms of regular expression are implemented in most programming languages. It can be tricky to
think with, but offers a great deal of control when it comes to parsing text.

We can use the gsub function to replace all of the characters we specify with nothing.
```{r}
# you can get a feel for the way gsub works by entering an example string;
# the first argument is the expression to match, the second is what to replace it with,
# and the third is the text, so this will replace the blank space with an asterisk
gsub(' ', '*', 'a b')

# []s define a set of single characters, and the + will match any repeats,
# so one or more of the characters inside the brackets will be replaced with nothing.
# entering a vector into the third position will apply the replacement to each element
words = gsub('[,.?!\n-]+', '', words)
words[1]
```

The strsplit function splits one string into many based on a regular expression string.
```{r}
# this applies to each element of the vector, and returns a list of vectors
words = strsplit(words, ' +')
```

Now we have a list of tokens (the `words` object) which corresponds to our original text:
```{r}
# the first text
text[1]

# the tokenized version of that text
words[[1]]
```

Now that we have all of our words separated, we can translate the original texts into numerical vectors,
resulting in a document-term matrix (DTM).

The first step can be to set up an empty matrix, with a row for each text, and a column for each unique word:
```{r}
# since our words are in a list, we need to unlist them, then get only the unique ones
# you don't need to sort the words, but it can make it easier to look through
unique_words = sort(unique(unlist(words)))

# then we can make a matrix of 0s, and put the unique words in as column names
dtm = matrix(0, length(text), length(unique_words), dimnames = list(NULL, unique_words))
dtm[, 1:10]
```

Now we can fill the matrix with counts for each word for each text.
We can count up all of the words at once, so we only have to loop through texts:
```{r}
# this will perform whats inside the curly brackets as many times as there are texts
# i will be set to each number in the sequence, so i will equal 1 on the first run, and so on
# the words and dtm objects all line up, so words[[i]] corresponds to dtm[i,]
for(i in seq_along(words)){
  
  # we can use the table function to count the words in a text
  counts = table(words[[i]])
  
  # now the counts object has words associated with their count for the ith text
  # to get this into the dtm, we can use i to select the row, and counts' names to select the columns
  dtm[i, names(counts)] = counts
  
}

# now dtm is filled in with counts for each text
dtm[, 1:10]
```

## Processing with lingmatch

The lma_dtm function from the lingmatch package will do the same sort of processing
we just walked through.
```{r}
# load in the lingmatch package
library(lingmatch)

dtm = lma_dtm(text, sparse = FALSE)
dtm[, 1:10]
```

With even this simple level of processing, we can get some fun information:
```{r}
# a count of words across documents, or document count
colSums(dtm)[1:5]

# a count of words in each document, or word count
rowSums(dtm)

# correlation between words
cor(dtm)[1:5, 1:5]

# correlation between texts
cor(t(dtm))
```

This is getting into the fundamentals of information extraction/retrieval,
where, for example, term similarity can be used to enhance searches,
and document similarity can be the search criteria.
Document similarity is also secretly linguistic similarity.

# Document/Term Weighting

Using just raw word counts, common words have more weight -- potentially many times more.
Often, we're more interested in less common words, as those tend to be more meaningful
(they tend to be content words, as opposed to function words, and can better distinguish
relevant texts to a search).

The most simple, straightforward way to adjust the weight of words is to transform their
count values. For example, log transformations reduce the difference between values, and increase that
reduction as values increase:

`log(2) - log(1)` = `r log(2) - log(1)`

`log(5) - log(4)` = `r log(5) - log(4)`

Another source of weighting can come from matrix-wide information, such as document or term counts.
For example, dividing a term by its document frequency reduces the weight of higher frequency terms:

`c(1, 1, 0) / sum(c(1, 1, 0))` = `r c(1, 1, 0) / sum(c(1, 1, 0))`

`c(1, 1, 1) / sum(c(1, 1, 1))` = `r c(1, 1, 1) / sum(c(1, 1, 1))`

Different weighting schemes work to either amplify or dampen difference in values or in document
or term frequencies:
```{r}
#load in the splot package
library(splot)

# set up plotting options
op = list(y = 'term_frequency ~ document_frequency', line = 'connected', leg = 'outside')

# and the range of document frequency
document_frequency = seq_len(20)
```

Comparing term weights:
```{r, fig.height=5, fig.width=10, dev='CairoSVG'}
term_weights = c('binary', 'log', 'sqrt', 'count', 'amplify')
term_frequency = sapply(term_weights, function(w) lma_weight(matrix(document_frequency, 1), w, FALSE))
splot(myl = c(0, 25), options = op)
```

Comparing document weights (see `?lma_weight` for more about each scheme):
```{r, fig.height=5, fig.width=10, dev='CairoSVG'}
doc_weights = c('df', 'dfmax', 'idf', 'normal', 'poisson', 'ridf', 'entropy')
term_frequency = sapply(doc_weights,function(w)
  lma_weight(sapply(document_frequency, function(i) sample(0:i, 5000, TRUE)), w, FALSE, doc.only = TRUE)
)
splot(myl = c(-3, 3), mv.scale = TRUE, options = op)
```

We can weight our example dtm and see how it affects the correlation between documents:
```{r}
# idf dampens frequent words
cor(t(lma_weight(dtm, 'idf')))

# whereas entropy amplifies frequent words
cor(t(lma_weight(dtm, 'entropy')))
```

For now, we'll just apply standard word count (frequency) weighting
```{r}
wdtm = lma_weight(dtm)
```

# Dimension Reduction

A more thorough way to change the dtm space is by reducing its dimensions.
This is effectively what LIWC (or any dictionary-based approach) does --
reducing the number of terms by throwing them into categories:
```{r}
cwdtm = lma_termcat(wdtm)
cwdtm[, 1:6]
```

# Language Style Matching

The way we've set things up allows us to look at the standard LSM calculation:
```{r}
lma_simets(cwdtm, metric = 'canberra')
```

What makes language style matching "language style matching" (as opposed to, say, "document similarity" or "latent semantic similarity") is our choice of weighting, dimension reduction, and similarity metric.

***
<div style='text-align:center'>
![Brought to you by the <a href="https://www.depts.ttu.edu/psy/lusi/resources.php" target="_blank">Language Use and Social Interaction</a> lab at Texas Tech University](https://www.depts.ttu.edu/psy/lusi/files/lusi.jpg)
</div>
