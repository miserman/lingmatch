---
title: "Making Meaning from Matching"
output: rmarkdown::html_document
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: accommodation.bib
csl: apa.csl
link-citations: true
---

Covers a few ways to probe matching scores in order to make inferences about them.

*Built with R 
`r getRversion()`
on 
`r format(Sys.time(),'%B %d %Y')`*

Similarity scores can give some absolute sense of how similar two texts are in terms of included features, within the given metric's distribution. Relative senses of similarity are often more interesting, but require additional data. These are general statistical inference considerations, but can be particularly relevant when interpreting similarity between texts.

***
# Meaningful difference...

First, we can process a few short texts, and calculate the similarities between the first and each subsequent text:
```{r}
library(lingmatch)
library(splot)

text_sims = lingmatch(c(
  'Comparing everything to this text.',
  'Not this text to everything!',
  'This text to the first also...',
  'Same thing for this text...',
  'And for this one...',
  'But then totally different?'
), 'Comparing everything to this text.', metric = 'all')
text_sims$sim
```

## from other scores

For all metrics, scores are 1 when texts match perfectly, and scores decrease as texts become less similar, but scores decrease at different rates:
```{r, fig.height=4, fig.width=10, dev='CairoSVG'}
splot(text_sims$sim ~ 1:6, line.type = 'b', title = FALSE, labx = 'Text', laby = 'Score')
```

The metrics are also differently distributed within this small feature space, which we can get a better sense of by loosely simulating texts (allowing for counts over 1):
```{r, fig.height=4, fig.width=10, dev='CairoSVG'}
nfeatures = ncol(text_sims$dtm)

# this draws new features from a Poisson distribution
# parameterized by the real features, then calculates
# all pairwise comparisons between the resulting texts
sim_sims = vapply(
  lma_simets(matrix(
    rpois(nfeatures * 99, colMeans(text_sims$dtm)), ncol = nfeatures, byrow = TRUE
  )), '[', numeric(99 * 98 / 2), which(lower.tri(diag(99)))
)
splot(sim_sims, title = FALSE, labx = 'Score', mxl = c(-.5, 1))
```

These simulated distributions could be used as a null distribution for a non-parametric significance estimate (percent of null distribution over the observed score; interpretable like _p_-values):
```{r}
# this loops through metrics and texts to
# calculate the proportion of the metric's null distribution
# over the observed metric score
vapply(colnames(text_sims$sim), function(met){
  vapply(seq_len(nrow(text_sims$sim)), function(i){
    mean(sim_sims[, met] > text_sims$sim[i, met])
  }, 0)
}, text_sims$sim[, 1])
```

Null distributions could also be estimated from generated features, independently sampled from the real features:
```{r, fig.height=4, fig.width=10, dev='CairoSVG'}
# same process as the simulated texts,
# but draws from the real features rather than Poisson distributions
samp_sims = vapply(
  lma_simets(vapply(colnames(text_sims$dtm), function(feat){
    sample(text_sims$dtm[, feat], 99, TRUE)
  }, numeric(99))), '[', numeric(99 * 98 / 2), which(lower.tri(diag(99)))
)
splot(samp_sims, title = FALSE, labx = 'Score', mxl = c(-.5, 1))
vapply(colnames(text_sims$sim), function(met){
  vapply(seq_len(nrow(text_sims$sim)), function(i){
    mean(samp_sims[, met] > text_sims$sim[i, met])
  }, 0)
}, text_sims$sim[, 1])
```

Resampling makes fewer assumptions than simulating new features (which, in this case, assumed features were Poisson-distributed), but may depend more on the number of real observations.

These are just two examples of many that work toward some significance estimate, but the common goal is to estimate a distribution of either the null or effect distribution for a metric within a feature space. The meaning this adds to the score is an idea of its likelihood, where texts with sufficiently unlikely scores might be considered conspicuously similar or dissimilar.

When texts are conspicuously similar, it may indicate that something interesting is making them similar, such as some psychological process like understanding or accommodation, but such processes are not the only explanation for conspicuous similarity. One way to winnow down possible explanations for similarity is to ensure that obvious alternatives are not present. For example, if the topic of conversation varies between texts, texts of the same topic would probably be particularly similar, depending on the features going into the measure of similarity. Even if features are not obviously associated with topics (as in the case of function words), their baseline similarity may vary between topics. Ensuring topics are the same in all text would avoid this particular issue, but given a mix of topics, you might still be able to get at subtler differences by considering a baseline.

## from a baseline
The distributional consideration of the previous section could apply to any measure. One thing that makes measures of linguistic similarity a little more distinct is the fact that language often has a relevant source and context.

To explore contextualized differences, we can simulate speaker interactions:
```{r}
# define each speaker's default use frequency of each feature
priors = matrix(.05, 3, 26, dimnames = list(c('context', 'speaker_a', 'speaker_b'), letters))
for(f in rownames(priors)) priors[f, sample(letters, 8)] = 7 * rbeta(8, 2, 1)

data = data.frame(
  speaker = rep(rownames(priors), 50),
  turn = rep(1:50, each = 3),
  matrix(0, 150, 26, dimnames = list(NULL, letters))
)
for(i in 1:50){
  p = priors
  d = data[data$turn == i, -2]
  if(i != 1){
    p[2,] = as.numeric(p[2,] * .7 + d[d$speaker == 'context', -1] * .3)
    p[3,] = as.numeric(p[3,] * .1 + d[d$speaker == 'context', -1] * .1 + d[d$speaker == 'speaker_a', -1] * .8)
  }
  data[data$turn == i, -(1:2)] = matrix(rpois(length(p), p), 3, byrow = TRUE)
}

splot(rowMeans(data[, -(1:2)]) ~ turn * speaker, data)
splot(rowMeans(data[, names(which(priors[1,] != .05))]) ~ turn * speaker, data)
splot(rowMeans(data[, names(which(priors[2,] != .05))]) ~ turn * speaker, data)
summary(lm(rowMeans(data[, -(1:2)]) ~ turn * speaker, data))


lma_simets(rowMeans(data[data$speaker == 'speaker_a', -(1:2)]), metric = 'cor', lag = 1)
lma_simets(rowMeans(data[data$speaker == 'speaker_b', -(1:2)]), metric = 'cor', lag = 1)
```

### Trends
```{r}
splot(
  data$agg ~ turn * speaker, data,
  title = FALSE, laby = 'Feature Average', leg = 'out', line = FALSE,
  add = abline(h = colMeans(baselines), col = c('grey', seg$cols),
    lwd = c(1, 2, 2, 2), lty = c(2, 1, 1, 1), xpd = FALSE)
)

tt1 = lingmatch(data[, -(1:4)], 'seq', group = data[, 1:2])
splot(cosine ~ rep(1:49, 3) * group, tt1$sim)

tt2 = lingmatch(data[, -(1:4)], 'pair', group = data[, 1])
splot(tt2$sim$cosine ~ rep(1:50, each = 3) * tt2$sim$`data[, 1]`)

lma_simets(rowMeans(data[data$speaker == 'stalwart', -(1:4)]), metric = 'cor', lag = -1)
lma_simets(rowMeans(data[data$speaker == 'context_sensitive', -(1:4)]), metric = 'cor', lag = -1)
lma_simets(rowMeans(data[data$speaker == 'speaker_sensitive', -(1:4)]), metric = 'cor', lag = -1)

td = data[, -(1:3)]
sua = data$speaker == 'a'
splot(
  lma_simets(td[data$speaker == 'c',], td[sua,], lag = 0) ~ seq_len(sum(sua)),
  mv.scale = TRUE
)

```

# References
